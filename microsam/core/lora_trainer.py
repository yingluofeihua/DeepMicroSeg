"""
LoRAè®­ç»ƒå™¨ - å®Œæ•´å¤šå®ä¾‹ç‰ˆæœ¬
æ”¯æŒSAMçš„å¤šå®ä¾‹ç»†èƒåˆ†å‰²è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import time
import json
from tqdm import tqdm
import wandb

from config.lora_config import LoRATrainingSettings
from lora.sam_lora_wrapper import SAMLoRAWrapper, create_sam_lora_model
from lora.data_loaders import create_data_loaders
from lora.training_utils import (
    SAMLoss, prepare_sam_inputs, TrainingMetrics,
    # ğŸ”§ æ–°å¢å¤šå®ä¾‹ç›¸å…³å‡½æ•°
    SAMLossMultiInstance, 
    prepare_sam_inputs_multi_instance,
    create_sam_training_step_multi_instance,
    validate_sam_batch_multi_instance
)
from core.metrics import ComprehensiveMetrics, MetricsResult
from utils.file_utils import setup_logging
from utils.model_utils import optimize_memory, get_device_info, print_model_summary


class LoRATrainer:
    """LoRAè®­ç»ƒå™¨ - æ”¯æŒå¤šå®ä¾‹SAMç»†èƒåˆ†å‰²"""
    
    def __init__(self, config: LoRATrainingSettings):
        self.config = config
        self.device = self._setup_device()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.data_loaders = {}
        self.loss_fn = None
        self.metrics_calculator = ComprehensiveMetrics()
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_metric = float('inf')
        self.early_stopping_counter = 0
        
        # ğŸ”§ å¤šå®ä¾‹è®­ç»ƒç›¸å…³
        self.use_multi_instance = True  # å¯ç”¨å¤šå®ä¾‹è®­ç»ƒ
        self.training_step_fn = None
        
        # æ—¥å¿—å’Œç›‘æ§
        self.writer = None
        self.logger = setup_logging()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = self.config.get_model_output_dir()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        self.config.save_to_json(self.output_dir / "config.json")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if self.config.experiment.device == "auto":
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            device = torch.device(self.config.experiment.device)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device.type == "cuda":
            device_info = get_device_info()
            print(f"GPUè®¾å¤‡æ•°é‡: {device_info['cuda_device_count']}")
            print(f"å½“å‰GPU: {torch.cuda.current_device()}")
            if device_info['gpu_memory']:
                current_gpu = f"gpu_{torch.cuda.current_device()}"
                if current_gpu in device_info['gpu_memory']:
                    gpu_info = device_info['gpu_memory'][current_gpu]
                    print(f"GPUå†…å­˜: {gpu_info['total_memory'] / 1e9:.1f} GB")
        
        return device
    
    def setup_model(self) -> bool:
        """è®¾ç½®SAM LoRAæ¨¡å‹ - æ”¯æŒå¤šå®ä¾‹"""
        try:
            print("æ­£åœ¨è®¾ç½®SAM LoRAå¤šå®ä¾‹æ¨¡å‹...")
            
            # åˆ›å»ºLoRAé…ç½®
            lora_config = {
                'rank': self.config.lora.rank,
                'alpha': self.config.lora.alpha,
                'dropout': self.config.lora.dropout,
                'target_modules': self.config.lora.target_modules,
                'apply_lora_to': self.config.model.apply_lora_to,
                'freeze_image_encoder': self.config.model.freeze_backbone,
                'freeze_prompt_encoder': self.config.model.freeze_prompt_encoder,
                'freeze_mask_decoder': self.config.model.freeze_mask_decoder
            }
            
            # åˆ›å»ºSAM LoRAæ¨¡å‹
            self.model = create_sam_lora_model(
                model_type=self.config.model.base_model_name,
                lora_config=lora_config,
                device=str(self.device)
            )
            
            if self.model is None:
                print("SAM LoRAæ¨¡å‹åˆ›å»ºå¤±è´¥")
                return False
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            self.model = self.model.to(self.device)
            
            # ç¡®ä¿æ‰€æœ‰LoRAæ¨¡å—åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if hasattr(self.model, 'lora_modules'):
                for name, lora_module in self.model.lora_modules.items():
                    if hasattr(lora_module, 'lora'):
                        lora_module.lora = lora_module.lora.to(self.device)
                    lora_module = lora_module.to(self.device)
            
            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            self.model.print_model_info()
            print_model_summary(self.model)
            
            # éªŒè¯æ¨¡å‹è®¾å¤‡ä¸€è‡´æ€§
            self._verify_model_device_consistency()
            
            return True
            
        except Exception as e:
            print(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _verify_model_device_consistency(self):
        """éªŒè¯æ¨¡å‹è®¾å¤‡ä¸€è‡´æ€§"""
        print(f"\néªŒè¯æ¨¡å‹è®¾å¤‡ä¸€è‡´æ€§...")
        
        device_counts = {}
        
        # æ£€æŸ¥ä¸»è¦ç»„ä»¶
        for name, module in [
            ('image_encoder', self.model.image_encoder),
            ('prompt_encoder', self.model.prompt_encoder), 
            ('mask_decoder', self.model.mask_decoder)
        ]:
            if module is not None:
                for param_name, param in module.named_parameters():
                    device = str(param.device)
                    device_counts[device] = device_counts.get(device, 0) + 1
        
        # æ£€æŸ¥LoRAæ¨¡å—
        if hasattr(self.model, 'lora_modules'):
            for lora_name, lora_module in self.model.lora_modules.items():
                if hasattr(lora_module, 'lora'):
                    for param_name, param in lora_module.lora.named_parameters():
                        device = str(param.device)
                        device_counts[device] = device_counts.get(device, 0) + 1
        
        print(f"è®¾å¤‡åˆ†å¸ƒ: {device_counts}")
        
        if len(device_counts) > 1:
            print(f"âš ï¸  å‘ç°å¤šä¸ªè®¾å¤‡ï¼Œæ­£åœ¨ç»Ÿä¸€åˆ° {self.device}")
            self.model = self.model.to(self.device)
            print(f"âœ… æ‰€æœ‰æ¨¡å‹ç»„ä»¶å·²ç§»åŠ¨åˆ° {self.device}")
        else:
            print(f"âœ… æ‰€æœ‰æ¨¡å‹ç»„ä»¶éƒ½åœ¨ {self.device} ä¸Š")
    
    def setup_data_loaders(self) -> bool:
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ - æ”¯æŒå¤šå®ä¾‹"""
        try:
            print("æ­£åœ¨åˆ›å»ºå¤šå®ä¾‹æ•°æ®åŠ è½½å™¨...")
            
            # ğŸ”§ ä½¿ç”¨SAMæ•°æ®é›†æ ¼å¼ï¼Œæ”¯æŒå¤šå®ä¾‹
            self.data_loaders = create_data_loaders(
                config=self.config.data,
                dataset_type="sam"  # ä½¿ç”¨SAMæ•°æ®é›†æ ¼å¼
            )
            
            print("å¤šå®ä¾‹æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ:")
            for split, loader in self.data_loaders.items():
                print(f"  {split}: {len(loader)} æ‰¹æ¬¡, {len(loader.dataset)} æ ·æœ¬")
                
                # ğŸ”§ éªŒè¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ ¼å¼
                if len(loader) > 0:
                    try:
                        first_batch = next(iter(loader))
                        print(f"    - å›¾åƒå½¢çŠ¶: {first_batch['images'].shape}")
                        print(f"    - æ©ç å½¢çŠ¶: {first_batch['ground_truth_masks'].shape}")
                        print(f"    - æœ€å¤§å®ä¾‹æ•°: {first_batch['ground_truth_masks'].shape[1]}")
                    except Exception as e:
                        print(f"    - æ‰¹æ¬¡éªŒè¯å¤±è´¥: {e}")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å™¨è®¾ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def setup_optimizer_and_loss(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° - å¤šå®ä¾‹ç‰ˆæœ¬"""
        # åªä¼˜åŒ–LoRAå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(trainable_params)}")
        total_trainable = sum(p.numel() for p in trainable_params)
        print(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_trainable:,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if self.config.training.optimizer.lower() == "adamw":
            self.optimizer = optim.AdamW(
                trainable_params,
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay,
                betas=(self.config.training.adam_beta1, self.config.training.adam_beta2),
                eps=self.config.training.adam_epsilon
            )
        else:
            self.optimizer = optim.Adam(
                trainable_params,
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.training.lr_scheduler_type == "cosine":
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training.num_epochs,
                eta_min=self.config.training.learning_rate * 0.01
            )
        elif self.config.training.lr_scheduler_type == "step":
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.training.num_epochs // 3,
                gamma=0.1
            )
        else:
            self.scheduler = None
        
        # ğŸ”§ åˆ›å»ºå¤šå®ä¾‹æŸå¤±å‡½æ•°
        if self.use_multi_instance:
            loss_config = {
                'focal_loss_weight': 20.0,
                'dice_loss_weight': 1.0,
                'iou_loss_weight': 1.0,
                'instance_loss_weight': 5.0,
                'use_focal_loss': True,
                'use_dice_loss': True,
                'use_iou_loss': True,
                'use_instance_loss': True
            }
            self.loss_fn = SAMLossMultiInstance(**loss_config)
            print("ä½¿ç”¨å¤šå®ä¾‹SAMæŸå¤±å‡½æ•°")
        else:
            # ä¼ ç»Ÿå•å®ä¾‹æŸå¤±å‡½æ•°
            loss_config = {
                'focal_loss_weight': 20.0,
                'dice_loss_weight': 1.0,
                'iou_loss_weight': 1.0,
                'use_focal_loss': True,
                'use_dice_loss': True,
                'use_iou_loss': True
            }
            self.loss_fn = SAMLoss(**loss_config)
            print("ä½¿ç”¨ä¼ ç»ŸSAMæŸå¤±å‡½æ•°")
        
        print(f"ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ: {type(self.optimizer).__name__}")
        print(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {type(self.scheduler).__name__ if self.scheduler else 'None'}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        # TensorBoard
        log_dir = self.config.get_logs_dir()
        self.writer = SummaryWriter(log_dir)
        
        # Weights & Biases
        if self.config.experiment.use_wandb:
            wandb.init(
                project=self.config.experiment.wandb_project,
                entity=self.config.experiment.wandb_entity,
                name=self.config.experiment.run_name,
                config=self.config.to_dict(),
                dir=str(self.output_dir)
            )
    
    def train(self) -> bool:
        """å¼€å§‹è®­ç»ƒ - å¤šå®ä¾‹ç‰ˆæœ¬"""
        print("="*60)
        print("å¼€å§‹SAM LoRAå¤šå®ä¾‹å¾®è°ƒè®­ç»ƒ")
        print("="*60)
        
        # è®¾ç½®æ‰€æœ‰ç»„ä»¶
        if not self.setup_model():
            return False
        
        if not self.setup_data_loaders():
            return False
        
        self.setup_optimizer_and_loss()
        self.setup_logging()
        
        # ğŸ”§ åˆ›å»ºå¤šå®ä¾‹è®­ç»ƒæ­¥éª¤å‡½æ•°
        if self.use_multi_instance:
            self.training_step_fn = create_sam_training_step_multi_instance(
                self.model, self.optimizer, self.loss_fn, self.device
            )
            print("ä½¿ç”¨å¤šå®ä¾‹è®­ç»ƒæ­¥éª¤å‡½æ•°")
        else:
            # ä½¿ç”¨ä¼ ç»Ÿè®­ç»ƒæ­¥éª¤
            from lora.training_utils import create_sam_training_step
            self.training_step_fn = create_sam_training_step(
                self.model, self.optimizer, self.loss_fn, self.device
            )
            print("ä½¿ç”¨ä¼ ç»Ÿè®­ç»ƒæ­¥éª¤å‡½æ•°")
        
        # è®­ç»ƒå¾ªç¯
        try:
            for epoch in range(self.config.training.num_epochs):
                self.current_epoch = epoch
                
                print(f"\nå¼€å§‹ç¬¬ {epoch + 1}/{self.config.training.num_epochs} è½®è®­ç»ƒ")
                
                # ğŸ”§ ä½¿ç”¨å¯¹åº”çš„è®­ç»ƒå‡½æ•°
                if self.use_multi_instance:
                    train_metrics = self.train_epoch_multi_instance()
                else:
                    train_metrics = self.train_epoch()
                
                # éªŒè¯
                if self.use_multi_instance:
                    val_metrics = self.validate_multi_instance() if 'val' in self.data_loaders else {}
                else:
                    val_metrics = self.validate() if 'val' in self.data_loaders else {}

                # è®°å½•æ—¥å¿—
                self.log_metrics(train_metrics, val_metrics, epoch)
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % max(1, self.config.training.save_steps // len(self.data_loaders['train'])) == 0:
                    self.save_checkpoint(epoch, val_metrics)
                
                # æ—©åœæ£€æŸ¥
                if self.check_early_stopping(val_metrics):
                    print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒ")
                    break
                
                # æ›´æ–°å­¦ä¹ ç‡
                if self.scheduler:
                    self.scheduler.step()
                
                # å†…å­˜ä¼˜åŒ–
                optimize_memory()
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_final_model()
            
            print("\n" + "="*60)
            print("å¤šå®ä¾‹è®­ç»ƒå®Œæˆ!")
            print("="*60)
            return True
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            if self.writer:
                self.writer.close()
            if self.config.experiment.use_wandb:
                wandb.finish()
    
    def train_epoch_multi_instance(self) -> Dict[str, float]:
        """å¤šå®ä¾‹è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_metrics = TrainingMetrics()
        
        progress_bar = tqdm(
            self.data_loaders['train'],
            desc=f"Epoch {self.current_epoch + 1}/{self.config.training.num_epochs}"
        )
        
        successful_steps = 0
        failed_steps = 0
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                # ğŸ”§ ä½¿ç”¨å¤šå®ä¾‹è®­ç»ƒæ­¥éª¤å‡½æ•°
                step_metrics = self.training_step_fn(batch)
                
                # æ£€æŸ¥è®­ç»ƒç»“æœ
                if 'error' not in step_metrics:
                    epoch_metrics.update(step_metrics)
                    self.global_step += 1
                    successful_steps += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'loss': f"{step_metrics.get('total_loss', 0):.4f}",
                        'focal': f"{step_metrics.get('focal_loss', 0):.4f}",
                        'dice': f"{step_metrics.get('dice_loss', 0):.4f}",
                        'instance': f"{step_metrics.get('instance_loss', 0):.4f}",
                        'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}",
                        'success': f"{successful_steps}/{successful_steps + failed_steps}"
                    })
                    
                    # è®°å½•æ­¥éª¤æ—¥å¿—
                    if self.global_step % self.config.training.logging_steps == 0:
                        self.log_step_metrics(step_metrics, batch_idx)
                else:
                    failed_steps += 1
                    error_type = step_metrics.get('error_type', 'unknown')
                    print(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {error_type}")
                    
                    # å¦‚æœå¤±è´¥ç‡è¿‡é«˜ï¼Œåœæ­¢è®­ç»ƒ
                    if failed_steps > successful_steps and batch_idx > 10:
                        print(f"å¤±è´¥ç‡è¿‡é«˜ ({failed_steps}/{successful_steps + failed_steps})ï¼Œåœæ­¢è®­ç»ƒ")
                        break
                
            except Exception as e:
                print(f"è®­ç»ƒæ­¥éª¤å¼‚å¸¸ (æ‰¹æ¬¡ {batch_idx}): {e}")
                failed_steps += 1
                continue
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_metrics = epoch_metrics.compute()
        avg_metrics['learning_rate'] = self.optimizer.param_groups[0]['lr']
        avg_metrics['successful_steps'] = successful_steps
        avg_metrics['failed_steps'] = failed_steps
        avg_metrics['success_rate'] = successful_steps / (successful_steps + failed_steps) if (successful_steps + failed_steps) > 0 else 0
        
        print(f"Epochå®Œæˆ: æˆåŠŸ {successful_steps}, å¤±è´¥ {failed_steps}, æˆåŠŸç‡ {avg_metrics['success_rate']:.2%}")
        
        return avg_metrics
    
    def train_epoch(self) -> Dict[str, float]:
        """ä¼ ç»Ÿè®­ç»ƒä¸€ä¸ªepochï¼ˆå•å®ä¾‹ï¼‰"""
        self.model.train()
        
        epoch_metrics = TrainingMetrics()
        
        progress_bar = tqdm(
            self.data_loaders['train'],
            desc=f"Epoch {self.current_epoch + 1}/{self.config.training.num_epochs}"
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                step_metrics = self.training_step_fn(batch)
                
                # æ›´æ–°ç»Ÿè®¡
                if 'error' not in step_metrics:
                    epoch_metrics.update(step_metrics)
                    self.global_step += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'loss': f"{step_metrics.get('total_loss', 0):.4f}",
                        'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}"
                    })
                    
                    # è®°å½•æ­¥éª¤æ—¥å¿—
                    if self.global_step % self.config.training.logging_steps == 0:
                        self.log_step_metrics(step_metrics, batch_idx)
                else:
                    print(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥")
                    
            except Exception as e:
                print(f"è®­ç»ƒæ­¥éª¤å¤±è´¥ (æ‰¹æ¬¡ {batch_idx}): {e}")
                continue
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_metrics = epoch_metrics.compute()
        avg_metrics['learning_rate'] = self.optimizer.param_groups[0]['lr']
        
        return avg_metrics
    
    def validate_multi_instance(self) -> Dict[str, float]:
        """å¤šå®ä¾‹éªŒè¯æ¨¡å‹"""
        if 'val' not in self.data_loaders:
            return {}
        
        print("æ­£åœ¨è¿›è¡Œå¤šå®ä¾‹éªŒè¯...")
        self.model.eval()
        
        val_metrics = TrainingMetrics()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(self.data_loaders['val'], desc="Validating")):
                try:
                    # ğŸ”§ ä½¿ç”¨å¤šå®ä¾‹è¾“å…¥å‡†å¤‡
                    inputs, targets = prepare_sam_inputs_multi_instance(batch)
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            inputs[key] = value.to(self.device)
                        elif isinstance(value, list):
                            inputs[key] = [v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value]
                    
                    for key, value in targets.items():
                        if isinstance(value, torch.Tensor):
                            targets[key] = value.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    predictions = self.model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_fn(predictions, targets)
                    val_metrics.update(loss_dict)
                    
                    # ğŸ”§ å¤„ç†å¤šå®ä¾‹é¢„æµ‹å’Œç›®æ ‡ç”¨äºæŒ‡æ ‡è®¡ç®—
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()  # [B, N_pred, H, W]
                    target_masks = targets['masks'].cpu().numpy()  # [B, N_target, H, W]
                    
                    # ä¸ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©æœ€ä½³é¢„æµ‹å’Œåˆå¹¶ç›®æ ‡
                    for i in range(pred_masks.shape[0]):
                        # é€‰æ‹©IoUå¾—åˆ†æœ€é«˜çš„é¢„æµ‹æ©ç 
                        if pred_masks.shape[1] > 1 and 'iou_predictions' in predictions:
                            iou_scores = predictions['iou_predictions'][i].cpu().numpy()
                            best_pred_idx = np.argmax(iou_scores)
                            pred_mask = pred_masks[i, best_pred_idx]
                        else:
                            pred_mask = pred_masks[i, 0] if pred_masks.shape[1] > 0 else np.zeros_like(target_masks[i, 0])
                        
                        # åˆå¹¶æ‰€æœ‰ç›®æ ‡å®ä¾‹ï¼ˆç”¨äºæ•´ä½“è¯„ä¼°ï¼‰
                        target_combined = (target_masks[i].sum(axis=0) > 0).astype(np.float32)
                        
                        all_predictions.append(pred_mask)
                        all_targets.append(target_combined)
                    
                    # é™åˆ¶éªŒè¯æ‰¹æ¬¡æ•°é‡
                    if batch_idx >= 10:
                        break
                        
                except Exception as e:
                    print(f"éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_val_metrics = val_metrics.compute()
        
        # è®¡ç®—åˆ†å‰²æŒ‡æ ‡
        if all_predictions and all_targets:
            print(f"è®¡ç®—åˆ†å‰²æŒ‡æ ‡ï¼Œæ ·æœ¬æ•°é‡={len(all_predictions)}")
            seg_metrics = self._compute_segmentation_metrics_multi_instance(all_predictions, all_targets)
            avg_val_metrics.update(seg_metrics)
        else:
            print("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹/ç›®æ ‡æ•°æ®")
            avg_val_metrics.update({'val_iou': 0.0, 'val_dice': 0.0})
        
        return avg_val_metrics
    
    def validate(self) -> Dict[str, float]:
        """ä¼ ç»ŸéªŒè¯æ¨¡å‹ï¼ˆå•å®ä¾‹ï¼‰"""
        if 'val' not in self.data_loaders:
            return {}
        
        print("æ­£åœ¨éªŒè¯...")
        self.model.eval()
        
        val_metrics = TrainingMetrics()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(self.data_loaders['val'], desc="Validating")):
                try:
                    # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                    inputs, targets = prepare_sam_inputs(batch)
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            inputs[key] = value.to(self.device)
                        elif isinstance(value, list):
                            inputs[key] = [v.to(device) if isinstance(v, torch.Tensor) else v for v in value]
                    
                    for key, value in targets.items():
                        if isinstance(value, torch.Tensor):
                            targets[key] = value.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    predictions = self.model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_fn(predictions, targets)
                    val_metrics.update(loss_dict)
                    
                    # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    target_masks = targets['masks'].cpu().numpy()
                    
                    for i in range(pred_masks.shape[0]):
                        pred_mask = pred_masks[i]
                        target_mask = target_masks[i]
                        
                        all_predictions.append(pred_mask)
                        all_targets.append(target_mask)
                    
                    # åªå¤„ç†å‰å‡ ä¸ªæ‰¹æ¬¡ç”¨äºéªŒè¯
                    if batch_idx >= 5:
                        break
                        
                except Exception as e:
                    print(f"éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_val_metrics = val_metrics.compute()
        
        # è®¡ç®—åˆ†å‰²æŒ‡æ ‡
        if all_predictions and all_targets:
            seg_metrics = self._compute_segmentation_metrics(all_predictions, all_targets)
            avg_val_metrics.update(seg_metrics)
        else:
            avg_val_metrics.update({'val_iou': 0.0, 'val_dice': 0.0})
        
        return avg_val_metrics
    
    def _compute_segmentation_metrics_multi_instance(self, predictions: List, targets: List) -> Dict[str, float]:
        """è®¡ç®—å¤šå®ä¾‹åˆ†å‰²æŒ‡æ ‡"""
        try:
            all_ious = []
            all_dices = []
            
            for pred, target in zip(predictions, targets):
                # å¤„ç†ç»´åº¦
                if pred.ndim > 2:
                    pred = pred[0] if pred.shape[0] == 1 else pred.mean(axis=0)
                if target.ndim > 2:
                    target = target[0] if target.shape[0] == 1 else target.mean(axis=0)
                
                # äºŒå€¼åŒ–
                pred_binary = (pred > 0.5).astype(np.int32)
                target_binary = (target > 0.5).astype(np.int32)
                
                # è®¡ç®—IoUå’ŒDice
                intersection = np.sum(pred_binary * target_binary)
                pred_sum = np.sum(pred_binary)
                target_sum = np.sum(target_binary)
                union = pred_sum + target_sum - intersection
                
                if union > 0:
                    iou = intersection / union
                    dice = 2 * intersection / (pred_sum + target_sum)
                    all_ious.append(iou)
                    all_dices.append(dice)
                else:
                    # å¦‚æœéƒ½æ˜¯ç©ºçš„ï¼Œè®¤ä¸ºå®Œå…¨åŒ¹é…
                    if pred_sum == 0 and target_sum == 0:
                        all_ious.append(1.0)
                        all_dices.append(1.0)
                    else:
                        all_ious.append(0.0)
                        all_dices.append(0.0)
            
            avg_iou = np.mean(all_ious) if all_ious else 0.0
            avg_dice = np.mean(all_dices) if all_dices else 0.0
            
            print(f"å¤šå®ä¾‹éªŒè¯å®Œæˆï¼Œå¹³å‡IoU={avg_iou:.4f}, å¹³å‡Dice={avg_dice:.4f}")
            
            return {
                'val_iou': avg_iou,
                'val_dice': avg_dice,
                'val_samples': len(all_ious)
            }
            
        except Exception as e:
            print(f"å¤šå®ä¾‹åˆ†å‰²æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'val_iou': 0.0, 'val_dice': 0.0}
    
    def _compute_segmentation_metrics(self, predictions: List, targets: List) -> Dict[str, float]:
        """è®¡ç®—åˆ†å‰²æŒ‡æ ‡ï¼ˆä¼ ç»Ÿç‰ˆæœ¬ï¼‰"""
        try:
            all_ious = []
            all_dices = []
            
            for pred, target in zip(predictions, targets):
                # å¤„ç†ç»´åº¦
                if pred.ndim > 2:
                    pred = pred[0] if pred.shape[0] == 1 else pred.mean(axis=0)
                if target.ndim > 2:
                    target = target[0] if target.shape[0] == 1 else target.mean(axis=0)
                
                # å°ºå¯¸åŒ¹é…
                if pred.shape != target.shape:
                    import torch.nn.functional as F
                    import torch
                    
                    pred_tensor = torch.from_numpy(pred).unsqueeze(0).unsqueeze(0).float()
                    target_size = target.shape
                    
                    pred_resized = F.interpolate(
                        pred_tensor, 
                        size=target_size, 
                        mode='bilinear',
                        align_corners=False
                    )
                    pred = pred_resized.squeeze().numpy()
                
                # äºŒå€¼åŒ–
                pred_binary = (pred > 0.5).astype(np.int32)
                target_binary = (target > 0.5).astype(np.int32)
                
                # è®¡ç®—IoUå’ŒDice
                intersection = np.sum(pred_binary * target_binary)
                union = np.sum(pred_binary) + np.sum(target_binary) - intersection
                
                if union > 0:
                    iou = intersection / union
                    dice = 2 * intersection / (np.sum(pred_binary) + np.sum(target_binary))
                    
                    all_ious.append(iou)
                    all_dices.append(dice)
                else:
                    all_ious.append(0.0)
                    all_dices.append(0.0)
            
            avg_iou = np.mean(all_ious) if all_ious else 0.0
            avg_dice = np.mean(all_dices) if all_dices else 0.0
            
            print(f"éªŒè¯å®Œæˆï¼Œå¹³å‡IoU={avg_iou:.4f}, å¹³å‡Dice={avg_dice:.4f}")
            
            return {
                'val_iou': avg_iou,
                'val_dice': avg_dice
            }
            
        except Exception as e:
            print(f"åˆ†å‰²æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'val_iou': 0.0, 'val_dice': 0.0}
    
    def log_metrics(self, train_metrics: Dict[str, float], 
                   val_metrics: Dict[str, float], epoch: int):
        """è®°å½•æŒ‡æ ‡"""
        # TensorBoardæ—¥å¿—
        if self.writer:
            for key, value in train_metrics.items():
                self.writer.add_scalar(f'train/{key}', value, epoch)
            
            for key, value in val_metrics.items():
                self.writer.add_scalar(f'val/{key}', value, epoch)
        
        # Weights & Biasesæ—¥å¿—
        if self.config.experiment.use_wandb:
            log_dict = {f'train/{k}': v for k, v in train_metrics.items()}
            log_dict.update({f'val/{k}': v for k, v in val_metrics.items()})
            log_dict['epoch'] = epoch
            wandb.log(log_dict)
        
        # æ§åˆ¶å°è¾“å‡º
        print(f"\nEpoch {epoch + 1}/{self.config.training.num_epochs} - æŒ‡æ ‡æ‘˜è¦:")
        print("è®­ç»ƒæŒ‡æ ‡:")
        for key, value in train_metrics.items():
            if key in ['successful_steps', 'failed_steps']:
                print(f"  {key}: {int(value)}")
            elif key == 'success_rate':
                print(f"  {key}: {value:.2%}")
            else:
                print(f"  {key}: {value:.4f}")
        
        if val_metrics:
            print("éªŒè¯æŒ‡æ ‡:")
            for key, value in val_metrics.items():
                print(f"  {key}: {value:.4f}")
    
    def log_step_metrics(self, step_metrics: Dict[str, float], step: int):
        """è®°å½•æ­¥éª¤æŒ‡æ ‡"""
        if self.writer:
            for key, value in step_metrics.items():
                if key not in ['error', 'success']:
                    self.writer.add_scalar(f'train/step_{key}', value, self.global_step)
        
        if self.config.experiment.use_wandb:
            log_dict = {f'train/step_{k}': v for k, v in step_metrics.items() 
                       if k not in ['error', 'success']}
            log_dict['global_step'] = self.global_step
            wandb.log(log_dict)
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = self.config.get_checkpoint_dir()
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_metric': self.best_metric,
            'config': self.config.to_dict(),
            'metrics': metrics,
            'use_multi_instance': self.use_multi_instance
        }
        
        # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹
        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch + 1}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_metric = metrics.get('avg_total_loss', float('inf'))
        if current_metric < self.best_metric:
            self.best_metric = current_metric
            best_path = checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {current_metric:.4f})")
        
        # ä¿å­˜LoRAæƒé‡
        lora_save_path = checkpoint_dir / f"lora_epoch_{epoch + 1}"
        self.model.save_lora_weights(lora_save_path)
        
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def check_early_stopping(self, metrics: Dict[str, float]) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        if not metrics or 'avg_total_loss' not in metrics:
            return False
        
        current_metric = metrics['avg_total_loss']
        
        if current_metric < self.best_metric:
            self.early_stopping_counter = 0
        else:
            self.early_stopping_counter += 1
        
        return self.early_stopping_counter >= self.config.training.early_stopping_patience
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        # ä¿å­˜LoRAæƒé‡
        final_dir = self.output_dir / "final_model"
        self.model.save_lora_weights(final_dir)
        
        # ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
        merged_path = final_dir / "merged_sam_model.pth"
        self.model.merge_and_save_full_model(merged_path)
        
        # ä¿å­˜è®­ç»ƒæ‘˜è¦
        summary = {
            'training_completed': True,
            'total_epochs': self.current_epoch + 1,
            'total_steps': self.global_step,
            'best_metric': self.best_metric,
            'model_type': self.config.model.base_model_name,
            'use_multi_instance': self.use_multi_instance,
            'lora_config': {
                'rank': self.config.lora.rank,
                'alpha': self.config.lora.alpha,
                'dropout': self.config.lora.dropout
            },
            'config': self.config.to_dict()
        }
        
        with open(final_dir / "training_summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_dir}")
    
    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if self.scheduler and checkpoint['scheduler_state_dict']:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint['epoch']
            self.global_step = checkpoint['global_step']
            self.best_metric = checkpoint['best_metric']
            
            # æ¢å¤å¤šå®ä¾‹è®¾ç½®
            if 'use_multi_instance' in checkpoint:
                self.use_multi_instance = checkpoint['use_multi_instance']
            
            print(f"æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
            print(f"ä»ç¬¬ {self.current_epoch + 1} è½®ç»§ç»­è®­ç»ƒ")
            print(f"ä½¿ç”¨å¤šå®ä¾‹æ¨¡å¼: {self.use_multi_instance}")
            
            return True
            
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def evaluate_model(self, test_data_loader: Optional[DataLoader] = None) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if test_data_loader is None:
            test_data_loader = self.data_loaders.get('test')
        
        if test_data_loader is None:
            print("æ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯„ä¼°")
            return {}
        
        self.model.eval()
        test_metrics = TrainingMetrics()
        all_predictions = []
        all_targets = []
        
        print("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        with torch.no_grad():
            for batch in tqdm(test_data_loader, desc="Evaluating"):
                try:
                    # æ ¹æ®æ¨¡å¼é€‰æ‹©è¾“å…¥å‡†å¤‡å‡½æ•°
                    if self.use_multi_instance:
                        inputs, targets = prepare_sam_inputs_multi_instance(batch)
                    else:
                        inputs, targets = prepare_sam_inputs(batch)
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            inputs[key] = value.to(self.device)
                        elif isinstance(value, list):
                            inputs[key] = [v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value]
                    
                    for key, value in targets.items():
                        if isinstance(value, torch.Tensor):
                            targets[key] = value.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    predictions = self.model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_fn(predictions, targets)
                    test_metrics.update(loss_dict)
                    
                    # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    target_masks = targets['masks'].cpu().numpy()
                    
                    if self.use_multi_instance:
                        # å¤šå®ä¾‹å¤„ç†
                        for i in range(pred_masks.shape[0]):
                            if pred_masks.shape[1] > 1 and 'iou_predictions' in predictions:
                                iou_scores = predictions['iou_predictions'][i].cpu().numpy()
                                best_pred_idx = np.argmax(iou_scores)
                                pred_mask = pred_masks[i, best_pred_idx]
                            else:
                                pred_mask = pred_masks[i, 0] if pred_masks.shape[1] > 0 else np.zeros_like(target_masks[i, 0])
                            
                            target_combined = (target_masks[i].sum(axis=0) > 0).astype(np.float32)
                            all_predictions.append(pred_mask)
                            all_targets.append(target_combined)
                    else:
                        # å•å®ä¾‹å¤„ç†
                        all_predictions.extend(pred_masks)
                        all_targets.extend(target_masks)
                    
                except Exception as e:
                    print(f"è¯„ä¼°æ­¥éª¤å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_metrics = test_metrics.compute()
        
        # è®¡ç®—åˆ†å‰²æŒ‡æ ‡
        if all_predictions and all_targets:
            if self.use_multi_instance:
                seg_metrics = self._compute_segmentation_metrics_multi_instance(all_predictions, all_targets)
            else:
                seg_metrics = self._compute_segmentation_metrics(all_predictions, all_targets)
            final_metrics.update(seg_metrics)
        
        print("è¯„ä¼°å®Œæˆ:")
        for key, value in final_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        return final_metrics


def create_trainer_from_config(config_path: str) -> LoRATrainer:
    """ä»é…ç½®æ–‡ä»¶åˆ›å»ºè®­ç»ƒå™¨"""
    config = LoRATrainingSettings.from_json(config_path)
    return LoRATrainer(config)


def resume_training(checkpoint_path: str, config_path: Optional[str] = None) -> LoRATrainer:
    """æ¢å¤è®­ç»ƒ"""
    if config_path:
        trainer = create_trainer_from_config(config_path)
    else:
        # ä»æ£€æŸ¥ç‚¹ç›®å½•åŠ è½½é…ç½®
        checkpoint_dir = Path(checkpoint_path).parent
        config_file = checkpoint_dir.parent / "config.json"
        if config_file.exists():
            trainer = create_trainer_from_config(str(config_file))
        else:
            print("æ— æ³•æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            from config.lora_config import LoRATrainingSettings
            trainer = LoRATrainer(LoRATrainingSettings())
    
    trainer.load_checkpoint(checkpoint_path)
    return trainer


def create_multi_instance_trainer(data_dir: str, 
                                model_type: str = "vit_b_lm",
                                batch_size: int = 4,
                                epochs: int = 15,
                                learning_rate: float = 1e-4,
                                rank: int = 8,
                                output_dir: str = "./multi_instance_experiments") -> LoRATrainer:
    """å¿«é€Ÿåˆ›å»ºå¤šå®ä¾‹è®­ç»ƒå™¨"""
    
    from config.lora_config import LoRATrainingSettings
    
    config = LoRATrainingSettings()
    
    # æ•°æ®é…ç½®
    config.data.train_data_dir = data_dir
    config.data.batch_size = batch_size
    config.data.max_objects_per_image = 20
    config.data.use_data_augmentation = True
    
    # æ¨¡å‹é…ç½®
    config.model.base_model_name = model_type
    config.model.apply_lora_to = ['image_encoder']
    
    # LoRAé…ç½®
    config.lora.rank = rank
    config.lora.alpha = rank * 2.0
    config.lora.dropout = 0.1
    
    # è®­ç»ƒé…ç½®
    config.training.num_epochs = epochs
    config.training.learning_rate = learning_rate
    config.training.batch_size = batch_size
    config.training.save_steps = 200
    config.training.eval_steps = 50
    config.training.logging_steps = 10
    
    # å®éªŒé…ç½®
    config.experiment.output_dir = output_dir
    config.experiment.experiment_name = f"multi_instance_{model_type}_r{rank}"
    
    trainer = LoRATrainer(config)
    trainer.use_multi_instance = True  # ç¡®ä¿å¯ç”¨å¤šå®ä¾‹
    
    return trainer


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¤šå®ä¾‹è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = create_multi_instance_trainer(
        data_dir="/path/to/your/cell/data",
        model_type="vit_b_lm",
        batch_size=4,
        epochs=15,
        learning_rate=1e-4,
        rank=8
    )
    
    success = trainer.train()
    
    if success:
        print("ğŸ‰ å¤šå®ä¾‹SAM LoRAè®­ç»ƒæˆåŠŸå®Œæˆ!")
    else:
        print("âŒ è®­ç»ƒå¤±è´¥")