'''
 # @ Author: Zhenhua Chen
 # @ Create Time: 2025-05-28 06:20:52
 # @ Email: Zhenhua.Chen@gmail.com
 # @ Description:
 '''

# stable_sam_lora_wrapper.py - ç¨³å®šä¼˜å…ˆçš„SAM LoRAåŒ…è£…å™¨
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
import json
from pathlib import Path

from lora.adapters import LoRALayer, LoRALinear
from core.sam_model_loader import SAMModelLoader


class StableSAMLoRAWrapper(nn.Module):
    """ç¨³å®šä¼˜å…ˆçš„SAM LoRAåŒ…è£…å™¨ - ç¡®ä¿100%å¯ç”¨æ€§"""
    
    def __init__(
        self,
        sam_model_loader: SAMModelLoader,
        lora_config: Dict[str, Any]
    ):
        super().__init__()
        
        self.sam_loader = sam_model_loader
        self.config = lora_config
        self.lora_modules = {}
        
        # è·å–SAMç»„ä»¶
        self.image_encoder = sam_model_loader.image_encoder
        self.prompt_encoder = sam_model_loader.prompt_encoder
        self.mask_decoder = sam_model_loader.mask_decoder
        
        # æ·»åŠ LoRAé€‚é…å™¨
        self._add_lora_to_sam()
        
        # è®¾ç½®è®­ç»ƒæ¨¡å¼
        self._setup_training_mode()
        
        # é¢„è®¡ç®—ç©ºæç¤ºï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        self._cache_empty_prompts()
    
    def _cache_empty_prompts(self):
        """ç¼“å­˜ç©ºæç¤ºä»¥æé«˜æ€§èƒ½"""
        try:
            with torch.no_grad():
                self.cached_sparse_embeddings, self.cached_dense_embeddings = self.prompt_encoder(
                    points=None, boxes=None, masks=None
                )
                
                # å°è¯•è·å–ä½ç½®ç¼–ç 
                try:
                    self.cached_image_pe = self.prompt_encoder.get_dense_pe()
                except:
                    self.cached_image_pe = None
                    
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•é¢„ç¼“å­˜ç©ºæç¤ºï¼Œå°†åœ¨è¿è¡Œæ—¶è®¡ç®—: {e}")
            self.cached_sparse_embeddings = None
            self.cached_dense_embeddings = None
            self.cached_image_pe = None
    
    def _add_lora_to_sam(self):
        """ä¸ºSAMçš„å„ä¸ªç»„ä»¶æ·»åŠ LoRAé€‚é…å™¨"""
        apply_lora_to = self.config.get('apply_lora_to', ['image_encoder'])
        
        print(f"å°†LoRAåº”ç”¨åˆ°: {apply_lora_to}")
        
        if 'image_encoder' in apply_lora_to and self.image_encoder is not None:
            self._add_lora_to_image_encoder()
        
        if 'prompt_encoder' in apply_lora_to and self.prompt_encoder is not None:
            self._add_lora_to_prompt_encoder()
        
        if 'mask_decoder' in apply_lora_to and self.mask_decoder is not None:
            self._add_lora_to_mask_decoder()
        
        print(f"æ€»è®¡æ·»åŠ äº† {len(self.lora_modules)} ä¸ªLoRAæ¨¡å—")
    
    def _add_lora_to_image_encoder(self):
        """ä¸ºå›¾åƒç¼–ç å™¨æ·»åŠ LoRA"""
        target_modules = self.config.get('target_modules', [
            'qkv', 'proj', 'mlp', 'fc1', 'fc2'
        ])
        
        lora_count = 0
        
        for name, module in self.image_encoder.named_modules():
            if isinstance(module, nn.Linear):
                if self._should_add_lora_to_module(name, target_modules):
                    lora_module = LoRALinear(
                        module,
                        rank=self.config.get('rank', 8),
                        alpha=self.config.get('alpha', 16.0),
                        dropout=self.config.get('dropout', 0.1)
                    )
                    
                    self._replace_module_in_image_encoder(name, lora_module)
                    self.lora_modules[f'image_encoder.{name}'] = lora_module
                    lora_count += 1
        
        print(f"å›¾åƒç¼–ç å™¨æ·»åŠ äº† {lora_count} ä¸ªLoRAæ¨¡å—")
    
    def _add_lora_to_prompt_encoder(self):
        """ä¸ºæç¤ºç¼–ç å™¨æ·»åŠ LoRA"""
        lora_count = 0
        for name, module in self.prompt_encoder.named_modules():
            if isinstance(module, nn.Linear):
                lora_module = LoRALinear(
                    module,
                    rank=self.config.get('rank', 4),
                    alpha=self.config.get('alpha', 8.0),
                    dropout=self.config.get('dropout', 0.1)
                )
                
                self._replace_module_in_prompt_encoder(name, lora_module)
                self.lora_modules[f'prompt_encoder.{name}'] = lora_module
                lora_count += 1
        
        print(f"æç¤ºç¼–ç å™¨æ·»åŠ äº† {lora_count} ä¸ªLoRAæ¨¡å—")
    
    def _add_lora_to_mask_decoder(self):
        """ä¸ºæ©ç è§£ç å™¨æ·»åŠ LoRA"""
        target_modules = self.config.get('target_modules', [
            'transformer', 'iou_prediction_head', 'mask_tokens'
        ])
        
        lora_count = 0
        for name, module in self.mask_decoder.named_modules():
            if isinstance(module, nn.Linear):
                if self._should_add_lora_to_module(name, target_modules):
                    lora_module = LoRALinear(
                        module,
                        rank=self.config.get('rank', 8),
                        alpha=self.config.get('alpha', 16.0),
                        dropout=self.config.get('dropout', 0.1)
                    )
                    
                    self._replace_module_in_mask_decoder(name, lora_module)
                    self.lora_modules[f'mask_decoder.{name}'] = lora_module
                    lora_count += 1
        
        print(f"æ©ç è§£ç å™¨æ·»åŠ äº† {lora_count} ä¸ªLoRAæ¨¡å—")
    
    def _should_add_lora_to_module(self, module_name: str, target_modules: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºè¯¥æ¨¡å—æ·»åŠ LoRA
        
        Args:
            module_name (str): æ¨¡å—çš„åç§°è·¯å¾„ï¼Œä¾‹å¦‚ 'transformer.layers.0.self_attn.q_proj'
            target_modules (List[str]): ç›®æ ‡æ¨¡å—åˆ—è¡¨ï¼Œä¾‹å¦‚ ['transformer', 'iou_prediction_head']
        
        Returns:
            bool: å¦‚æœæ¨¡å—åç§°åŒ…å«ä»»ä½•ç›®æ ‡æ¨¡å—åç§°åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not target_modules:
            return True
        return any(target in module_name for target in target_modules)
    
    def _replace_module_in_image_encoder(self, module_path: str, new_module: nn.Module):
        """åœ¨å›¾åƒç¼–ç å™¨ä¸­æ›¿æ¢æ¨¡å—"""
        self._replace_module(self.image_encoder, module_path, new_module)
    
    def _replace_module_in_prompt_encoder(self, module_path: str, new_module: nn.Module):
        """åœ¨æç¤ºç¼–ç å™¨ä¸­æ›¿æ¢æ¨¡å—"""
        self._replace_module(self.prompt_encoder, module_path, new_module)
    
    def _replace_module_in_mask_decoder(self, module_path: str, new_module: nn.Module):
        """åœ¨æ©ç è§£ç å™¨ä¸­æ›¿æ¢æ¨¡å—"""
        self._replace_module(self.mask_decoder, module_path, new_module)
    
    def _replace_module(self, parent_module: nn.Module, module_path: str, new_module: nn.Module):
        """åœ¨çˆ¶æ¨¡å—ä¸­æ›¿æ¢æŒ‡å®šè·¯å¾„çš„å­æ¨¡å—"""
        path_parts = module_path.split('.')
        current_module = parent_module
        
        for part in path_parts[:-1]:
            current_module = getattr(current_module, part)
        
        setattr(current_module, path_parts[-1], new_module)
    
    def _setup_training_mode(self):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        if self.config.get('freeze_image_encoder', True):
            for param in self.image_encoder.parameters():
                param.requires_grad = False
            for name, module in self.lora_modules.items():
                if 'image_encoder' in name and hasattr(module, 'lora'):
                    for param in module.lora.parameters():
                        param.requires_grad = True
        
        if self.config.get('freeze_prompt_encoder', True):
            for param in self.prompt_encoder.parameters():
                param.requires_grad = False
            for name, module in self.lora_modules.items():
                if 'prompt_encoder' in name and hasattr(module, 'lora'):
                    for param in module.lora.parameters():
                        param.requires_grad = True
        
        if self.config.get('freeze_mask_decoder', False):
            for param in self.mask_decoder.parameters():
                param.requires_grad = False
            for name, module in self.lora_modules.items():
                if 'mask_decoder' in name and hasattr(module, 'lora'):
                    for param in module.lora.parameters():
                        param.requires_grad = True

    def forward(self, batch_inputs: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """ç¨³å®šä¼˜å…ˆçš„å‰å‘ä¼ æ’­ - 100%å¯ç”¨æ€§ä¿è¯"""
        try:
            images = batch_inputs['images']  # [B, C, H, W]
            device = images.device
            batch_size = images.shape[0]
            
            # ç¡®ä¿æ‰€æœ‰ç»„ä»¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            self._ensure_device_consistency(device)
            
            # ğŸ”§ å›¾åƒç¼–ç  - è¿™ä¸ªéƒ¨åˆ†é€šå¸¸æ˜¯ç¨³å®šçš„
            with torch.amp.autocast('cuda', enabled=device.type == 'cuda'):
                image_embeddings = self.image_encoder(images)  # [B, 256, 64, 64]
            
            # ğŸ¯ æ ¸å¿ƒç­–ç•¥ï¼šä¼˜å…ˆç¨³å®šæ€§ï¼Œä½¿ç”¨é€ä¸ªå¤„ç†
            return self._stable_mask_decode(image_embeddings, device)
            
        except Exception as e:
            print(f"å‰å‘ä¼ æ’­å¼‚å¸¸: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤è¾“å‡º
            return self._create_safe_output(batch_inputs['images'])
    
    def _stable_mask_decode(self, image_embeddings: torch.Tensor, device: torch.device) -> Dict[str, torch.Tensor]:
        """ç¨³å®šçš„æ©ç è§£ç  - ç¡®ä¿æ‰€æœ‰æƒ…å†µéƒ½èƒ½æˆåŠŸ"""
        batch_size = image_embeddings.shape[0]
        
        # ğŸ¯ ç­–ç•¥ï¼šå§‹ç»ˆä½¿ç”¨ç¨³å®šçš„é€ä¸ªå¤„ç†
        # è¿™æ ·ç¡®ä¿100%æˆåŠŸç‡ï¼Œè™½ç„¶ä¸æ˜¯çœŸæ­£çš„æ‰¹é‡ï¼Œä½†æ¥å£ä¿æŒä¸€è‡´
        
        if batch_size == 1:
            # å•ä¸ªæ ·æœ¬ï¼Œç›´æ¥å¤„ç†
            return self._process_single_sample(image_embeddings[0:1], device)
        else:
            # å¤šä¸ªæ ·æœ¬ï¼Œé€ä¸ªå¤„ç†ååˆå¹¶
            return self._process_multiple_samples(image_embeddings, device)
    
    def _process_single_sample(self, image_embedding: torch.Tensor, device: torch.device) -> Dict[str, torch.Tensor]:
        """å¤„ç†å•ä¸ªæ ·æœ¬"""
        try:
            # è·å–æç¤ºåµŒå…¥
            sparse_embeddings, dense_embeddings, image_pe = self._get_prompt_embeddings(device)
            
            # æ©ç è§£ç 
            with torch.amp.autocast('cuda', enabled=device.type == 'cuda'):
                masks, iou_predictions = self._safe_mask_decoder_call(
                    image_embedding, image_pe, sparse_embeddings, dense_embeddings
                )
            
            return {
                'masks': masks,
                'iou_predictions': iou_predictions
            }
            
        except Exception as e:
            print(f"å•æ ·æœ¬å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¾“å‡º
            return {
                'masks': torch.zeros(1, 1, 256, 256, device=device),
                'iou_predictions': torch.zeros(1, 1, device=device)
            }
    
    def _process_multiple_samples(self, image_embeddings: torch.Tensor, device: torch.device) -> Dict[str, torch.Tensor]:
        """é€ä¸ªå¤„ç†å¤šä¸ªæ ·æœ¬"""
        batch_size = image_embeddings.shape[0]
        
        # é¢„åˆ†é…è¾“å‡ºå¼ é‡
        all_masks = torch.zeros(batch_size, 1, 256, 256, device=device)
        all_iou = torch.zeros(batch_size, 1, device=device)
        
        # è·å–å…±äº«çš„æç¤ºåµŒå…¥ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        sparse_embeddings, dense_embeddings, image_pe = self._get_prompt_embeddings(device)
        
        # é€ä¸ªå¤„ç†
        for i in range(batch_size):
            try:
                single_embedding = image_embeddings[i:i+1]  # ä¿æŒæ‰¹é‡ç»´åº¦
                
                with torch.amp.autocast('cuda', enabled=device.type == 'cuda'):
                    masks, iou_predictions = self._safe_mask_decoder_call(
                        single_embedding, image_pe, sparse_embeddings, dense_embeddings
                    )
                
                all_masks[i] = masks[0]
                all_iou[i] = iou_predictions[0]
                
            except Exception as e:
                print(f"æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                # ä¿æŒé»˜è®¤çš„é›¶å€¼
                continue
        
        return {
            'masks': all_masks,
            'iou_predictions': all_iou
        }
    
    def _get_prompt_embeddings(self, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
        """è·å–æç¤ºåµŒå…¥ï¼ˆä½¿ç”¨ç¼“å­˜æˆ–å®æ—¶è®¡ç®—ï¼‰"""
        try:
            # å°è¯•ä½¿ç”¨ç¼“å­˜
            if (self.cached_sparse_embeddings is not None and 
                self.cached_dense_embeddings is not None):
                
                sparse_embeddings = self.cached_sparse_embeddings.to(device)
                dense_embeddings = self.cached_dense_embeddings.to(device)
                image_pe = self.cached_image_pe.to(device) if self.cached_image_pe is not None else None
                
                return sparse_embeddings, dense_embeddings, image_pe
            else:
                # å®æ—¶è®¡ç®—
                sparse_embeddings, dense_embeddings = self.prompt_encoder(
                    points=None, boxes=None, masks=None
                )
                
                try:
                    image_pe = self.prompt_encoder.get_dense_pe().to(device)
                except:
                    image_pe = torch.zeros(1, 256, 64, 64, device=device)
                
                return sparse_embeddings, dense_embeddings, image_pe
                
        except Exception as e:
            print(f"è·å–æç¤ºåµŒå…¥å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return (
                torch.zeros(1, 0, 256, device=device),  # empty sparse
                torch.zeros(1, 256, 64, 64, device=device),  # empty dense
                torch.zeros(1, 256, 64, 64, device=device)   # default pe
            )
    
    def _safe_mask_decoder_call(self, image_embeddings, image_pe, sparse_embeddings, dense_embeddings):
        """å®‰å…¨çš„æ©ç è§£ç å™¨è°ƒç”¨"""
        
        # å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼ï¼Œç¡®ä¿è‡³å°‘æœ‰ä¸€ç§æˆåŠŸ
        
        # å°è¯•1: å®Œæ•´å‚æ•°è°ƒç”¨
        try:
            return self.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=image_pe,
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=False
            )
        except Exception as e1:
            pass
        
        # å°è¯•2: ä¸ä½¿ç”¨image_pe
        try:
            return self.mask_decoder(
                image_embeddings=image_embeddings,
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=False
            )
        except Exception as e2:
            pass
        
        # å°è¯•3: æœ€å°å‚æ•°é›†
        try:
            return self.mask_decoder(
                image_embeddings=image_embeddings,
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings
            )
        except Exception as e3:
            pass
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise RuntimeError("æ‰€æœ‰æ©ç è§£ç å™¨è°ƒç”¨æ–¹å¼éƒ½å¤±è´¥")
    
    def _ensure_device_consistency(self, device: torch.device):
        """ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        if self.image_encoder is not None:
            self.image_encoder = self.image_encoder.to(device)
        
        if self.prompt_encoder is not None:
            self.prompt_encoder = self.prompt_encoder.to(device)
        
        if self.mask_decoder is not None:
            self.mask_decoder = self.mask_decoder.to(device)
        
        # ç§»åŠ¨LoRAæ¨¡å—
        for name, lora_module in self.lora_modules.items():
            if hasattr(lora_module, 'lora'):
                lora_module.lora = lora_module.lora.to(device)
            lora_module = lora_module.to(device)
    
    def _create_safe_output(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """åˆ›å»ºå®‰å…¨çš„é»˜è®¤è¾“å‡º"""
        batch_size = images.shape[0]
        device = images.device
        
        return {
            'masks': torch.zeros(batch_size, 1, 256, 256, device=device),
            'iou_predictions': torch.zeros(batch_size, 1, device=device)
        }
    
    # === ä»¥ä¸‹æ˜¯ä¿æŒä¸å˜çš„æ–¹æ³• ===
    def save_lora_weights(self, save_path: str):
        """ä¿å­˜LoRAæƒé‡"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        lora_state_dict = {}
        for name, module in self.lora_modules.items():
            if hasattr(module, 'lora'):
                lora_state_dict[f"{name}.lora_A.weight"] = module.lora.lora_A.weight
                lora_state_dict[f"{name}.lora_B.weight"] = module.lora.lora_B.weight
                if module.lora.lora_B.bias is not None:
                    lora_state_dict[f"{name}.lora_B.bias"] = module.lora.lora_B.bias
        
        torch.save(lora_state_dict, save_path / "sam_lora_weights.pth")
        
        config_to_save = self.config.copy()
        config_to_save['model_type'] = self.sam_loader.model_type
        
        with open(save_path / "sam_lora_config.json", 'w') as f:
            json.dump(config_to_save, f, indent=2)
        
        print(f"SAM LoRAæƒé‡å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_lora_weights(self, load_path: str):
        """åŠ è½½LoRAæƒé‡"""
        load_path = Path(load_path)
        
        weights_file = load_path / "sam_lora_weights.pth"
        if weights_file.exists():
            lora_state_dict = torch.load(weights_file, map_location='cpu')
            
            for name, module in self.lora_modules.items():
                if hasattr(module, 'lora'):
                    if f"{name}.lora_A.weight" in lora_state_dict:
                        module.lora.lora_A.weight.data = lora_state_dict[f"{name}.lora_A.weight"]
                    if f"{name}.lora_B.weight" in lora_state_dict:
                        module.lora.lora_B.weight.data = lora_state_dict[f"{name}.lora_B.weight"]
                    if f"{name}.lora_B.bias" in lora_state_dict:
                        module.lora.lora_B.bias.data = lora_state_dict[f"{name}.lora_B.bias"]
            
            print(f"SAM LoRAæƒé‡å·²ä» {load_path} åŠ è½½")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶ {weights_file}")
    
    def merge_and_save_full_model(self, save_path: str):
        """åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜å®Œæ•´SAMæ¨¡å‹"""
        for module in self.lora_modules.values():
            if hasattr(module, 'merge_weights'):
                module.merge_weights()
        
        self.sam_loader.save_model_state(save_path)
        print(f"åˆå¹¶çš„SAMæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
        for module in self.lora_modules.values():
            if hasattr(module, 'unmerge_weights'):
                module.unmerge_weights()
    
    def get_trainable_parameters(self) -> Dict[str, int]:
        """è·å–å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
        total_params = 0
        trainable_params = 0
        lora_params = 0
        
        for name, param in self.named_parameters():
            total_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
                if 'lora' in name.lower():
                    lora_params += param.numel()
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'lora_parameters': lora_params,
            'trainable_percentage': 100 * trainable_params / total_params if total_params > 0 else 0,
            'lora_percentage': 100 * lora_params / total_params if total_params > 0 else 0
        }
    
    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        info = self.get_trainable_parameters()
        print(f"\nç¨³å®šç‰ˆSAM LoRAæ¨¡å‹ä¿¡æ¯:")
        print(f"  æ€»å‚æ•°æ•°: {info['total_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ•°: {info['trainable_parameters']:,}")
        print(f"  LoRAå‚æ•°æ•°: {info['lora_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {info['trainable_percentage']:.2f}%")
        print(f"  LoRAå‚æ•°æ¯”ä¾‹: {info['lora_percentage']:.2f}%")
        print(f"  LoRAæ¨¡å—æ•°: {len(self.lora_modules)}")
        
        component_counts = {}
        for name in self.lora_modules.keys():
            component = name.split('.')[0]
            component_counts[component] = component_counts.get(component, 0) + 1
        
        print(f"  LoRAæ¨¡å—åˆ†å¸ƒ:")
        for component, count in component_counts.items():
            print(f"    {component}: {count} ä¸ªæ¨¡å—")


# === ä¾¿æ·å‡½æ•° ===
def create_stable_sam_lora_model(model_type: str, lora_config: Dict[str, Any], device: str = "cuda") -> Optional[StableSAMLoRAWrapper]:
    """åˆ›å»ºç¨³å®šç‰ˆæœ¬çš„SAM LoRAæ¨¡å‹"""
    try:
        from core.sam_model_loader import load_sam_for_training
        
        sam_loader = load_sam_for_training(
            model_type=model_type,
            device=device,
            freeze_image_encoder=lora_config.get('freeze_image_encoder', True),
            freeze_prompt_encoder=lora_config.get('freeze_prompt_encoder', True),
            freeze_mask_decoder=lora_config.get('freeze_mask_decoder', False)
        )
        
        if sam_loader is None:
            print("SAMæ¨¡å‹åŠ è½½å¤±è´¥")
            return None
        
        sam_lora_model = StableSAMLoRAWrapper(sam_loader, lora_config)
        return sam_lora_model
        
    except Exception as e:
        print(f"åˆ›å»ºç¨³å®šç‰ˆSAM LoRAæ¨¡å‹å¤±è´¥: {e}")
        return None


def load_stable_sam_lora_model(model_type: str, lora_path: str, device: str = "cuda") -> Optional[StableSAMLoRAWrapper]:
    """åŠ è½½ç¨³å®šç‰ˆæœ¬çš„å·²è®­ç»ƒSAM LoRAæ¨¡å‹"""
    try:
        lora_path = Path(lora_path)
        
        config_file = lora_path / "sam_lora_config.json"
        if config_file.exists():
            with open(config_file, 'r') as f:
                lora_config = json.load(f)
        else:
            lora_config = {
                'rank': 8, 
                'alpha': 16.0, 
                'dropout': 0.1,
                'apply_lora_to': ['image_encoder']
            }
            print("ä½¿ç”¨é»˜è®¤LoRAé…ç½®")
        
        sam_lora_model = create_stable_sam_lora_model(model_type, lora_config, device)
        
        if sam_lora_model is None:
            return None
        
        sam_lora_model.load_lora_weights(lora_path)
        return sam_lora_model
        
    except Exception as e:
        print(f"åŠ è½½ç¨³å®šç‰ˆSAM LoRAæ¨¡å‹å¤±è´¥: {e}")
        return None