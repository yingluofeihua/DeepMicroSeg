"""
LoRAè®­ç»ƒä¸»å…¥å£æ–‡ä»¶ (ä¿®æ”¹ç‰ˆ)
æ”¯æŒLoRAå¾®è°ƒå’Œè¯„æµ‹çš„å®Œæ•´æµç¨‹
ä½¿ç”¨æ–°çš„SAMæ¨¡å‹æ¶æ„
"""

import sys
import argparse
from pathlib import Path
import json
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from config.lora_config import LoRATrainingSettings, LORA_PRESET_CONFIGS, get_config_for_model
from core.lora_trainer import LoRATrainer, create_trainer_from_config, resume_training
from core.evaluator import BatchEvaluator
from core.dataset_manager import DatasetManager
from config.settings import BatchEvaluationSettings
from lora.data_loaders import split_dataset
from utils.file_utils import setup_logging
from utils.model_utils import get_device_info, optimize_memory


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="SAM LoRAå¾®è°ƒè®­ç»ƒç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¿«é€Ÿè®­ç»ƒ
  python lora_main.py train --preset quick --data-dir /path/to/data
  
  # æ ‡å‡†è®­ç»ƒ
  python lora_main.py train --data-dir /path/to/data --model vit_b_lm --epochs 10
  
  # ä»é…ç½®æ–‡ä»¶è®­ç»ƒ
  python lora_main.py train --config config.json
  
  # æ¢å¤è®­ç»ƒ
  python lora_main.py resume --checkpoint /path/to/checkpoint.pth
  
  # è¯„æµ‹LoRAæ¨¡å‹
  python lora_main.py evaluate --lora-model /path/to/lora --data-dir /path/to/data
  
  # è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹
  python lora_main.py train-and-eval --data-dir /path/to/data --eval-data /path/to/eval
  
  # å‡†å¤‡æ•°æ®
  python lora_main.py prepare-data --data-dir /path/to/data
        """
    )
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è®­ç»ƒå‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒLoRAæ¨¡å‹')
    add_train_arguments(train_parser)
    
    # æ¢å¤è®­ç»ƒå‘½ä»¤
    resume_parser = subparsers.add_parser('resume', help='æ¢å¤è®­ç»ƒ')
    resume_parser.add_argument('--checkpoint', required=True, help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    resume_parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # è¯„æµ‹å‘½ä»¤
    eval_parser = subparsers.add_parser('evaluate', help='è¯„æµ‹LoRAæ¨¡å‹')
    add_eval_arguments(eval_parser)
    
    # è®­ç»ƒ+è¯„æµ‹å‘½ä»¤
    train_eval_parser = subparsers.add_parser('train-and-eval', help='è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹')
    add_train_arguments(train_eval_parser)
    add_eval_arguments(train_eval_parser)
    
    # æ•°æ®å‡†å¤‡å‘½ä»¤
    data_parser = subparsers.add_parser('prepare-data', help='å‡†å¤‡è®­ç»ƒæ•°æ®')
    data_parser.add_argument('--data-dir', required=True, help='æ•°æ®ç›®å½•')
    data_parser.add_argument('--train-ratio', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹')
    data_parser.add_argument('--val-ratio', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹')
    
    # æ¨¡å‹ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser('info', help='æ˜¾ç¤ºæ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯')
    info_parser.add_argument('--model', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                            default='vit_b_lm', help='æ¨¡å‹ç±»å‹')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    return parser.parse_args()


def add_train_arguments(parser):
    """æ·»åŠ è®­ç»ƒç›¸å…³å‚æ•°"""
    # æ•°æ®é…ç½®
    parser.add_argument('--data-dir', required=True, help='è®­ç»ƒæ•°æ®ç›®å½•')
    parser.add_argument('--val-data-dir', help='éªŒè¯æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', default='./data/lora_experiments', help='è¾“å‡ºç›®å½•')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                       default='vit_b_lm', help='åŸºç¡€æ¨¡å‹')
    parser.add_argument('--preset', choices=list(LORA_PRESET_CONFIGS.keys()), 
                       help='é¢„è®¾é…ç½®')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # LoRAé…ç½®
    parser.add_argument('--rank', type=int, default=8, help='LoRA rank')
    parser.add_argument('--alpha', type=float, default=16.0, help='LoRA alpha')
    parser.add_argument('--dropout', type=float, default=0.1, help='LoRA dropout')
    parser.add_argument('--lora-target', choices=['image_encoder', 'mask_decoder', 'both'],
                       default='image_encoder', help='LoRAåº”ç”¨ç›®æ ‡')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    
    # å®éªŒé…ç½®
    parser.add_argument('--experiment-name', default='sam_lora_finetune', help='å®éªŒåç§°')
    parser.add_argument('--use-wandb', action='store_true', help='ä½¿ç”¨Weights & Biases')
    parser.add_argument('--wandb-project', default='sam_lora_training', help='W&Bé¡¹ç›®å')

    # æ·»åŠ ä¿å­˜ç›¸å…³å‚æ•°
    parser.add_argument('--save-steps', type=int, default=500, help='ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--eval-steps', type=int, default=100, help='éªŒè¯çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--logging-steps', type=int, default=50, help='æ—¥å¿—è®°å½•çš„æ­¥æ•°é—´éš”')
    # æ·»åŠ ç»†èƒç±»å‹è¿‡æ»¤å‚æ•°
    parser.add_argument('--cell-types', nargs='+', help='è¦è®­ç»ƒçš„ç»†èƒç±»å‹ï¼Œå¦‚: --cell-types 293T MSC')

def add_eval_arguments(parser):
    """æ·»åŠ è¯„æµ‹ç›¸å…³å‚æ•°"""
    parser.add_argument('--lora-model', help='LoRAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--eval-data', help='è¯„æµ‹æ•°æ®ç›®å½•')
    parser.add_argument('--eval-output', help='è¯„æµ‹ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--compare-baseline', action='store_true', help='ä¸åŸºç¡€æ¨¡å‹å¯¹æ¯”')


def create_config_from_args(args) -> LoRATrainingSettings:
    """ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®"""
    
    # ä½¿ç”¨é¢„è®¾é…ç½®
    if hasattr(args, 'preset') and args.preset:
        config = LORA_PRESET_CONFIGS[args.preset]
        print(f"ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    elif hasattr(args, 'config') and args.config:
        config = LoRATrainingSettings.from_json(args.config)
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
    
    # ä¸ºç‰¹å®šæ¨¡å‹åˆ›å»ºé…ç½®
    elif hasattr(args, 'model'):
        config = get_config_for_model(args.model)
        print(f"ä¸ºæ¨¡å‹ {args.model} åˆ›å»ºé…ç½®")
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    else:
        config = LoRATrainingSettings()
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # æ›´æ–°é…ç½®
    if hasattr(args, 'data_dir'):
        config.data.train_data_dir = args.data_dir
    
    if hasattr(args, 'val_data_dir') and args.val_data_dir:
        config.data.val_data_dir = args.val_data_dir
    
    if hasattr(args, 'output_dir'):
        config.experiment.output_dir = args.output_dir
    
    if hasattr(args, 'model'):
        config.model.base_model_name = args.model
    
    # LoRAå‚æ•°
    if hasattr(args, 'rank'):
        config.lora.rank = args.rank
    if hasattr(args, 'alpha'):
        config.lora.alpha = args.alpha
    if hasattr(args, 'dropout'):
        config.lora.dropout = args.dropout
    
    # LoRAç›®æ ‡è®¾ç½®
    if hasattr(args, 'lora_target'):
        if args.lora_target == 'image_encoder':
            config.model.apply_lora_to = ['image_encoder']
        elif args.lora_target == 'mask_decoder':
            config.model.apply_lora_to = ['mask_decoder']
        elif args.lora_target == 'both':
            config.model.apply_lora_to = ['image_encoder', 'mask_decoder']
    
    # è®­ç»ƒå‚æ•°
    if hasattr(args, 'epochs'):
        config.training.num_epochs = args.epochs
    if hasattr(args, 'batch_size'):
        config.training.batch_size = args.batch_size
    if hasattr(args, 'learning_rate'):
        config.training.learning_rate = args.learning_rate
    if hasattr(args, 'weight_decay'):
        config.training.weight_decay = args.weight_decay
    if hasattr(args, 'save_steps'):
        config.training.save_steps = args.save_steps
    if hasattr(args, 'eval_steps'):
        config.training.eval_steps = args.eval_steps  
    if hasattr(args, 'logging_steps'):
        config.training.logging_steps = args.logging_steps
    
    # å®éªŒé…ç½®
    if hasattr(args, 'experiment_name'):
        config.experiment.experiment_name = args.experiment_name
    if hasattr(args, 'use_wandb'):
        config.experiment.use_wandb = args.use_wandb
    if hasattr(args, 'wandb_project'):
        config.experiment.wandb_project = args.wandb_project

    # æ·»åŠ ç»†èƒç±»å‹è¿‡æ»¤
    if hasattr(args, 'cell_types') and args.cell_types:
        config._cell_types_filter = args.cell_types
    else:
        config._cell_types_filter = None
    
    # è°ƒè¯•æ¨¡å¼
    if hasattr(args, 'debug') and args.debug:
        config.experiment.debug_mode = True
        config.training.num_epochs = 2
        config.training.batch_size = 2
        config.training.save_steps = 10
        config.training.eval_steps = 5
        config.training.logging_steps = 1
        print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    return config


def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
    device_info = get_device_info()
    print(f"CUDAå¯ç”¨: {device_info['cuda_available']}")
    
    if device_info['cuda_available']:
        print(f"GPUæ•°é‡: {device_info['cuda_device_count']}")
        for gpu_name, gpu_info in device_info['gpu_memory'].items():
            print(f"  {gpu_name}: {gpu_info['name']}, "
                  f"å†…å­˜: {gpu_info['total_memory'] / 1e9:.1f} GB")
    
    # æ£€æŸ¥å†…å­˜
    cpu_memory = device_info['cpu_memory']
    print(f"CPUå†…å­˜: {cpu_memory['total'] / 1e9:.1f} GB "
          f"(å¯ç”¨: {cpu_memory['available'] / 1e9:.1f} GB)")
    
    # æ£€æŸ¥micro_sam
    try:
        import micro_sam
        print(f"micro_samå·²å®‰è£…")
    except ImportError:
        print("è­¦å‘Š: micro_samæœªå®‰è£…ï¼Œå¯èƒ½å½±å“æ¨¡å‹åŠ è½½")
    
    print("ç³»ç»Ÿæ£€æŸ¥å®Œæˆ\n")


def train_lora_model(args) -> str:
    """è®­ç»ƒLoRAæ¨¡å‹"""
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†åˆ«è®­ç»ƒå¤šä¸ªç»†èƒç±»å‹
    if hasattr(args, 'cell_types') and args.cell_types and len(args.cell_types) > 1:
        return train_multiple_cell_types(args)
    
    # åŸæ¥çš„å•æ¨¡å‹è®­ç»ƒé€»è¾‘
    print("="*60)
    print("å¼€å§‹SAM LoRAå¾®è°ƒè®­ç»ƒ")
    print("="*60)
    
    check_system_requirements()
    config = create_config_from_args(args)
    
    if not config.validate():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return None
    
    # å¦‚æœæ˜¯å•ä¸ªç»†èƒç±»å‹ï¼Œæ·»åŠ åˆ°å®éªŒåç§°ä¸­
    if hasattr(args, 'cell_types') and args.cell_types and len(args.cell_types) == 1:
        cell_type = args.cell_types[0]
        config.experiment.experiment_name = f"sam_lora_{cell_type.lower()}"
        config.experiment.output_dir = f"{config.experiment.output_dir}_{cell_type.lower()}"
        print(f"è®­ç»ƒç»†èƒç±»å‹: {cell_type}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  åŸºç¡€æ¨¡å‹: {config.model.base_model_name}")
    print(f"  LoRAé…ç½®:")
    print(f"    rank: {config.lora.rank}")
    print(f"    alpha: {config.lora.alpha}")
    print(f"    dropout: {config.lora.dropout}")
    print(f"    åº”ç”¨åˆ°: {config.model.apply_lora_to}")
    print(f"  è®­ç»ƒé…ç½®:")
    print(f"    å­¦ä¹ ç‡: {config.training.learning_rate}")
    print(f"    æ‰¹å¤§å°: {config.training.batch_size}")
    print(f"    è®­ç»ƒè½®æ•°: {config.training.num_epochs}")
    print(f"  è¾“å‡ºç›®å½•: {config.experiment.output_dir}")
    print(f"  æ•°æ®ç›®å½•: {config.data.train_data_dir}")
    
    trainer = LoRATrainer(config)
    success = trainer.train()
    
    if success:
        model_path = trainer.output_dir / "final_model"
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        return str(model_path)
    else:
        print("\nè®­ç»ƒå¤±è´¥!")
        return None


def train_multiple_cell_types(args) -> str:
    """ä¸ºå¤šä¸ªç»†èƒç±»å‹åˆ†åˆ«è®­ç»ƒæ¨¡å‹"""
    print("="*60)
    print("å¼€å§‹å¤šç»†èƒç±»å‹åˆ†åˆ«è®­ç»ƒ")
    print("="*60)
    print(f"å°†è®­ç»ƒçš„ç»†èƒç±»å‹: {', '.join(args.cell_types)}")
    
    results = {}
    
    for cell_type in args.cell_types:
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ {cell_type} æ¨¡å‹...")
        
        # åˆ›å»ºå•ä¸ªç»†èƒç±»å‹çš„å‚æ•°å‰¯æœ¬
        single_args = type(args)()
        for attr in dir(args):
            if not attr.startswith('_'):
                setattr(single_args, attr, getattr(args, attr))
        
        # è®¾ç½®ä¸ºå•ä¸ªç»†èƒç±»å‹
        single_args.cell_types = [cell_type]
        
        # è®­ç»ƒ
        model_path = train_lora_model(single_args)
        results[cell_type] = model_path
        
        if model_path:
            print(f"âœ… {cell_type} è®­ç»ƒå®Œæˆ: {model_path}")
        else:
            print(f"âŒ {cell_type} è®­ç»ƒå¤±è´¥")
        
        # æ¸…ç†GPUå†…å­˜
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # ç”Ÿæˆæ‘˜è¦
    print(f"\n{'='*60}")
    print("å¤šç»†èƒç±»å‹è®­ç»ƒå®Œæˆ")
    print(f"{'='*60}")
    
    successful = sum(1 for path in results.values() if path)
    print(f"æˆåŠŸè®­ç»ƒ: {successful}/{len(args.cell_types)}")
    
    for cell_type, path in results.items():
        if path:
            print(f"  âœ… {cell_type}: {path}")
        else:
            print(f"  âŒ {cell_type}: å¤±è´¥")
    
    return f"å¤šç»†èƒç±»å‹è®­ç»ƒå®Œæˆï¼ŒæˆåŠŸ: {successful}/{len(args.cell_types)}"


def resume_lora_training(args) -> str:
    """æ¢å¤LoRAè®­ç»ƒ"""
    print("="*60)
    print("æ¢å¤SAM LoRAè®­ç»ƒ")
    print("="*60)
    
    trainer = resume_training(args.checkpoint, args.config)
    success = trainer.train()
    
    if success:
        model_path = trainer.output_dir / "final_model"
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        return str(model_path)
    else:
        print("\nè®­ç»ƒå¤±è´¥!")
        return None


def evaluate_lora_model(args, lora_model_path: str = None) -> bool:
    """è¯„æµ‹LoRAæ¨¡å‹"""
    print("="*60)
    print("å¼€å§‹SAM LoRAæ¨¡å‹è¯„æµ‹")
    print("="*60)
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if lora_model_path is None:
        lora_model_path = args.lora_model
    
    if not lora_model_path:
        print("é”™è¯¯: æœªæŒ‡å®šLoRAæ¨¡å‹è·¯å¾„")
        return False
    
    # ç¡®å®šè¯„æµ‹æ•°æ®
    eval_data_dir = getattr(args, 'eval_data', None) or getattr(args, 'data_dir', None)
    if not eval_data_dir:
        print("é”™è¯¯: æœªæŒ‡å®šè¯„æµ‹æ•°æ®ç›®å½•")
        return False
    
    try:
        # åŠ è½½LoRAæ¨¡å‹è¿›è¡Œè¯„æµ‹
        from lora.sam_lora_wrapper import load_sam_lora_model
        from core.metrics import ComprehensiveMetrics
        from lora.data_loaders import create_data_loaders
        from config.lora_config import DataConfig
        
        print(f"LoRAæ¨¡å‹è·¯å¾„: {lora_model_path}")
        print(f"è¯„æµ‹æ•°æ®ç›®å½•: {eval_data_dir}")
        
        # åˆ›å»ºè¯„æµ‹æ•°æ®åŠ è½½å™¨
        data_config = DataConfig()
        data_config.test_data_dir = eval_data_dir
        
        data_loaders = create_data_loaders(data_config, dataset_type="sam")
        if 'test' not in data_loaders:
            print("æ— æ³•åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨")
            return False
        
        test_loader = data_loaders['test']
        print(f"æµ‹è¯•æ•°æ®: {len(test_loader)} æ‰¹æ¬¡")
        
        # åŠ è½½LoRAæ¨¡å‹
        model_type = "vit_b_lm"  # é»˜è®¤æ¨¡å‹ç±»å‹ï¼Œå¯ä»¥ä»é…ç½®ä¸­è¯»å–
        lora_model = load_sam_lora_model(model_type, lora_model_path)
        
        if lora_model is None:
            print("LoRAæ¨¡å‹åŠ è½½å¤±è´¥")
            return False
        
        print("LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
        metrics_calculator = ComprehensiveMetrics()
        
        # è¿›è¡Œè¯„æµ‹
        lora_model.eval()
        all_results = []
        
        print("å¼€å§‹è¯„æµ‹...")
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                if batch_idx >= 10:  # é™åˆ¶è¯„æµ‹æ•°é‡
                    break
                
                try:
                    # å‡†å¤‡è¾“å…¥
                    from lora.training_utils import prepare_sam_inputs
                    inputs, targets = prepare_sam_inputs(batch)
                    
                    # æ¨¡å‹é¢„æµ‹
                    predictions = lora_model(inputs)
                    
                    # è®¡ç®—æŒ‡æ ‡
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    target_masks = targets['masks'].cpu().numpy()
                    
                    for pred, target in zip(pred_masks, target_masks):
                        if pred.ndim > 2:
                            pred = pred[0]
                        if target.ndim > 2:
                            target = target[0]
                        
                        pred_binary = (pred > 0.5).astype(int)
                        target_binary = (target > 0.5).astype(int)
                        
                        result = metrics_calculator.compute_all_metrics(target_binary, pred_binary)
                        all_results.append(result.to_dict())
                
                except Exception as e:
                    print(f"è¯„æµ‹æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if all_results:
            avg_metrics = {}
            for key in all_results[0].keys():
                values = [r[key] for r in all_results if key in r and r[key] is not None]
                if values:
                    if key == 'hd95':
                        finite_values = [v for v in values if not (v == float('inf') or v != v)]
                        avg_metrics[key] = sum(finite_values) / len(finite_values) if finite_values else float('inf')
                    else:
                        avg_metrics[key] = sum(values) / len(values)
            
            print(f"\nè¯„æµ‹ç»“æœ (åŸºäº {len(all_results)} ä¸ªæ ·æœ¬):")
            for key, value in avg_metrics.items():
                print(f"  {key}: {value:.4f}")
            
            # ä¿å­˜ç»“æœ
            if hasattr(args, 'eval_output') and args.eval_output:
                output_dir = Path(args.eval_output)
                output_dir.mkdir(parents=True, exist_ok=True)
                
                results_file = output_dir / "lora_evaluation_results.json"
                with open(results_file, 'w') as f:
                    json.dump({
                        'average_metrics': avg_metrics,
                        'individual_results': all_results,
                        'model_path': lora_model_path,
                        'data_path': eval_data_dir
                    }, f, indent=2)
                
                print(f"è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        else:
            print("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ç»“æœ")
            return False
        
        print("LoRAæ¨¡å‹è¯„æµ‹å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"LoRAæ¨¡å‹è¯„æµ‹å¤±è´¥: {e}")
        if hasattr(args, 'verbose') and args.verbose:
            import traceback
            traceback.print_exc()
        return False


def prepare_training_data(args):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("="*60)
    print("å‡†å¤‡è®­ç»ƒæ•°æ®")
    print("="*60)
    
    data_dir = args.data_dir
    train_ratio = args.train_ratio
    val_ratio = args.val_ratio
    
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"åˆ†å‰²æ¯”ä¾‹ - è®­ç»ƒ: {train_ratio}, éªŒè¯: {val_ratio}, æµ‹è¯•: {1-train_ratio-val_ratio}")
    
    try:
        split_dataset(data_dir, train_ratio, val_ratio)
        print("æ•°æ®å‡†å¤‡å®Œæˆ!")
    except Exception as e:
        print(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def show_model_info(args):
    """æ˜¾ç¤ºæ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯"""
    print("="*60)
    print("æ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯")
    print("="*60)
    
    # ç³»ç»Ÿä¿¡æ¯
    check_system_requirements()
    
    # å°è¯•åŠ è½½æ¨¡å‹ä¿¡æ¯
    try:
        from core.sam_model_loader import create_sam_model_loader
        from utils.model_utils import print_model_summary
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ä¿¡æ¯: {args.model}")
        
        loader = create_sam_model_loader(args.model, "cpu")  # ä½¿ç”¨CPUåŠ è½½ä»¥èŠ‚çœæ˜¾å­˜
        if loader.load_model():
            print("\næ¨¡å‹åŠ è½½æˆåŠŸ!")
            
            # æ‰“å°æ¨¡å‹ç»„ä»¶ä¿¡æ¯
            components = loader.get_trainable_components()
            print(f"\næ¨¡å‹ç»„ä»¶:")
            for name, component in components.items():
                param_count = sum(p.numel() for p in component.parameters())
                print(f"  {name}: {param_count:,} å‚æ•°")
            
            # æ‰“å°è¯¦ç»†çš„æ¨¡å‹æ‘˜è¦ï¼ˆå¦‚æœæœ‰å®Œæ•´æ¨¡å‹ï¼‰
            if loader.model is not None:
                print_model_summary(loader.model)
            
        else:
            print("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    except Exception as e:
        print(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # è§£æå‚æ•°
    args = parse_arguments()
    
    if args.command is None:
        print("è¯·æŒ‡å®šå‘½ä»¤ã€‚ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        return
    
    try:
        if args.command == 'train':
            train_lora_model(args)
        
        elif args.command == 'resume':
            resume_lora_training(args)
        
        elif args.command == 'evaluate':
            evaluate_lora_model(args)
        
        elif args.command == 'train-and-eval':
            # å…ˆè®­ç»ƒ
            lora_model_path = train_lora_model(args)
            
            # å†è¯„æµ‹
            if lora_model_path:
                print("\n" + "="*60)
                print("å¼€å§‹è‡ªåŠ¨è¯„æµ‹")
                print("="*60)
                evaluate_lora_model(args, lora_model_path)
        
        elif args.command == 'prepare-data':
            prepare_training_data(args)
        
        elif args.command == 'info':
            show_model_info(args)
        
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {args.command}")
    
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        optimize_memory()


if __name__ == "__main__":
    main()