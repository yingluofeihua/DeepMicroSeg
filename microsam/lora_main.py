"""
LoRAè®­ç»ƒä¸»å…¥å£æ–‡ä»¶ (ä¿®å¤ç‰ˆ)
ä¿®å¤ --cell-types å‚æ•°é‡å¤å®šä¹‰å¯¼è‡´çš„å†²çªé—®é¢˜
"""

import sys
import argparse
from pathlib import Path
import json
import torch
import numpy as np
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from config.lora_config import LoRATrainingSettings, LORA_PRESET_CONFIGS, get_config_for_model
from core.lora_trainer import LoRATrainer, create_trainer_from_config, resume_training
from core.evaluator import BatchEvaluator
from core.dataset_manager import DatasetManager
from config.settings import BatchEvaluationSettings
from lora.data_loaders import split_dataset, list_cached_splits, clean_old_splits, preview_data_split
from utils.file_utils import setup_logging
from utils.model_utils import get_device_info, optimize_memory
from utils.data_splitter import DatasetSplitter, print_split_summary


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - ä¿®å¤å‚æ•°å†²çª"""
    parser = argparse.ArgumentParser(
        description="SAM LoRAå¾®è°ƒè®­ç»ƒç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¿«é€Ÿè®­ç»ƒ
  python lora_main.py train --preset quick --data-dir /path/to/data
  
  # æ ‡å‡†è®­ç»ƒï¼ˆè‡ªåŠ¨åˆ’åˆ†æ•°æ®é›†ï¼‰
  python lora_main.py train --data-dir /path/to/data --model vit_b_lm --epochs 10 --test-split 0.2
  
  # æŒ‡å®šç»†èƒç±»å‹å’Œæµ‹è¯•é›†æ¯”ä¾‹
  python lora_main.py train --data-dir /path/to/data --cell-types 293T --test-split 0.15
  
  # ä»é…ç½®æ–‡ä»¶è®­ç»ƒ
  python lora_main.py train --config config.json
  
  # æ¢å¤è®­ç»ƒ
  python lora_main.py resume --checkpoint /path/to/checkpoint.pth
  
  # è¯„æµ‹LoRAæ¨¡å‹
  python lora_main.py evaluate --lora-model /path/to/lora --split-file /path/to/split.json
  
  # è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹
  python lora_main.py train-and-eval --data-dir /path/to/data --eval-data /path/to/eval
  
  # å‡†å¤‡æ•°æ®/é¢„è§ˆæ•°æ®åˆ’åˆ†
  python lora_main.py prepare-data --data-dir /path/to/data --test-split 0.2
  
  # ç®¡ç†æ•°æ®åˆ’åˆ†ç¼“å­˜
  python lora_main.py manage-splits --list --clean --keep 5
        """
    )
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è®­ç»ƒå‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒLoRAæ¨¡å‹')
    add_train_arguments(train_parser)
    
    # æ¢å¤è®­ç»ƒå‘½ä»¤
    resume_parser = subparsers.add_parser('resume', help='æ¢å¤è®­ç»ƒ')
    resume_parser.add_argument('--checkpoint', required=True, help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    resume_parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # è¯„æµ‹å‘½ä»¤ - ä½¿ç”¨æ–°çš„å¢å¼ºè¯„æµ‹å‚æ•°
    eval_parser = subparsers.add_parser('evaluate', help='è¯„æµ‹LoRAæ¨¡å‹')
    add_enhanced_eval_arguments(eval_parser)
    
    # ğŸ”§ ä¿®å¤ï¼šè®­ç»ƒ+è¯„æµ‹å‘½ä»¤ - åªæ·»åŠ è®­ç»ƒå‚æ•°ï¼Œè¯„æµ‹éƒ¨åˆ†æ‰‹åŠ¨æ·»åŠ é¿å…å†²çª
    train_eval_parser = subparsers.add_parser('train-and-eval', help='è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹')
    add_train_arguments(train_eval_parser)
    # æ‰‹åŠ¨æ·»åŠ è¯„æµ‹ç›¸å…³å‚æ•°ï¼Œé¿å…ä¸è®­ç»ƒå‚æ•°å†²çª
    add_eval_arguments_no_conflict(train_eval_parser)
    
    # æ•°æ®å‡†å¤‡å‘½ä»¤
    data_parser = subparsers.add_parser('prepare-data', help='å‡†å¤‡è®­ç»ƒæ•°æ®/é¢„è§ˆæ•°æ®åˆ’åˆ†')
    data_parser.add_argument('--data-dir', required=True, help='æ•°æ®ç›®å½•')
    data_parser.add_argument('--train-ratio', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹')
    data_parser.add_argument('--val-ratio', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹')
    data_parser.add_argument('--test-split', type=float, default=0.1, help='æµ‹è¯•é›†æ¯”ä¾‹')
    data_parser.add_argument('--cell-types', nargs='+', help='è¦å¤„ç†çš„ç»†èƒç±»å‹')
    data_parser.add_argument('--split-method', choices=['random', 'by_dataset'], default='random', help='åˆ’åˆ†æ–¹æ³•')
    data_parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    data_parser.add_argument('--preview-only', action='store_true', help='åªé¢„è§ˆï¼Œä¸åˆ›å»ºå®é™…åˆ’åˆ†')
    
    # æ•°æ®åˆ’åˆ†ç®¡ç†å‘½ä»¤
    splits_parser = subparsers.add_parser('manage-splits', help='ç®¡ç†æ•°æ®åˆ’åˆ†ç¼“å­˜')
    splits_parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ’åˆ†')
    splits_parser.add_argument('--clean', action='store_true', help='æ¸…ç†æ—§çš„åˆ’åˆ†æ–‡ä»¶')
    splits_parser.add_argument('--keep', type=int, default=10, help='ä¿ç•™çš„æœ€æ–°åˆ’åˆ†æ•°é‡')
    splits_parser.add_argument('--split-dir', default='./data/lora_split', help='åˆ’åˆ†æ–‡ä»¶å­˜å‚¨ç›®å½•')
    
    # æ¨¡å‹ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser('info', help='æ˜¾ç¤ºæ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯')
    info_parser.add_argument('--model', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                            default='vit_b_lm', help='æ¨¡å‹ç±»å‹')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    return parser.parse_args()


def add_train_arguments(parser):
    """æ·»åŠ è®­ç»ƒç›¸å…³å‚æ•°"""
    # æ•°æ®é…ç½®
    parser.add_argument('--data-dir', required=True, help='è®­ç»ƒæ•°æ®ç›®å½•')
    parser.add_argument('--val-data-dir', help='éªŒè¯æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', default='./data/lora_experiments', help='è¾“å‡ºç›®å½•')
    
    # æ•°æ®åˆ’åˆ†å‚æ•°
    parser.add_argument('--test-split', type=float, default=0.1, help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰')
    parser.add_argument('--val-split', type=float, help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œä»train_ratioè®¡ç®—ï¼‰')
    parser.add_argument('--split-method', choices=['random', 'by_dataset'], default='random', help='æ•°æ®åˆ’åˆ†æ–¹æ³•')
    parser.add_argument('--split-seed', type=int, default=42, help='æ•°æ®åˆ’åˆ†éšæœºç§å­')
    parser.add_argument('--no-cached-split', action='store_true', help='ä¸ä½¿ç”¨ç¼“å­˜çš„æ•°æ®åˆ’åˆ†')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                       default='vit_b_lm', help='åŸºç¡€æ¨¡å‹')
    parser.add_argument('--preset', choices=list(LORA_PRESET_CONFIGS.keys()), 
                       help='é¢„è®¾é…ç½®')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # LoRAé…ç½®
    parser.add_argument('--rank', type=int, default=8, help='LoRA rank')
    parser.add_argument('--alpha', type=float, default=16.0, help='LoRA alpha')
    parser.add_argument('--dropout', type=float, default=0.1, help='LoRA dropout')
    parser.add_argument('--lora-target', choices=['image_encoder', 'mask_decoder', 'both'],
                       default='image_encoder', help='LoRAåº”ç”¨ç›®æ ‡')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--save-steps', type=int, default=500, help='ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--eval-steps', type=int, default=100, help='éªŒè¯çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--logging-steps', type=int, default=50, help='æ—¥å¿—è®°å½•çš„æ­¥æ•°é—´éš”')
    
    # å®éªŒé…ç½®
    parser.add_argument('--experiment-name', default='sam_lora_finetune', help='å®éªŒåç§°')
    parser.add_argument('--use-wandb', action='store_true', help='ä½¿ç”¨Weights & Biases')
    parser.add_argument('--wandb-project', default='sam_lora_training', help='W&Bé¡¹ç›®å')
    
    # ç»†èƒç±»å‹è¿‡æ»¤å‚æ•°
    parser.add_argument('--cell-types', nargs='+', help='è¦è®­ç»ƒçš„ç»†èƒç±»å‹ï¼Œå¦‚: --cell-types 293T MSC')


def add_enhanced_eval_arguments(parser):
    """æ·»åŠ å¢å¼ºçš„è¯„æµ‹ç›¸å…³å‚æ•°"""
    # åŸºæœ¬è¯„æµ‹å‚æ•°
    parser.add_argument('--lora-model', help='LoRAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--split-file', help='æ•°æ®åˆ’åˆ†æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('--eval-output', help='è¯„æµ‹ç»“æœè¾“å‡ºç›®å½•')
    
    # ğŸ”§ ä¿®å¤ï¼šåªåœ¨çº¯è¯„æµ‹å‘½ä»¤ä¸­æ·»åŠ  cell-types å‚æ•°
    parser.add_argument('--cell-types', nargs='+', help='ç»†èƒç±»å‹è¿‡æ»¤')
    
    # æ‰¹é‡æµ‹è¯•é€‰é¡¹
    parser.add_argument('--batch-test', action='store_true', help='æ‰¹é‡æµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨åˆ’åˆ†æ–‡ä»¶ä¸­çš„æµ‹è¯•é›†ï¼‰')
    parser.add_argument('--max-samples', type=int, help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡é™åˆ¶')
    
    # è¯„æµ‹é€‰é¡¹
    parser.add_argument('--compare-baseline', action='store_true', help='ä¸åŸºç¡€æ¨¡å‹å¯¹æ¯”')
    parser.add_argument('--save-detailed', action='store_true', help='ä¿å­˜è¯¦ç»†çš„è¯„æµ‹ç»“æœ')
    parser.add_argument('--save-predictions', action='store_true', help='ä¿å­˜é¢„æµ‹ç»“æœå›¾åƒ')
    
    # æ¨¡å‹ç›¸å…³
    parser.add_argument('--model-type', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                       default='vit_b_lm', help='åŸºç¡€æ¨¡å‹ç±»å‹')
    parser.add_argument('--device', default='auto', help='è®¡ç®—è®¾å¤‡')
    
    # åº¦é‡ç›¸å…³
    parser.add_argument('--metrics', nargs='+', 
                       choices=['ap50', 'ap75', 'iou', 'dice', 'hd95'],
                       default=['ap50', 'ap75', 'iou', 'dice'],
                       help='è¦è®¡ç®—çš„è¯„æµ‹æŒ‡æ ‡')
    
    # å¯è§†åŒ–
    parser.add_argument('--generate-plots', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--plot-examples', type=int, default=10, help='å¯è§†åŒ–ç¤ºä¾‹æ•°é‡')
    
    # é€šç”¨å‚æ•°ï¼ˆåœ¨å­å‘½ä»¤ä¸­é‡å¤å®šä¹‰ä»¥æ”¯æŒä½ç½®çµæ´»æ€§ï¼‰
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')


def add_eval_arguments_no_conflict(parser):
    """ä¸ºtrain-and-evalå‘½ä»¤æ·»åŠ è¯„æµ‹å‚æ•°ï¼Œé¿å…ä¸è®­ç»ƒå‚æ•°å†²çª"""
    # æ³¨æ„ï¼šä¸æ·»åŠ  --cell-types å› ä¸ºè®­ç»ƒå‚æ•°ä¸­å·²ç»æœ‰äº†
    
    # åŸºæœ¬è¯„æµ‹å‚æ•°
    parser.add_argument('--eval-output', help='è¯„æµ‹ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--split-file', help='æ•°æ®åˆ’åˆ†æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    
    # æ‰¹é‡æµ‹è¯•é€‰é¡¹
    parser.add_argument('--batch-test', action='store_true', help='æ‰¹é‡æµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨åˆ’åˆ†æ–‡ä»¶ä¸­çš„æµ‹è¯•é›†ï¼‰')
    parser.add_argument('--max-samples', type=int, help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°é‡é™åˆ¶')
    
    # è¯„æµ‹é€‰é¡¹
    parser.add_argument('--compare-baseline', action='store_true', help='ä¸åŸºç¡€æ¨¡å‹å¯¹æ¯”')
    parser.add_argument('--save-detailed', action='store_true', help='ä¿å­˜è¯¦ç»†çš„è¯„æµ‹ç»“æœ')
    parser.add_argument('--save-predictions', action='store_true', help='ä¿å­˜é¢„æµ‹ç»“æœå›¾åƒ')
    
    # åº¦é‡ç›¸å…³
    parser.add_argument('--metrics', nargs='+', 
                       choices=['ap50', 'ap75', 'iou', 'dice', 'hd95'],
                       default=['ap50', 'ap75', 'iou', 'dice'],
                       help='è¦è®¡ç®—çš„è¯„æµ‹æŒ‡æ ‡')
    
    # å¯è§†åŒ–
    parser.add_argument('--generate-plots', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--plot-examples', type=int, default=10, help='å¯è§†åŒ–ç¤ºä¾‹æ•°é‡')


# ğŸ”§ æ–°å¢ï¼šå¢å¼ºçš„è¯„æµ‹å‡½æ•°ï¼Œæ”¯æŒä»åˆ’åˆ†æ–‡ä»¶åŠ è½½æ•°æ®
def evaluate_lora_model_enhanced(args, lora_model_path: str = None) -> bool:
    """å¢å¼ºçš„LoRAæ¨¡å‹è¯„æµ‹ - æ”¯æŒä»åˆ’åˆ†æ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®"""
    print("="*60)
    print("å¼€å§‹SAM LoRAæ¨¡å‹è¯„æµ‹ (å¢å¼ºç‰ˆ)")
    print("="*60)
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if lora_model_path is None:
        lora_model_path = args.lora_model
    
    if not lora_model_path:
        print("é”™è¯¯: æœªæŒ‡å®šLoRAæ¨¡å‹è·¯å¾„")
        return False
    
    try:
        # åŠ è½½LoRAæ¨¡å‹
        from lora.sam_lora_wrapper import load_sam_lora_model
        from core.metrics import ComprehensiveMetrics
        from lora.training_utils import prepare_sam_inputs
        
        print(f"LoRAæ¨¡å‹è·¯å¾„: {lora_model_path}")
        
        # ç¡®å®šæ¨¡å‹ç±»å‹
        model_type = getattr(args, 'model_type', 'vit_b_lm')
        
        # åŠ è½½LoRAæ¨¡å‹
        lora_model = load_sam_lora_model(model_type, lora_model_path)
        if lora_model is None:
            print("LoRAæ¨¡å‹åŠ è½½å¤±è´¥")
            return False
        
        print("LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # ğŸ”§ æ–°å¢ï¼šä»åˆ’åˆ†æ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®
        test_samples = None
        if hasattr(args, 'split_file') and args.split_file:
            print(f"ä»åˆ’åˆ†æ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®: {args.split_file}")
            
            try:
                import json
                with open(args.split_file, 'r', encoding='utf-8') as f:
                    split_data = json.load(f)
                
                if 'test_samples' in split_data:
                    test_samples = split_data['test_samples']
                    print(f"ä»åˆ’åˆ†æ–‡ä»¶åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
                    
                    # ç»†èƒç±»å‹è¿‡æ»¤
                    if hasattr(args, 'cell_types') and args.cell_types:
                        filtered_samples = [s for s in test_samples if s.get('cell_type') in args.cell_types]
                        test_samples = filtered_samples
                        print(f"ç»†èƒç±»å‹è¿‡æ»¤å: {len(test_samples)} ä¸ªæ ·æœ¬")
                    
                    # é™åˆ¶æ ·æœ¬æ•°é‡
                    if hasattr(args, 'max_samples') and args.max_samples and len(test_samples) > args.max_samples:
                        test_samples = test_samples[:args.max_samples]
                        print(f"é™åˆ¶æ ·æœ¬æ•°é‡: {len(test_samples)} ä¸ªæ ·æœ¬")
                        
                else:
                    print("åˆ’åˆ†æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ ·æœ¬")
                    return False
                    
            except Exception as e:
                print(f"åŠ è½½åˆ’åˆ†æ–‡ä»¶å¤±è´¥: {e}")
                return False
        
    #     # å¦‚æœæ²¡æœ‰ä»åˆ’åˆ†æ–‡ä»¶åŠ è½½ï¼Œåˆ™ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    #     if test_samples is None:
    #         print("ä½¿ç”¨ä¼ ç»Ÿæ•°æ®åŠ è½½æ–¹å¼...")
            
    #         # ç¡®å®šè¯„æµ‹æ•°æ®
    #         eval_data_dir = getattr(args, 'eval_data', None) or getattr(args, 'data_dir', None)
    #         if not eval_data_dir:
    #             print("é”™è¯¯: æœªæŒ‡å®šè¯„æµ‹æ•°æ®ç›®å½•")
    #             return False
            
    #         # åˆ›å»ºè¯„æµ‹æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
    #         from lora.data_loaders import create_data_loaders
    #         from config.lora_config import DataConfig
            
    #         data_config = DataConfig()
    #         data_config.test_data_dir = eval_data_dir
            
    #         data_loaders = create_data_loaders(data_config, dataset_type="sam")
    #         if 'test' not in data_loaders:
    #             print("æ— æ³•åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨")
    #             return False
            
    #         test_loader = data_loaders['test']
            
    #     else:
        # ğŸ”§ æ–°å¢ï¼šä»æ ·æœ¬åˆ—è¡¨åˆ›å»ºæ•°æ®åŠ è½½å™¨
        from lora.data_loaders import SAMDataset
        from torch.utils.data import DataLoader
        from config.lora_config import DataConfig
        
        print(f"test_samples: {test_samples}")
        data_config = DataConfig()
        test_dataset = SAMDataset(
            data_dir=None,
            config=data_config,
            split='test',
            samples=test_samples  # ç›´æ¥ä¼ å…¥æ ·æœ¬åˆ—è¡¨
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=1,
            shuffle=False,
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            collate_fn=lambda x: x[0] if len(x) == 1 else x  # ç®€å•çš„collateå‡½æ•°
        )
        
        print(f"æµ‹è¯•æ•°æ®: {len(test_loader)} æ‰¹æ¬¡")
        
        # åˆ›å»ºæŒ‡æ ‡è®¡ç®—å™¨
        metrics_calculator = ComprehensiveMetrics()
        
        # è¿›è¡Œè¯„æµ‹
        lora_model.eval()
        all_results = []
        
        print("å¼€å§‹è¯„æµ‹...")
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                
                try:
                    # å‡†å¤‡è¾“å…¥ - å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                    if isinstance(batch, dict):
                        # æ ‡å‡†çš„æ‰¹æ¬¡æ ¼å¼
                        inputs, targets = prepare_sam_inputs(batch)
                    else:
                        # ç®€å•æ ¼å¼ï¼Œéœ€è¦è½¬æ¢
                        sample = batch
                        batch_formatted = {
                            'images': sample['image'].unsqueeze(0),
                            'ground_truth_masks': sample['masks'].unsqueeze(0),
                            'sample_ids': [sample['sample_id']]
                        }
                        inputs, targets = prepare_sam_inputs(batch_formatted)
                    
                    # æ¨¡å‹é¢„æµ‹
                    predictions = lora_model(inputs)
                    print(predictions)
                    
                    # è®¡ç®—æŒ‡æ ‡
                    # pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    pred_masks = predictions['masks'].cpu().numpy()
                    # print(f"pred_masks: {pred_masks}")
                    target_masks = targets['masks'].cpu().numpy()
                    
                    for pred, target in zip(pred_masks, target_masks):
                        if pred.ndim > 2:
                            pred = pred[0]
                        if target.ndim > 2:
                            target = target[0]
                        
                        pred_binary = (pred > 0.5).astype(int)
                        print(f"pred_binary: {np.sum(pred_binary)}")
                        target_binary = (target > 0.5).astype(int)
                        print(f"target_binary: {np.sum(target_binary)}")
                        
                        result = metrics_calculator.compute_all_metrics(target_binary, pred_binary)
                        result_dict = result.to_dict()
                        
                        # æ·»åŠ å…ƒæ•°æ®
                        if isinstance(batch, dict) and batch_idx < len(batch.get('sample_ids', [])):
                            result_dict['sample_id'] = batch['sample_ids'][batch_idx]
                        else:
                            result_dict['sample_id'] = f"sample_{batch_idx}"
                        
                        all_results.append(result_dict)
                
                except Exception as e:
                    print(f"è¯„æµ‹æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    if hasattr(args, 'verbose') and args.verbose:
                        import traceback
                        traceback.print_exc()
                    continue
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if all_results:
            avg_metrics = {}
            for key in all_results[0].keys():
                if key == 'sample_id':
                    continue
                    
                values = [r[key] for r in all_results if key in r and r[key] is not None]
                if values:
                    if key == 'hd95':
                        finite_values = [v for v in values if not (v == float('inf') or v != v)]
                        avg_metrics[key] = sum(finite_values) / len(finite_values) if finite_values else float('inf')
                    else:
                        avg_metrics[key] = sum(values) / len(values)
            
            print(f"\nè¯„æµ‹ç»“æœ (åŸºäº {len(all_results)} ä¸ªæ ·æœ¬):")
            for key, value in avg_metrics.items():
                if key == 'hd95' and value == float('inf'):
                    print(f"  {key}: inf")
                else:
                    print(f"  {key}: {value:.4f}")
            
            # ä¿å­˜ç»“æœ
            if hasattr(args, 'eval_output') and args.eval_output:
                output_dir = Path(args.eval_output)
                output_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜è¯¦ç»†ç»“æœ
                if hasattr(args, 'save_detailed') and args.save_detailed:
                    import pandas as pd
                    results_df = pd.DataFrame(all_results)
                    results_file = output_dir / "detailed_evaluation_results.csv"
                    results_df.to_csv(results_file, index=False)
                    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
                
                # ä¿å­˜æ‘˜è¦ç»“æœ
                results_file = output_dir / "lora_evaluation_results.json"
                with open(results_file, 'w') as f:
                    json.dump({
                        'average_metrics': avg_metrics,
                        'individual_results': all_results if len(all_results) <= 1000 else all_results[:1000],  # é™åˆ¶å¤§å°
                        'model_path': lora_model_path,
                        'evaluation_config': {
                            'model_type': model_type,
                            'num_samples': len(all_results),
                            'split_file': getattr(args, 'split_file', None),
                            'cell_types': getattr(args, 'cell_types', None),
                            'max_samples': getattr(args, 'max_samples', None)
                        }
                    }, f, indent=2)
                
                print(f"è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        else:
            print("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ç»“æœ")
            return False
        
        print("LoRAæ¨¡å‹è¯„æµ‹å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"LoRAæ¨¡å‹è¯„æµ‹å¤±è´¥: {e}")
        if hasattr(args, 'verbose') and args.verbose:
            import traceback
            traceback.print_exc()
        return False


# å…¶ä»–å‡½æ•°ä¿æŒä¸å˜...
def create_config_from_args(args) -> LoRATrainingSettings:
    """ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®"""
    
    # ä½¿ç”¨é¢„è®¾é…ç½®
    if hasattr(args, 'preset') and args.preset:
        config = LORA_PRESET_CONFIGS[args.preset]
        print(f"ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    elif hasattr(args, 'config') and args.config:
        config = LoRATrainingSettings.from_json(args.config)
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
    
    # ä¸ºç‰¹å®šæ¨¡å‹åˆ›å»ºé…ç½®
    elif hasattr(args, 'model'):
        config = get_config_for_model(args.model)
        print(f"ä¸ºæ¨¡å‹ {args.model} åˆ›å»ºé…ç½®")
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    else:
        config = LoRATrainingSettings()
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # æ›´æ–°é…ç½®
    if hasattr(args, 'data_dir'):
        config.data.train_data_dir = args.data_dir
    
    if hasattr(args, 'val_data_dir') and args.val_data_dir:
        config.data.val_data_dir = args.val_data_dir
    
    if hasattr(args, 'output_dir'):
        config.experiment.output_dir = args.output_dir
    
    if hasattr(args, 'model'):
        config.model.base_model_name = args.model
    
    # æ•°æ®åˆ’åˆ†é…ç½®
    if hasattr(args, 'test_split'):
        config.data.test_split_ratio = args.test_split
        
        # è‡ªåŠ¨è°ƒæ•´è®­ç»ƒé›†å’ŒéªŒè¯é›†æ¯”ä¾‹
        if hasattr(args, 'val_split') and args.val_split is not None:
            config.data.val_split_ratio = args.val_split
            config.data.train_split_ratio = 1.0 - args.test_split - args.val_split
        else:
            # å¦‚æœæµ‹è¯•é›†æ¯”ä¾‹å¾ˆé«˜ï¼Œè®¾ç½®éªŒè¯é›†ä¸º0
            if args.test_split >= 0.9:
                config.data.val_split_ratio = 0.0
                config.data.train_split_ratio = 1.0 - args.test_split
            else:
                # ä¿æŒåŸæœ‰éªŒè¯é›†æ¯”ä¾‹ï¼Œè°ƒæ•´è®­ç»ƒé›†æ¯”ä¾‹
                remaining_ratio = 1.0 - args.test_split
                config.data.train_split_ratio = remaining_ratio * 0.9
                config.data.val_split_ratio = remaining_ratio * 0.1
        
        print(f"æ•°æ®åˆ’åˆ†æ¯”ä¾‹: train={config.data.train_split_ratio:.3f}, "
              f"val={config.data.val_split_ratio:.3f}, test={config.data.test_split_ratio:.3f}")
    
    if hasattr(args, 'split_method'):
        config.data.split_method = args.split_method
        
    if hasattr(args, 'split_seed'):
        config.data.split_seed = args.split_seed
        
    if hasattr(args, 'no_cached_split'):
        config.data.use_cached_split = not args.no_cached_split
    
    # LoRAå‚æ•°
    if hasattr(args, 'rank'):
        config.lora.rank = args.rank
    if hasattr(args, 'alpha'):
        config.lora.alpha = args.alpha
    if hasattr(args, 'dropout'):
        config.lora.dropout = args.dropout
    
    # LoRAç›®æ ‡è®¾ç½®
    if hasattr(args, 'lora_target'):
        if args.lora_target == 'image_encoder':
            config.model.apply_lora_to = ['image_encoder']
        elif args.lora_target == 'mask_decoder':
            config.model.apply_lora_to = ['mask_decoder']
        elif args.lora_target == 'both':
            config.model.apply_lora_to = ['image_encoder', 'mask_decoder']
    
    # è®­ç»ƒå‚æ•°
    if hasattr(args, 'epochs'):
        config.training.num_epochs = args.epochs
    if hasattr(args, 'batch_size'):
        config.training.batch_size = args.batch_size
    if hasattr(args, 'learning_rate'):
        config.training.learning_rate = args.learning_rate
    if hasattr(args, 'weight_decay'):
        config.training.weight_decay = args.weight_decay
    if hasattr(args, 'save_steps'):
        config.training.save_steps = args.save_steps
    if hasattr(args, 'eval_steps'):
        config.training.eval_steps = args.eval_steps  
    if hasattr(args, 'logging_steps'):
        config.training.logging_steps = args.logging_steps
    
    # å®éªŒé…ç½®
    if hasattr(args, 'experiment_name'):
        config.experiment.experiment_name = args.experiment_name
    if hasattr(args, 'use_wandb'):
        config.experiment.use_wandb = args.use_wandb
    if hasattr(args, 'wandb_project'):
        config.experiment.wandb_project = args.wandb_project

    # æ·»åŠ ç»†èƒç±»å‹è¿‡æ»¤
    if hasattr(args, 'cell_types') and args.cell_types:
        config.data._cell_types_filter = args.cell_types
    else:
        config.data._cell_types_filter = None
    
    # è°ƒè¯•æ¨¡å¼
    if hasattr(args, 'debug') and args.debug:
        config.experiment.debug_mode = True
        config.training.num_epochs = 2
        config.training.batch_size = 2
        config.training.save_steps = 10
        config.training.eval_steps = 5
        config.training.logging_steps = 1
        print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    return config


def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
    device_info = get_device_info()
    print(f"CUDAå¯ç”¨: {device_info['cuda_available']}")
    
    if device_info['cuda_available']:
        print(f"GPUæ•°é‡: {device_info['cuda_device_count']}")
        for gpu_name, gpu_info in device_info['gpu_memory'].items():
            print(f"  {gpu_name}: {gpu_info['name']}, "
                  f"å†…å­˜: {gpu_info['total_memory'] / 1e9:.1f} GB")
    
    # æ£€æŸ¥å†…å­˜
    cpu_memory = device_info['cpu_memory']
    print(f"CPUå†…å­˜: {cpu_memory['total'] / 1e9:.1f} GB "
          f"(å¯ç”¨: {cpu_memory['available'] / 1e9:.1f} GB)")
    
    # æ£€æŸ¥micro_sam
    try:
        import micro_sam
        print(f"micro_samå·²å®‰è£…")
    except ImportError:
        print("è­¦å‘Š: micro_samæœªå®‰è£…ï¼Œå¯èƒ½å½±å“æ¨¡å‹åŠ è½½")
    
    print("ç³»ç»Ÿæ£€æŸ¥å®Œæˆ\n")


def train_lora_model(args) -> str:
    """è®­ç»ƒLoRAæ¨¡å‹"""
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†åˆ«è®­ç»ƒå¤šä¸ªç»†èƒç±»å‹
    if hasattr(args, 'cell_types') and args.cell_types and len(args.cell_types) > 1:
        return train_multiple_cell_types(args)
    
    # åŸæ¥çš„å•æ¨¡å‹è®­ç»ƒé€»è¾‘
    print("="*60)
    print("å¼€å§‹SAM LoRAå¾®è°ƒè®­ç»ƒ")
    print("="*60)
    
    check_system_requirements()
    config = create_config_from_args(args)
    
    if not config.validate():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return None
    
    # å¦‚æœæ˜¯å•ä¸ªç»†èƒç±»å‹ï¼Œåˆ›å»ºæ›´è¯¦ç»†çš„å®éªŒåç§°
    if hasattr(args, 'cell_types') and args.cell_types and len(args.cell_types) == 1:
        cell_type = args.cell_types[0]
        # ç”ŸæˆåŒ…å«æ•°æ®åˆ’åˆ†ä¿¡æ¯çš„å®éªŒåç§°
        test_ratio = int(args.test_split * 100) if hasattr(args, 'test_split') else 10
        val_ratio = int(args.val_split * 100) if hasattr(args, 'val_split') and args.val_split else 10
        train_ratio = 100 - test_ratio - val_ratio
        
        split_suffix = f"train{train_ratio}_val{val_ratio}_test{test_ratio}"
        config.experiment.experiment_name = f"sam_lora_{cell_type.lower()}_{split_suffix}"
        config.experiment.output_dir = f"{config.experiment.output_dir}_{cell_type.lower()}_{split_suffix}"
        
    # é¢„è§ˆæ•°æ®åˆ’åˆ†
    if hasattr(args, 'test_split') and args.test_split > 0:
        print(f"\né¢„è§ˆæ•°æ®åˆ’åˆ†...")
        try:
            preview_stats = preview_data_split(
                data_dir=config.data.train_data_dir,
                train_ratio=config.data.train_split_ratio,
                val_ratio=config.data.val_split_ratio,
                test_ratio=config.data.test_split_ratio,
                cell_types=config.data._cell_types_filter,
                split_method=config.data.split_method,
                seed=config.data.split_seed,
                split_storage_dir=config.data.split_storage_dir
            )
            
            if preview_stats:
                print(f"  æ€»æ ·æœ¬æ•°: {preview_stats['total_samples']}")
                print(f"  è®­ç»ƒé›†: {preview_stats['train_count']} æ ·æœ¬")
                print(f"  éªŒè¯é›†: {preview_stats['val_count']} æ ·æœ¬")
                print(f"  æµ‹è¯•é›†: {preview_stats['test_count']} æ ·æœ¬")
                
                if 'cell_type_distribution' in preview_stats:
                    print(f"  ç»†èƒç±»å‹åˆ†å¸ƒ: {preview_stats['cell_type_distribution']}")
                    
        except Exception as e:
            print(f"é¢„è§ˆæ•°æ®åˆ’åˆ†å¤±è´¥: {e}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  åŸºç¡€æ¨¡å‹: {config.model.base_model_name}")
    print(f"  LoRAé…ç½®:")
    print(f"    rank: {config.lora.rank}")
    print(f"    alpha: {config.lora.alpha}")
    print(f"    dropout: {config.lora.dropout}")
    print(f"    åº”ç”¨åˆ°: {config.model.apply_lora_to}")
    print(f"  è®­ç»ƒé…ç½®:")
    print(f"    å­¦ä¹ ç‡: {config.training.learning_rate}")
    print(f"    æ‰¹å¤§å°: {config.training.batch_size}")
    print(f"    è®­ç»ƒè½®æ•°: {config.training.num_epochs}")
    print(f"  è¾“å‡ºç›®å½•: {config.experiment.output_dir}")
    print(f"  æ•°æ®ç›®å½•: {config.data.train_data_dir}")
    
    trainer = LoRATrainer(config)
    success = trainer.train()
    
    if success:
        model_path = trainer.output_dir / "final_model"
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        return str(model_path)
    else:
        print("\nè®­ç»ƒå¤±è´¥!")
        return None


def train_multiple_cell_types(args) -> str:
    """ä¸ºå¤šä¸ªç»†èƒç±»å‹åˆ†åˆ«è®­ç»ƒæ¨¡å‹"""
    print("="*60)
    print("å¼€å§‹å¤šç»†èƒç±»å‹åˆ†åˆ«è®­ç»ƒ")
    print("="*60)
    print(f"å°†è®­ç»ƒçš„ç»†èƒç±»å‹: {', '.join(args.cell_types)}")
    
    results = {}
    
    for cell_type in args.cell_types:
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ {cell_type} æ¨¡å‹...")
        
        # åˆ›å»ºå•ä¸ªç»†èƒç±»å‹çš„å‚æ•°å‰¯æœ¬
        single_args = type(args)()
        for attr in dir(args):
            if not attr.startswith('_'):
                setattr(single_args, attr, getattr(args, attr))
        
        # è®¾ç½®ä¸ºå•ä¸ªç»†èƒç±»å‹
        single_args.cell_types = [cell_type]
        
        # è®­ç»ƒ
        model_path = train_lora_model(single_args)
        results[cell_type] = model_path
        
        if model_path:
            print(f"âœ… {cell_type} è®­ç»ƒå®Œæˆ: {model_path}")
        else:
            print(f"âŒ {cell_type} è®­ç»ƒå¤±è´¥")
        
        # æ¸…ç†GPUå†…å­˜
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # ç”Ÿæˆæ‘˜è¦
    print(f"\n{'='*60}")
    print("å¤šç»†èƒç±»å‹è®­ç»ƒå®Œæˆ")
    print(f"{'='*60}")
    
    successful = sum(1 for path in results.values() if path)
    print(f"æˆåŠŸè®­ç»ƒ: {successful}/{len(args.cell_types)}")
    
    for cell_type, path in results.items():
        if path:
            print(f"  âœ… {cell_type}: {path}")
        else:
            print(f"  âŒ {cell_type}: å¤±è´¥")
    
    return f"å¤šç»†èƒç±»å‹è®­ç»ƒå®Œæˆï¼ŒæˆåŠŸ: {successful}/{len(args.cell_types)}"


def resume_lora_training(args) -> str:
    """æ¢å¤LoRAè®­ç»ƒ"""
    print("="*60)
    print("æ¢å¤SAM LoRAè®­ç»ƒ")
    print("="*60)
    
    trainer = resume_training(args.checkpoint, args.config)
    success = trainer.train()
    
    if success:
        model_path = trainer.output_dir / "final_model"
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        return str(model_path)
    else:
        print("\nè®­ç»ƒå¤±è´¥!")
        return None


def prepare_training_data(args):
    """å‡†å¤‡è®­ç»ƒæ•°æ®/é¢„è§ˆæ•°æ®åˆ’åˆ†"""
    print("="*60)
    print("å‡†å¤‡è®­ç»ƒæ•°æ® / æ•°æ®åˆ’åˆ†é¢„è§ˆ")
    print("="*60)
    
    data_dir = args.data_dir
    train_ratio = args.train_ratio
    val_ratio = args.val_ratio
    test_ratio = getattr(args, 'test_split', 0.1)
    
    # éªŒè¯æ¯”ä¾‹æ€»å’Œ
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        print(f"è­¦å‘Š: æ¯”ä¾‹æ€»å’Œä¸ä¸º1.0 ({total_ratio})ï¼Œæ­£åœ¨è‡ªåŠ¨å½’ä¸€åŒ–...")
        train_ratio /= total_ratio
        val_ratio /= total_ratio
        test_ratio /= total_ratio
    
    cell_types = getattr(args, 'cell_types', None)
    split_method = getattr(args, 'split_method', 'random')
    seed = getattr(args, 'seed', 42)
    preview_only = getattr(args, 'preview_only', False)
    
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"åˆ†å‰²æ¯”ä¾‹ - è®­ç»ƒ: {train_ratio:.3f}, éªŒè¯: {val_ratio:.3f}, æµ‹è¯•: {test_ratio:.3f}")
    print(f"ç»†èƒç±»å‹è¿‡æ»¤: {cell_types}")
    print(f"åˆ†å‰²æ–¹æ³•: {split_method}")
    print(f"éšæœºç§å­: {seed}")
    print(f"é¢„è§ˆæ¨¡å¼: {preview_only}")
    
    try:
        if preview_only:
            # åªé¢„è§ˆï¼Œä¸åˆ›å»ºå®é™…æ–‡ä»¶
            stats = preview_data_split(
                data_dir=data_dir,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio,
                cell_types=cell_types,
                split_method=split_method,
                seed=seed
            )
            
            if stats:
                print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†é¢„è§ˆ:")
                print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
                print(f"  è®­ç»ƒé›†: {stats['train_count']} æ ·æœ¬ ({stats['train_count']/stats['total_samples']*100:.1f}%)")
                print(f"  éªŒè¯é›†: {stats['val_count']} æ ·æœ¬ ({stats['val_count']/stats['total_samples']*100:.1f}%)")
                print(f"  æµ‹è¯•é›†: {stats['test_count']} æ ·æœ¬ ({stats['test_count']/stats['total_samples']*100:.1f}%)")
                
                if 'cell_type_distribution' in stats:
                    print(f"  ç»†èƒç±»å‹åˆ†å¸ƒ:")
                    for cell_type, count in stats['cell_type_distribution'].items():
                        print(f"    {cell_type}: {count} æ ·æœ¬")
            else:
                print("é¢„è§ˆå¤±è´¥")
        else:
            # åˆ›å»ºå®é™…çš„æ•°æ®åˆ’åˆ†
            from utils.data_splitter import create_data_split, print_split_summary
            
            split_result = create_data_split(
                data_dir=data_dir,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio,
                cell_types=cell_types,
                split_method=split_method,
                seed=seed,
                use_cached=True
            )
            
            # æ‰“å°æ‘˜è¦
            print_split_summary(split_result)
            
        print("æ•°æ®å‡†å¤‡å®Œæˆ!")
        
    except Exception as e:
        print(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def manage_data_splits(args):
    """ç®¡ç†æ•°æ®åˆ’åˆ†ç¼“å­˜"""
    print("="*60)
    print("æ•°æ®åˆ’åˆ†ç¼“å­˜ç®¡ç†")
    print("="*60)
    
    split_dir = args.split_dir
    
    if args.list:
        print(f"ğŸ“‹ åˆ—å‡ºç¼“å­˜çš„æ•°æ®åˆ’åˆ† (ç›®å½•: {split_dir})")
        try:
            cached_splits = list_cached_splits(split_dir)
            
            if not cached_splits:
                print("  æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜çš„æ•°æ®åˆ’åˆ†")
            else:
                print(f"  æ‰¾åˆ° {len(cached_splits)} ä¸ªç¼“å­˜æ–‡ä»¶:")
                
                for i, split_info in enumerate(cached_splits, 1):
                    print(f"\n  {i}. æ–‡ä»¶: {Path(split_info['file_path']).name}")
                    print(f"     å¤§å°: {split_info['file_size_mb']:.2f} MB")
                    print(f"     æ•°æ®ç›®å½•: {split_info.get('data_dir', 'N/A')}")
                    print(f"     æ¯”ä¾‹: train={split_info.get('train_ratio', 0):.2f}, "
                          f"val={split_info.get('val_ratio', 0):.2f}, "
                          f"test={split_info.get('test_ratio', 0):.2f}")
                    print(f"     æ ·æœ¬æ•°: {split_info.get('total_samples', 0)}")
                    if split_info.get('cell_types'):
                        print(f"     ç»†èƒç±»å‹: {split_info['cell_types']}")
                    print(f"     åˆ›å»ºæ—¶é—´: {split_info.get('created_at', 'N/A')}")
                        
        except Exception as e:
            print(f"åˆ—å‡ºç¼“å­˜å¤±è´¥: {e}")
    
    if args.clean:
        print(f"\nğŸ§¹ æ¸…ç†æ—§çš„æ•°æ®åˆ’åˆ†æ–‡ä»¶ (ä¿ç•™æœ€æ–° {args.keep} ä¸ª)")
        try:
            clean_old_splits(split_dir, args.keep)
            print("æ¸…ç†å®Œæˆ!")
        except Exception as e:
            print(f"æ¸…ç†å¤±è´¥: {e}")


def show_model_info(args):
    """æ˜¾ç¤ºæ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯"""
    print("="*60)
    print("æ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯")
    print("="*60)
    
    # ç³»ç»Ÿä¿¡æ¯
    check_system_requirements()
    
    # å°è¯•åŠ è½½æ¨¡å‹ä¿¡æ¯
    try:
        from core.sam_model_loader import create_sam_model_loader
        from utils.model_utils import print_model_summary
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ä¿¡æ¯: {args.model}")
        
        loader = create_sam_model_loader(args.model, "cpu")  # ä½¿ç”¨CPUåŠ è½½ä»¥èŠ‚çœæ˜¾å­˜
        if loader.load_model():
            print("\næ¨¡å‹åŠ è½½æˆåŠŸ!")
            
            # æ‰“å°æ¨¡å‹ç»„ä»¶ä¿¡æ¯
            components = loader.get_trainable_components()
            print(f"\næ¨¡å‹ç»„ä»¶:")
            for name, component in components.items():
                param_count = sum(p.numel() for p in component.parameters())
                print(f"  {name}: {param_count:,} å‚æ•°")
            
            # æ‰“å°è¯¦ç»†çš„æ¨¡å‹æ‘˜è¦ï¼ˆå¦‚æœæœ‰å®Œæ•´æ¨¡å‹ï¼‰
            if loader.model is not None:
                print_model_summary(loader.model)
            
        else:
            print("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    except Exception as e:
        print(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # è§£æå‚æ•°
    args = parse_arguments()
    
    if args.command is None:
        print("è¯·æŒ‡å®šå‘½ä»¤ã€‚ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        return
    
    try:
        if args.command == 'train':
            train_lora_model(args)
        
        elif args.command == 'resume':
            resume_lora_training(args)
        
        elif args.command == 'evaluate':
            # ğŸ”§ ä½¿ç”¨å¢å¼ºçš„è¯„æµ‹å‡½æ•°
            evaluate_lora_model_enhanced(args)
        
        elif args.command == 'train-and-eval':
            # å…ˆè®­ç»ƒ
            lora_model_path = train_lora_model(args)
            
            # å†è¯„æµ‹
            if lora_model_path:
                print("\n" + "="*60)
                print("å¼€å§‹è‡ªåŠ¨è¯„æµ‹")
                print("="*60)
                # ğŸ”§ ä½¿ç”¨å¢å¼ºçš„è¯„æµ‹å‡½æ•°
                evaluate_lora_model_enhanced(args, lora_model_path)
        
        elif args.command == 'prepare-data':
            prepare_training_data(args)
        
        elif args.command == 'manage-splits':
            manage_data_splits(args)
        
        elif args.command == 'info':
            show_model_info(args)
        
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {args.command}")
    
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        optimize_memory()


if __name__ == "__main__":
    main()