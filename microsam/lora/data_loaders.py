"""
LoRAè®­ç»ƒæ•°æ®åŠ è½½å™¨ (ä¿®å¤ç‰ˆ)
æ”¯æŒç»†èƒåˆ†å‰²ä»»åŠ¡çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
å…¼å®¹core/dataset_manager.pyçš„æ•°æ®ç»“æ„
"""

import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Any
import random
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import json

from config.lora_config import DataConfig
from utils.file_utils import load_image, load_mask
from config.paths import DatasetPathValidator


class CellSegmentationDataset(Dataset):
    """ç»†èƒåˆ†å‰²æ•°æ®é›† - æ”¯æŒå±‚æ¬¡åŒ–æ•°æ®ç»“æ„"""
    
    def __init__(
        self,
        data_dir: str,
        config: DataConfig,
        split: str = "train",
        transform: Optional[Any] = None
    ):
        self.data_dir = Path(data_dir)
        self.config = config
        self.split = split
        self.transform = transform
        
        # åŠ è½½æ•°æ®
        self.samples = self._load_samples()
        
        # åˆ›å»ºå¢å¼ºå˜æ¢
        if transform is None:
            self.transform = self._create_default_transforms()
        
        print(f"åŠ è½½äº† {len(self.samples)} ä¸ª{split}æ ·æœ¬")
    
    def _load_samples(self) -> List[Dict]:
        """åŠ è½½æ•°æ®æ ·æœ¬ - æ”¯æŒç»†èƒç±»å‹è¿‡æ»¤"""
        samples = []
        
        try:
            valid_datasets = DatasetPathValidator.validate_dataset_structure(self.data_dir)
            print(f"å‘ç° {len(valid_datasets)} ä¸ªæœ‰æ•ˆæ•°æ®é›†")
            
            for dataset_info in valid_datasets:
                # ğŸ”§ æ·»åŠ ç»†èƒç±»å‹è¿‡æ»¤
                if hasattr(self.config, '_cell_types_filter') and self.config._cell_types_filter:
                    if dataset_info['cell_type'] not in self.config._cell_types_filter:
                        continue  # è·³è¿‡ä¸åŒ¹é…çš„ç»†èƒç±»å‹
                
                images_dir = Path(dataset_info['images_dir'])
                masks_dir = Path(dataset_info['masks_dir'])
                
                image_mask_pairs = self._get_image_mask_pairs(images_dir, masks_dir)
                
                for img_path, mask_path in image_mask_pairs:
                    samples.append({
                        'image_path': str(img_path),
                        'mask_path': str(mask_path),
                        'sample_id': img_path.stem,
                        'cell_type': dataset_info['cell_type'],
                        'date': dataset_info['date'],
                        'magnification': dataset_info['magnification'],
                        'dataset_id': dataset_info['dataset_id']
                    })
            
            # æ‰“å°è¿‡æ»¤åçš„ç»Ÿè®¡
            if hasattr(self.config, '_cell_types_filter') and self.config._cell_types_filter:
                print(f"è¿‡æ»¤åæ ·æœ¬æ•° ({self.config._cell_types_filter}): {len(samples)}")
            
            samples = self._split_samples(samples)
            
            valid_samples = []
            for sample in samples:
                if self._validate_sample(sample):
                    valid_samples.append(sample)
            
            return valid_samples
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®æ ·æœ¬å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _get_image_mask_pairs(self, images_dir: Path, masks_dir: Path) -> List[Tuple[Path, Path]]:
        """è·å–å›¾åƒ-æ©ç å¯¹"""
        pairs = []
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']:
            image_files.extend(list(images_dir.glob(f"*{ext}")))
            image_files.extend(list(images_dir.glob(f"*{ext.upper()}")))
        
        # ä¸ºæ¯ä¸ªå›¾åƒæ–‡ä»¶æŸ¥æ‰¾å¯¹åº”çš„æ©ç 
        for img_file in image_files:
            mask_file = DatasetPathValidator.find_matching_mask(img_file, masks_dir)
            if mask_file:
                pairs.append((img_file, mask_file))
        
        return pairs
    
    def _split_samples(self, samples: List[Dict]) -> List[Dict]:
        """æ ¹æ®splitå‚æ•°åˆ†å‰²æ•°æ®"""
        if not samples:
            return []
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„
        dataset_groups = {}
        for sample in samples:
            dataset_id = sample['dataset_id']
            if dataset_id not in dataset_groups:
                dataset_groups[dataset_id] = []
            dataset_groups[dataset_id].append(sample)
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†è¿›è¡Œåˆ†å‰²
        split_samples = []
        for dataset_id, dataset_samples in dataset_groups.items():
            n_total = len(dataset_samples)
            n_train = int(n_total * self.config.train_split_ratio)
            n_val = int(n_total * self.config.val_split_ratio)
            
            # éšæœºæ‰“ä¹±ï¼ˆä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯é‡ç°ï¼‰
            random.seed(42)
            random.shuffle(dataset_samples)
            
            if self.split == "train":
                split_samples.extend(dataset_samples[:n_train])
            elif self.split == "val":
                split_samples.extend(dataset_samples[n_train:n_train + n_val])
            elif self.split == "test":
                split_samples.extend(dataset_samples[n_train + n_val:])
            else:  # "all"
                split_samples.extend(dataset_samples)
        
        return split_samples
    
    def _validate_sample(self, sample: Dict) -> bool:
        """éªŒè¯æ ·æœ¬æœ‰æ•ˆæ€§"""
        img_path = Path(sample['image_path'])
        mask_path = Path(sample['mask_path'])
        
        if not (img_path.exists() and mask_path.exists()):
            return False
        
        try:
            # å°è¯•åŠ è½½å›¾åƒå’Œæ©ç 
            image = load_image(img_path, convert_to_grayscale=False)
            mask = load_mask(mask_path)
            
            if image is None or mask is None:
                return False
            
            # æ£€æŸ¥å°ºå¯¸åŒ¹é…
            if image.shape[:2] != mask.shape:
                return False
            
            # æ£€æŸ¥æ©ç æ˜¯å¦æœ‰å‰æ™¯å¯¹è±¡
            if self.config.filter_empty_images and np.max(mask) == 0:
                return False
            
            return True
            
        except Exception:
            return False
    
    def _create_default_transforms(self):
        """åˆ›å»ºé»˜è®¤çš„æ•°æ®å˜æ¢"""
        if self.split == "train" and self.config.use_data_augmentation:
            # è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®å¢å¼º
            transform = A.Compose([
                A.Resize(self.config.image_size[0], self.config.image_size[1]),
                A.OneOf([
                    A.HorizontalFlip(p=0.5),
                    A.VerticalFlip(p=0.5),
                    A.RandomRotate90(p=0.5),
                ], p=0.5),
                A.OneOf([
                    A.RandomBrightnessContrast(
                        brightness_limit=0.2, 
                        contrast_limit=0.2, 
                        p=0.5
                    ),
                    A.HueSaturationValue(
                        hue_shift_limit=10,
                        sat_shift_limit=20,
                        val_shift_limit=20,
                        p=0.5
                    ),
                ], p=0.3),
                A.OneOf([
                    A.GaussianBlur(blur_limit=3, p=0.3),
                    A.GaussNoise(p=0.3),  # ç§»é™¤var_limitå‚æ•°
                ], p=0.2),
                A.Normalize(mean=self.config.normalize_mean, std=self.config.normalize_std),
                ToTensorV2()
            ])
        else:
            # éªŒè¯/æµ‹è¯•æ—¶åªåšåŸºæœ¬å˜æ¢
            transform = A.Compose([
                A.Resize(self.config.image_size[0], self.config.image_size[1]),
                A.Normalize(mean=self.config.normalize_mean, std=self.config.normalize_std),
                ToTensorV2()
            ])
        
        return transform
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        sample = self.samples[idx]
        
        # åŠ è½½å›¾åƒå’Œæ©ç 
        image = load_image(sample['image_path'], convert_to_grayscale=False)
        mask = load_mask(sample['mask_path'])
        
        if image is None or mask is None:
            # è¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„ç©ºæ ·æœ¬
            return self._get_empty_sample()
        
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
        if len(image.shape) == 2:
            image = np.stack([image] * 3, axis=-1)
        elif image.shape[-1] == 1:
            image = np.repeat(image, 3, axis=-1)
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            transformed = self.transform(image=image, mask=mask)
            image = transformed['image']
            mask = transformed['mask']
        
        # å¤„ç†æ©ç æ ¼å¼
        if isinstance(mask, torch.Tensor):
            mask = mask.long()
        else:
            mask = torch.from_numpy(mask).long()
        
        # ç”Ÿæˆå®ä¾‹æ©ç å’Œè¾¹ç•Œæ¡†
        instance_masks, boxes = self._process_mask(mask)
        
        return {
            'image': image,
            'masks': instance_masks,
            'boxes': boxes,
            'labels': torch.ones(len(boxes), dtype=torch.long),  # æ‰€æœ‰å¯¹è±¡éƒ½æ˜¯ç»†èƒ
            'sample_id': sample['sample_id'],
            'cell_type': sample['cell_type'],
            'date': sample['date'],
            'magnification': sample['magnification'],
            'dataset_id': sample['dataset_id']
        }
    
    def _get_empty_sample(self) -> Dict[str, torch.Tensor]:
        """è·å–ç©ºæ ·æœ¬"""
        h, w = self.config.image_size
        return {
            'image': torch.zeros(3, h, w),
            'masks': torch.zeros(0, h, w),
            'boxes': torch.zeros(0, 4),
            'labels': torch.zeros(0, dtype=torch.long),
            'sample_id': 'empty',
            'cell_type': 'unknown',
            'date': 'unknown',
            'magnification': 'unknown',
            'dataset_id': 'unknown'
        }
    
    def _process_mask(self, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å¤„ç†æ©ç ï¼Œæå–å®ä¾‹å’Œè¾¹ç•Œæ¡†"""
        # è·å–æ‰€æœ‰å®ä¾‹ID
        unique_ids = torch.unique(mask)
        unique_ids = unique_ids[unique_ids > 0]  # æ’é™¤èƒŒæ™¯
        
        if len(unique_ids) == 0:
            return torch.zeros(0, mask.shape[-2], mask.shape[-1]), torch.zeros(0, 4)
        
        # é™åˆ¶å®ä¾‹æ•°é‡
        if len(unique_ids) > self.config.max_objects_per_image:
            unique_ids = unique_ids[:self.config.max_objects_per_image]
        
        instance_masks = []
        boxes = []
        
        for instance_id in unique_ids:
            # åˆ›å»ºå•ä¸ªå®ä¾‹æ©ç 
            instance_mask = (mask == instance_id).float()
            
            # è¿‡æ»¤å¤ªå°çš„å¯¹è±¡
            if torch.sum(instance_mask) < self.config.min_object_size:
                continue
            
            # è¿‡æ»¤å¤ªå¤§çš„å¯¹è±¡
            if (self.config.max_object_size is not None and 
                torch.sum(instance_mask) > self.config.max_object_size):
                continue
            
            instance_masks.append(instance_mask)
            
            # è®¡ç®—è¾¹ç•Œæ¡†
            pos = torch.where(instance_mask)
            if len(pos[0]) > 0:
                xmin = torch.min(pos[1])
                xmax = torch.max(pos[1])
                ymin = torch.min(pos[0])
                ymax = torch.max(pos[0])
                
                boxes.append([xmin, ymin, xmax, ymax])
        
        if len(instance_masks) == 0:
            return torch.zeros(0, mask.shape[-2], mask.shape[-1]), torch.zeros(0, 4)
        
        instance_masks = torch.stack(instance_masks)
        boxes = torch.tensor(boxes, dtype=torch.float32)
        
        return instance_masks, boxes


class SAMDataset(CellSegmentationDataset):
    """ä¸“é—¨ä¸ºSAMæ¨¡å‹ä¼˜åŒ–çš„æ•°æ®é›†"""
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        sample = super().__getitem__(idx)
        
        # SAMéœ€è¦çš„ç‰¹æ®Šæ ¼å¼
        image = sample['image']
        masks = sample['masks']
        boxes = sample['boxes']
        
        # ä¸ºSAMå‡†å¤‡è¾“å…¥
        if len(boxes) > 0:
            # éšæœºé€‰æ‹©ä¸€äº›ç‚¹ä½œä¸ºprompt
            point_prompts = self._generate_point_prompts(masks)
            
            return {
                'image': image,
                'point_coords': point_prompts['coords'],
                'point_labels': point_prompts['labels'],
                'boxes': boxes,
                'mask_inputs': None,  # å¯ä»¥ä¸ºNone
                'multimask_output': False,
                'ground_truth_masks': masks,
                'sample_id': sample['sample_id'],
                'cell_type': sample['cell_type'],
                'date': sample['date'],
                'magnification': sample['magnification'],
                'dataset_id': sample['dataset_id']
            }
        else:
            # æ²¡æœ‰å¯¹è±¡çš„æƒ…å†µ
            h, w = image.shape[-2:]
            return {
                'image': image,
                'point_coords': torch.zeros(0, 2),
                'point_labels': torch.zeros(0),
                'boxes': torch.zeros(0, 4),
                'mask_inputs': None,
                'multimask_output': False,
                'ground_truth_masks': torch.zeros(0, h, w),
                'sample_id': sample['sample_id'],
                'cell_type': sample['cell_type'],
                'date': sample['date'],
                'magnification': sample['magnification'],
                'dataset_id': sample['dataset_id']
            }
    
    def _generate_point_prompts(self, masks: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ä¸ºæ¯ä¸ªæ©ç ç”Ÿæˆç‚¹æç¤º"""
        all_coords = []
        all_labels = []
        
        for mask in masks:
            # æ­£ä¾‹ç‚¹ï¼ˆåœ¨å¯¹è±¡å†…éƒ¨ï¼‰
            pos_coords = self._sample_points_from_mask(mask, num_points=1, positive=True)
            
            # è´Ÿä¾‹ç‚¹ï¼ˆåœ¨å¯¹è±¡å¤–éƒ¨ï¼‰
            neg_coords = self._sample_points_from_mask(mask, num_points=1, positive=False)
            
            coords = torch.cat([pos_coords, neg_coords], dim=0)
            labels = torch.tensor([1] * len(pos_coords) + [0] * len(neg_coords))
            
            all_coords.append(coords)
            all_labels.append(labels)
        
        if len(all_coords) > 0:
            return {
                'coords': torch.cat(all_coords, dim=0),
                'labels': torch.cat(all_labels, dim=0)
            }
        else:
            return {
                'coords': torch.zeros(0, 2),
                'labels': torch.zeros(0)
            }
    
    def _sample_points_from_mask(self, mask: torch.Tensor, num_points: int, positive: bool) -> torch.Tensor:
        """ä»æ©ç ä¸­é‡‡æ ·ç‚¹"""
        if positive:
            # ä»å‰æ™¯åŒºåŸŸé‡‡æ ·
            pos = torch.where(mask > 0.5)
        else:
            # ä»èƒŒæ™¯åŒºåŸŸé‡‡æ ·
            pos = torch.where(mask <= 0.5)
        
        if len(pos[0]) == 0:
            return torch.zeros(0, 2)
        
        # éšæœºé€‰æ‹©ç‚¹
        indices = torch.randperm(len(pos[0]))[:num_points]
        
        coords = torch.stack([
            pos[1][indices],  # x coordinates
            pos[0][indices]   # y coordinates
        ], dim=1)
        
        return coords.float()


def create_data_loaders(config: DataConfig, dataset_type: str = "standard") -> Dict[str, DataLoader]:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    datasets = {}
    data_loaders = {}
    
    # é€‰æ‹©æ•°æ®é›†ç±»å‹
    dataset_class = SAMDataset if dataset_type == "sam" else CellSegmentationDataset
    
    # è®­ç»ƒé›†
    if config.train_data_dir:
        datasets['train'] = dataset_class(
            data_dir=config.train_data_dir,
            config=config,
            split='train'
        )
        
        if len(datasets['train']) > 0:
            data_loaders['train'] = DataLoader(
                datasets['train'],
                batch_size=config.batch_size,
                shuffle=True,
                num_workers=config.num_workers,
                pin_memory=config.pin_memory,
                prefetch_factor=config.prefetch_factor,
                collate_fn=collate_fn
            )
        else:
            print("è­¦å‘Š: è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨")
    
    # éªŒè¯é›†
    if config.val_data_dir:
        datasets['val'] = dataset_class(
            data_dir=config.val_data_dir,
            config=config,
            split='val'
        )
        
        if len(datasets['val']) > 0:
            data_loaders['val'] = DataLoader(
                datasets['val'],
                batch_size=config.batch_size,
                shuffle=False,
                num_workers=config.num_workers,
                pin_memory=config.pin_memory,
                collate_fn=collate_fn
            )
    elif config.train_data_dir and len(datasets['train']) > 0:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šéªŒè¯é›†ï¼Œä»è®­ç»ƒæ•°æ®ä¸­åˆ›å»º
        datasets['val'] = dataset_class(
            data_dir=config.train_data_dir,
            config=config,
            split='val'
        )
        
        if len(datasets['val']) > 0:
            data_loaders['val'] = DataLoader(
                datasets['val'],
                batch_size=config.batch_size,
                shuffle=False,
                num_workers=config.num_workers,
                pin_memory=config.pin_memory,
                collate_fn=collate_fn
            )
    
    # æµ‹è¯•é›†
    if config.test_data_dir:
        datasets['test'] = dataset_class(
            data_dir=config.test_data_dir,
            config=config,
            split='test'
        )
        
        if len(datasets['test']) > 0:
            data_loaders['test'] = DataLoader(
                datasets['test'],
                batch_size=1,  # æµ‹è¯•æ—¶batch_size=1
                shuffle=False,
                num_workers=config.num_workers,
                pin_memory=config.pin_memory,
                collate_fn=collate_fn
            )
    
    return data_loaders


def collate_fn(batch):
    """è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•° - è¾“å‡ºå¼ é‡è€Œéåˆ—è¡¨"""
    # å¤„ç†å˜é•¿æ•°æ®
    images = []
    all_point_coords = []
    all_point_labels = []
    all_boxes = []
    all_masks = []
    sample_ids = []
    
    # æ”¶é›†æ‰€æœ‰æ•°æ®
    for item in batch:
        images.append(item['image'])
        
        if 'point_coords' in item:
            all_point_coords.append(item['point_coords'])
            all_point_labels.append(item['point_labels'])
        
        if 'boxes' in item:
            all_boxes.append(item['boxes'])
        
        # ç»Ÿä¸€å¤„ç†æ©ç æ•°æ®
        if 'ground_truth_masks' in item:
            masks = item['ground_truth_masks']
        elif 'masks' in item:
            masks = item['masks']
        else:
            # åˆ›å»ºé»˜è®¤çš„ç©ºæ©ç 
            h, w = item['image'].shape[-2:]
            masks = torch.zeros(1, h, w, dtype=torch.long)
        
        # ç¡®ä¿æ©ç æ˜¯å¼ é‡æ ¼å¼
        if not isinstance(masks, torch.Tensor):
            if isinstance(masks, np.ndarray):
                masks = torch.from_numpy(masks)
            else:
                h, w = item['image'].shape[-2:]
                masks = torch.zeros(1, h, w, dtype=torch.long)
        
        # ç¡®ä¿æ˜¯3Då¼ é‡ [N, H, W]
        if len(masks.shape) == 2:
            masks = masks.unsqueeze(0)
        
        all_masks.append(masks)
        sample_ids.append(item['sample_id'])
    
    # å †å å›¾åƒ
    images = torch.stack(images)
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†æ©ç åˆ—è¡¨è½¬æ¢ä¸ºç»Ÿä¸€çš„å¼ é‡
    if all_masks:
        # æ‰¾åˆ°æœ€å¤§å¯¹è±¡æ•°å’Œç»Ÿä¸€å°ºå¯¸
        max_objects = max([mask.shape[0] for mask in all_masks])
        batch_size = len(all_masks)
        h, w = all_masks[0].shape[-2:]  # å‡è®¾æ‰€æœ‰æ©ç å°ºå¯¸ç›¸åŒ
        
        # åˆ›å»ºç»Ÿä¸€çš„æ©ç å¼ é‡ [B, max_objects, H, W]
        unified_masks = torch.zeros(batch_size, max_objects, h, w, dtype=torch.long)
        
        for i, masks in enumerate(all_masks):
            # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š
            if images.device != masks.device:
                masks = masks.to(images.device)
            
            # å¤åˆ¶åˆ°ç»Ÿä¸€å¼ é‡ä¸­
            num_objects = min(masks.shape[0], max_objects)
            unified_masks[i, :num_objects] = masks[:num_objects]
    
    else:
        # å¦‚æœæ²¡æœ‰æ©ç ï¼Œåˆ›å»ºç©ºå¼ é‡
        batch_size = len(images)
        h, w = images.shape[-2:]
        unified_masks = torch.zeros(batch_size, 1, h, w, dtype=torch.long)
    
    # è¿”å›æ‰¹å¤„ç†æ•°æ®
    batch_data = {
        'images': images,
        'point_coords': all_point_coords,
        'point_labels': all_point_labels,
        'boxes': all_boxes,
        'ground_truth_masks': unified_masks,  # ğŸ¯ ç°åœ¨æ˜¯å¼ é‡ï¼[B, N, H, W]
        'sample_ids': sample_ids
    }
    
    return batch_data


def split_dataset(data_dir: str, train_ratio: float = 0.8, val_ratio: float = 0.1):
    """å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†"""
    print(f"æ•°æ®åˆ†å‰²åŠŸèƒ½å·²æ•´åˆåˆ°æ•°æ®é›†ç±»ä¸­")
    print(f"è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ¯”ä¾‹: {train_ratio}/{val_ratio}/{1-train_ratio-val_ratio}")
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ•°æ®é›†æ¥éªŒè¯æ•°æ®ç»“æ„
    from config.lora_config import DataConfig
    config = DataConfig()
    config.train_split_ratio = train_ratio
    config.val_split_ratio = val_ratio
    
    try:
        # æµ‹è¯•æ•°æ®åŠ è½½
        train_dataset = CellSegmentationDataset(data_dir, config, split='train')
        val_dataset = CellSegmentationDataset(data_dir, config, split='val')
        test_dataset = CellSegmentationDataset(data_dir, config, split='test')
        
        print(f"æ•°æ®åˆ†å‰²ç»“æœ:")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
    except Exception as e:
        print(f"æ•°æ®åˆ†å‰²æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()