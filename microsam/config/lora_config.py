"""
LoRAå¾®è°ƒé…ç½®ç®¡ç†æ¨¡å— (ä¿®å¤ç‰ˆ)
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional, Union
from pathlib import Path


@dataclass
class LoRAConfig:
    """LoRAé€‚é…å™¨é…ç½®"""
    
    # LoRAæ ¸å¿ƒå‚æ•°
    rank: int = 8                    # LoRA rank (é€šå¸¸4-64)
    alpha: float = 16.0              # LoRA alpha (é€šå¸¸ç­‰äºrankçš„2å€)
    dropout: float = 0.1             # LoRA dropout
    target_modules: Optional[List[str]] = None  # ç›®æ ‡æ¨¡å—ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    
    # è®­ç»ƒç›¸å…³
    bias: str = "none"               # biaså¤„ç†: "none", "all", "lora_only"
    task_type: str = "FEATURE_EXTRACTION"  # ä»»åŠ¡ç±»å‹
    
    def __post_init__(self):
        if self.target_modules is None:
            # é»˜è®¤ç›®æ ‡æ¨¡å—ï¼ˆé’ˆå¯¹Vision Transformerï¼‰
            self.target_modules = [
                "qkv", "proj", "fc1", "fc2", "mlp.lin1", "mlp.lin2",
                "attention.qkv", "attention.proj", "mlp"
            ]


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    
    # åŸºæœ¬è®­ç»ƒå‚æ•°
    learning_rate: float = 1e-4
    batch_size: int = 8
    num_epochs: int = 10
    warmup_steps: int = 100
    save_steps: int = 500
    eval_steps: int = 100
    logging_steps: int = 50
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    optimizer: str = "adamw"
    weight_decay: float = 0.01
    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    adam_epsilon: float = 1e-8
    max_grad_norm: float = 1.0
    
    # å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler_type: str = "cosine"
    num_warmup_steps: int = 100
    
    # æ•°æ®å¢å¼º
    use_data_augmentation: bool = True
    augmentation_probability: float = 0.5
    
    # æ—©åœå’Œä¿å­˜
    early_stopping_patience: int = 5
    save_total_limit: int = 3
    load_best_model_at_end: bool = True
    metric_for_best_model: str = "eval_loss"
    greater_is_better: bool = False


@dataclass
class DataConfig:
    """æ•°æ®é…ç½® (ä¿®å¤ç‰ˆ)"""
    
    # æ•°æ®è·¯å¾„
    train_data_dir: str = ""
    val_data_dir: str = ""
    test_data_dir: str = ""
    
    # ğŸ”§ æ–°å¢ï¼šæ•°æ®é›†åˆ’åˆ†é…ç½®
    train_split_ratio: float = 0.8
    val_split_ratio: float = 0.1
    test_split_ratio: float = 0.1  # æ–°å¢æµ‹è¯•é›†æ¯”ä¾‹
    split_method: str = "random"  # "random" æˆ– "by_dataset"
    split_seed: int = 42  # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°
    split_storage_dir: str = "./data/lora_split"  # åˆ’åˆ†ç»“æœå­˜å‚¨ç›®å½•
    use_cached_split: bool = True  # æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„åˆ’åˆ†ç»“æœ
    
    # æ•°æ®å¤„ç†
    image_size: tuple = (512, 512)  # SAMé»˜è®¤è¾“å…¥å°ºå¯¸
    max_objects_per_image: int = 100
    
    # æ•°æ®åŠ è½½
    batch_size: int = 8  # æ·»åŠ æ‰¹å¤§å°
    num_workers: int = 4
    pin_memory: bool = True
    prefetch_factor: int = 2
    
    # æ•°æ®è¿‡æ»¤
    min_object_size: int = 10
    max_object_size: Optional[int] = None
    filter_empty_images: bool = True
    
    # æ•°æ®å¢å¼º - æ·»åŠ ç¼ºå°‘çš„å±æ€§
    use_data_augmentation: bool = True
    augmentation_probability: float = 0.5
    horizontal_flip_prob: float = 0.5
    vertical_flip_prob: float = 0.5
    rotation_prob: float = 0.3
    brightness_contrast_prob: float = 0.3
    noise_prob: float = 0.2
    
    # å½’ä¸€åŒ–å‚æ•°
    normalize_mean: List[float] = field(default_factory=lambda: [0.485, 0.456, 0.406])
    normalize_std: List[float] = field(default_factory=lambda: [0.229, 0.224, 0.225])
    
    def __post_init__(self):
        """åå¤„ç†ï¼šéªŒè¯åˆ’åˆ†æ¯”ä¾‹"""
        total_ratio = self.train_split_ratio + self.val_split_ratio + self.test_split_ratio
        if abs(total_ratio - 1.0) > 1e-6:
            print(f"è­¦å‘Šï¼šæ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹æ€»å’Œä¸ä¸º1.0 ({total_ratio})")
            # è‡ªåŠ¨å½’ä¸€åŒ–
            self.train_split_ratio /= total_ratio
            self.val_split_ratio /= total_ratio
            self.test_split_ratio /= total_ratio
            print(f"è‡ªåŠ¨å½’ä¸€åŒ–åï¼štrain={self.train_split_ratio:.3f}, val={self.val_split_ratio:.3f}, test={self.test_split_ratio:.3f}")


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    
    # åŸºç¡€æ¨¡å‹
    base_model_name: str = "vit_b_lm"
    base_model_path: Optional[str] = None
    
    # æ¨¡å‹æ¶æ„
    freeze_backbone: bool = True
    freeze_prompt_encoder: bool = True
    freeze_mask_decoder: bool = False
    
    # LoRAè®¾ç½®
    apply_lora_to: List[str] = field(default_factory=lambda: ["image_encoder"])
    
    # è¾“å‡ºè®¾ç½®
    num_classes: int = 1
    use_aux_loss: bool = True


@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    
    # å®éªŒä¿¡æ¯
    experiment_name: str = "lora_finetune"
    run_name: Optional[str] = None
    description: str = ""
    tags: List[str] = field(default_factory=list)
    
    # è¾“å‡ºè·¯å¾„
    output_dir: str = "./data/patch/lora_experiments"
    logging_dir: str = "./data/patch/logs"
    cache_dir: str = "./data/patch/cache"
    
    # å®éªŒè¿½è¸ª
    use_wandb: bool = False
    wandb_project: str = "micro_sam_lora"
    wandb_entity: Optional[str] = None
    
    # ç¡¬ä»¶è®¾ç½®
    device: str = "auto"  # "auto", "cuda", "cpu"
    mixed_precision: bool = True
    dataloader_num_workers: int = 4
    
    # è°ƒè¯•å’Œç›‘æ§
    debug_mode: bool = False
    profile_training: bool = False
    save_strategy: str = "steps"
    evaluation_strategy: str = "steps"


@dataclass 
class LoRATrainingSettings:
    """LoRAè®­ç»ƒå®Œæ•´è®¾ç½®"""
    
    lora: LoRAConfig = field(default_factory=LoRAConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    data: DataConfig = field(default_factory=DataConfig)
    model: ModelConfig = field(default_factory=ModelConfig)
    experiment: ExperimentConfig = field(default_factory=ExperimentConfig)
    
    def __post_init__(self):
        # è‡ªåŠ¨è®¾ç½®è¿è¡Œåç§°
        if self.experiment.run_name is None:
            self.experiment.run_name = f"{self.experiment.experiment_name}_{self.model.base_model_name}_r{self.lora.rank}"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(self.experiment.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.experiment.logging_dir).mkdir(parents=True, exist_ok=True)
        Path(self.experiment.cache_dir).mkdir(parents=True, exist_ok=True)
        
        # ğŸ”§ æ–°å¢ï¼šåˆ›å»ºæ•°æ®é›†åˆ’åˆ†å­˜å‚¨ç›®å½•
        Path(self.data.split_storage_dir).mkdir(parents=True, exist_ok=True)
        
        # åŒæ­¥æ‰¹å¤§å°è®¾ç½®
        if hasattr(self.training, 'batch_size'):
            self.data.batch_size = self.training.batch_size
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        from dataclasses import asdict
        return asdict(self)
    
    @classmethod
    def from_dict(cls, config_dict: Dict):
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(
            lora=LoRAConfig(**config_dict.get('lora', {})),
            training=TrainingConfig(**config_dict.get('training', {})),
            data=DataConfig(**config_dict.get('data', {})),
            model=ModelConfig(**config_dict.get('model', {})),
            experiment=ExperimentConfig(**config_dict.get('experiment', {}))
        )
    
    @classmethod
    def from_json(cls, config_path: str):
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        import json
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        return cls.from_dict(config_dict)
    
    def save_to_json(self, config_path: str):
        """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
        import json
        config_dict = self.to_dict()
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        errors = []
        
        # éªŒè¯LoRAå‚æ•°
        if self.lora.rank <= 0:
            errors.append("LoRA rank must be positive")
        
        if self.lora.alpha <= 0:
            errors.append("LoRA alpha must be positive")
        
        if not (0 <= self.lora.dropout <= 1):
            errors.append("LoRA dropout must be between 0 and 1")
        
        # éªŒè¯è®­ç»ƒå‚æ•°
        if self.training.learning_rate <= 0:
            errors.append("Learning rate must be positive")
        
        if self.training.batch_size <= 0:
            errors.append("Batch size must be positive")
        
        if self.training.num_epochs <= 0:
            errors.append("Number of epochs must be positive")
        
        # éªŒè¯æ•°æ®é…ç½®
        if not self.data.train_data_dir:
            errors.append("Training data directory is required")
        
        # ğŸ”§ æ–°å¢ï¼šéªŒè¯æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
        total_ratio = self.data.train_split_ratio + self.data.val_split_ratio + self.data.test_split_ratio
        if abs(total_ratio - 1.0) > 1e-6:
            errors.append(f"Data split ratios must sum to 1.0, got {total_ratio}")
        
        if not (0 < self.data.train_split_ratio < 1):
            errors.append("Train split ratio must be between 0 and 1")
        
        if not (0 <= self.data.val_split_ratio < 1):
            errors.append("Val split ratio must be between 0 and 1") 
            
        if not (0 <= self.data.test_split_ratio < 1):
            errors.append("Test split ratio must be between 0 and 1")
        
        # éªŒè¯è·¯å¾„
        try:
            Path(self.experiment.output_dir).mkdir(parents=True, exist_ok=True)
            Path(self.data.split_storage_dir).mkdir(parents=True, exist_ok=True)
        except Exception as e:
            errors.append(f"Cannot create output directories: {e}")
        
        if errors:
            print("Configuration validation errors:")
            for error in errors:
                print(f"  - {error}")
            return False
        
        return True
    
    def get_model_output_dir(self) -> Path:
        """è·å–æ¨¡å‹è¾“å‡ºç›®å½•"""
        return Path(self.experiment.output_dir) / self.experiment.run_name
    
    def get_checkpoint_dir(self) -> Path:
        """è·å–æ£€æŸ¥ç‚¹ç›®å½•"""
        return self.get_model_output_dir() / "checkpoints"
    
    def get_logs_dir(self) -> Path:
        """è·å–æ—¥å¿—ç›®å½•"""
        return Path(self.experiment.logging_dir) / self.experiment.run_name


# é¢„å®šä¹‰çš„é…ç½®æ¨¡æ¿
LORA_PRESET_CONFIGS = {
    "quick": LoRATrainingSettings(
        lora=LoRAConfig(rank=4, alpha=8),
        training=TrainingConfig(
            batch_size=4,
            num_epochs=3,
            learning_rate=5e-4,
            save_steps=100,
            eval_steps=50
        ),
        experiment=ExperimentConfig(experiment_name="quick_test")
    ),
    
    "standard": LoRATrainingSettings(
        lora=LoRAConfig(rank=8, alpha=16),
        training=TrainingConfig(
            batch_size=8,
            num_epochs=10,
            learning_rate=1e-4,
            save_steps=500,
            eval_steps=100
        ),
        experiment=ExperimentConfig(experiment_name="standard_training")
    ),
    
    "high_rank": LoRATrainingSettings(
        lora=LoRAConfig(rank=32, alpha=64),
        training=TrainingConfig(
            batch_size=4,
            num_epochs=15,
            learning_rate=5e-5,
            save_steps=500,
            eval_steps=100
        ),
        experiment=ExperimentConfig(experiment_name="high_rank_training")
    ),
    
    "debug": LoRATrainingSettings(
        lora=LoRAConfig(rank=2, alpha=4),
        training=TrainingConfig(
            batch_size=2,
            num_epochs=1,
            learning_rate=1e-3,
            save_steps=10,
            eval_steps=5,
            logging_steps=1
        ),
        experiment=ExperimentConfig(
            experiment_name="debug",
            debug_mode=True
        )
    )
}


def get_config_for_model(model_name: str) -> LoRATrainingSettings:
    """ä¸ºç‰¹å®šæ¨¡å‹è·å–æ¨èé…ç½®"""
    base_config = LORA_PRESET_CONFIGS["standard"].copy() if hasattr(LORA_PRESET_CONFIGS["standard"], 'copy') else LoRATrainingSettings()
    
    # é‡æ–°åˆ›å»ºé…ç½®ä»¥é¿å…å¼•ç”¨é—®é¢˜
    base_config = LoRATrainingSettings()
    
    if model_name == "vit_t_lm":
        # å°æ¨¡å‹å¯ä»¥ç”¨æ›´é«˜çš„å­¦ä¹ ç‡å’Œæ›´å¤§çš„batch size
        base_config.training.learning_rate = 2e-4
        base_config.training.batch_size = 16
        base_config.lora.rank = 8
        
    elif model_name == "vit_b_lm":
        # ä¸­ç­‰æ¨¡å‹ä½¿ç”¨æ ‡å‡†é…ç½®
        base_config.training.learning_rate = 1e-4
        base_config.training.batch_size = 8
        base_config.lora.rank = 8
        
    elif model_name == "vit_l_lm":
        # å¤§æ¨¡å‹éœ€è¦æ›´å°çš„å­¦ä¹ ç‡å’Œbatch size
        base_config.training.learning_rate = 5e-5
        base_config.training.batch_size = 4
        base_config.lora.rank = 16
    
    base_config.model.base_model_name = model_name
    return base_config