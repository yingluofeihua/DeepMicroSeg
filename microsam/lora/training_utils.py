"""
LoRAè®­ç»ƒå·¥å…·å‡½æ•° - ä¿®å¤ç‰ˆ
ä¿®å¤SAMè®­ç»ƒä¸­çš„æ©ç å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Any, Optional


class SAMLoss(nn.Module):
    """SAMè®­ç»ƒçš„ç»¼åˆæŸå¤±å‡½æ•°"""
    
    def __init__(
        self,
        focal_loss_weight: float = 20.0,
        dice_loss_weight: float = 1.0,
        iou_loss_weight: float = 1.0,
        use_focal_loss: bool = True,
        use_dice_loss: bool = True,
        use_iou_loss: bool = True
    ):
        super().__init__()
        
        self.focal_loss_weight = focal_loss_weight
        self.dice_loss_weight = dice_loss_weight
        self.iou_loss_weight = iou_loss_weight
        self.use_focal_loss = use_focal_loss
        self.use_dice_loss = use_dice_loss
        self.use_iou_loss = use_iou_loss
        
        # Focal losså‚æ•°
        self.focal_alpha = 0.8
        self.focal_gamma = 2.0
    
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è®¡ç®—æ€»æŸå¤± - ä¿®å¤å¤šå®ä¾‹æ©ç å¤„ç†"""
        pred_masks = predictions['masks']  # [B, 1, H, W] - SAMè¾“å‡ºå•ä¸ªæ©ç 
        target_masks = targets['masks']    # [B, N, H, W] - å¤šä¸ªå®ä¾‹æ©ç 
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†å¤šå®ä¾‹ç›®æ ‡æ©ç åˆå¹¶ä¸ºå•ä¸ªäºŒè¿›åˆ¶æ©ç 
        if target_masks.shape[1] > 1:
            # å°†æ‰€æœ‰å®ä¾‹æ©ç åˆå¹¶ä¸ºä¸€ä¸ªäºŒè¿›åˆ¶æ©ç  [B, N, H, W] -> [B, 1, H, W]
            combined_target = (target_masks.sum(dim=1, keepdim=True) > 0).float()
        else:
            combined_target = target_masks.float()
        
        # ç¡®ä¿é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶åŒ¹é…
        if pred_masks.shape != combined_target.shape:
            # å¦‚æœç©ºé—´å°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´ç›®æ ‡æ©ç å°ºå¯¸
            combined_target = F.interpolate(
                combined_target, 
                size=pred_masks.shape[-2:], 
                mode='nearest'
            )
        
        loss_dict = {}
        total_loss = 0.0
        
        # Focal Loss
        if self.use_focal_loss:
            focal_loss = self._focal_loss(pred_masks, combined_target)
            loss_dict['focal_loss'] = focal_loss
            total_loss += self.focal_loss_weight * focal_loss
        
        # Dice Loss
        if self.use_dice_loss:
            dice_loss = self._dice_loss(pred_masks, combined_target)
            loss_dict['dice_loss'] = dice_loss
            total_loss += self.dice_loss_weight * dice_loss
        
        # IoU Loss
        if self.use_iou_loss:
            iou_loss = self._iou_loss(pred_masks, combined_target)
            loss_dict['iou_loss'] = iou_loss
            total_loss += self.iou_loss_weight * iou_loss
        
        # IoUé¢„æµ‹æŸå¤±ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'iou_predictions' in predictions and 'iou_targets' in targets:
            iou_pred_loss = F.mse_loss(
                predictions['iou_predictions'],
                targets['iou_targets']
            )
            loss_dict['iou_prediction_loss'] = iou_pred_loss
            total_loss += iou_pred_loss
        
        loss_dict['total_loss'] = total_loss
        return loss_dict
    
    def _focal_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """Focal Losså®ç°"""
        # ä½¿ç”¨sigmoidæ¿€æ´»
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        # è®¡ç®—focal loss
        ce_loss = F.binary_cross_entropy_with_logits(
            pred_masks, target_masks, reduction='none'
        )
        
        p_t = pred_sigmoid * target_masks + (1 - pred_sigmoid) * (1 - target_masks)
        alpha_t = self.focal_alpha * target_masks + (1 - self.focal_alpha) * (1 - target_masks)
        
        focal_weight = alpha_t * (1 - p_t) ** self.focal_gamma
        focal_loss = focal_weight * ce_loss
        
        return focal_loss.mean()
    
    def _dice_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """Dice Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        # å¹³æ»‘å› å­
        smooth = 1.0
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„dice loss
        intersection = (pred_sigmoid * target_masks).sum(dim=(-2, -1))
        total = pred_sigmoid.sum(dim=(-2, -1)) + target_masks.sum(dim=(-2, -1))
        
        dice_score = (2.0 * intersection + smooth) / (total + smooth)
        dice_loss = 1.0 - dice_score
        
        return dice_loss.mean()
    
    def _iou_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """IoU Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        # è®¡ç®—IoU
        intersection = (pred_sigmoid * target_masks).sum(dim=(-2, -1))
        union = pred_sigmoid.sum(dim=(-2, -1)) + target_masks.sum(dim=(-2, -1)) - intersection
        
        # é¿å…é™¤é›¶
        iou_score = intersection / (union + 1e-6)
        iou_loss = 1.0 - iou_score
        
        return iou_loss.mean()


def prepare_sam_inputs(batch: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor]]:
    """å‡†å¤‡SAMè®­ç»ƒçš„è¾“å…¥å’Œç›®æ ‡ - ä¿®å¤å¤šå®ä¾‹æ©ç å¤„ç†"""
    
    try:
        # è¾“å…¥æ•°æ®
        inputs = {
            'images': batch['images'],  # [B, C, H, W]
            'point_coords': batch.get('point_coords', []),
            'point_labels': batch.get('point_labels', []),
            'boxes': batch.get('boxes', []),
            'mask_inputs': batch.get('mask_inputs', None),
            'multimask_output': batch.get('multimask_output', False)
        }
        
        # ç›®æ ‡æ•°æ®
        ground_truth_masks = batch['ground_truth_masks']  # [B, N, H, W]
        
        # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        device = inputs['images'].device
        
        if isinstance(ground_truth_masks, torch.Tensor):
            targets_masks = ground_truth_masks.to(device)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†å¤šå®ä¾‹æ©ç 
            if targets_masks.shape[1] > 1:
                # æ–¹æ¡ˆ1ï¼šåˆå¹¶æ‰€æœ‰å®ä¾‹ä¸ºå•ä¸ªäºŒè¿›åˆ¶æ©ç 
                binary_masks = (targets_masks.sum(dim=1, keepdim=True) > 0).float()
                targets_masks = binary_masks
            
        else:
            # å‘åå…¼å®¹å¤„ç†
            print(f"WARNING: ground_truth_masksè¿˜æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè½¬æ¢ä¸ºå¼ é‡")
            
            if isinstance(ground_truth_masks, list):
                processed_masks = []
                
                for i, masks in enumerate(ground_truth_masks):
                    if isinstance(masks, torch.Tensor):
                        masks = masks.to(device)
                        if len(masks.shape) == 2:
                            masks = masks.unsqueeze(0)
                        
                        # å¦‚æœæœ‰å¤šä¸ªå®ä¾‹ï¼Œåˆå¹¶ä¸ºäºŒè¿›åˆ¶æ©ç 
                        if masks.shape[0] > 1:
                            binary_mask = (masks.sum(dim=0, keepdim=True) > 0).float()
                            processed_masks.append(binary_mask)
                        else:
                            processed_masks.append(masks)
                    else:
                        h, w = inputs['images'].shape[-2:]
                        default_mask = torch.zeros(1, h, w, dtype=torch.float32, device=device)
                        processed_masks.append(default_mask)
                
                # ç»Ÿä¸€å½¢çŠ¶å¹¶å †å 
                target_size = processed_masks[0].shape[-2:]
                unified_masks = []
                
                for masks in processed_masks:
                    if masks.shape[-2:] != target_size:
                        masks = torch.nn.functional.interpolate(
                            masks.unsqueeze(1).float(),
                            size=target_size,
                            mode='nearest'
                        ).squeeze(1)
                    unified_masks.append(masks)
                
                targets_masks = torch.stack(unified_masks)
            else:
                # åˆ›å»ºé»˜è®¤å¼ é‡
                batch_size = inputs['images'].shape[0]
                h, w = inputs['images'].shape[-2:]
                targets_masks = torch.zeros(batch_size, 1, h, w, dtype=torch.float32, device=device)
        
        targets = {
            'masks': targets_masks  # [B, 1, H, W] - ç°åœ¨æ˜¯å•ä¸ªäºŒè¿›åˆ¶æ©ç 
        }
        
        # è®¡ç®—IoUç›®æ ‡
        try:
            if targets_masks.numel() > 0:
                iou_targets = calculate_mask_iou_targets(targets_masks)
                targets['iou_targets'] = iou_targets
        except Exception as e:
            print(f"WARNING: IoU targets calculation failed: {e}")
            targets['iou_targets'] = torch.ones(targets_masks.shape[0], targets_masks.shape[1], device=device)
        return inputs, targets
        
    except Exception as e:
        print(f"ERROR in prepare_sam_inputs: {e}")
        import traceback
        traceback.print_exc()
        
        # è¿”å›é»˜è®¤å€¼
        batch_size = batch['images'].shape[0]
        device = batch['images'].device
        h, w = batch['images'].shape[-2:]
        
        inputs = {
            'images': batch['images'],
            'point_coords': [],
            'point_labels': [],
            'boxes': [],
            'mask_inputs': None,
            'multimask_output': False
        }
        
        targets = {
            'masks': torch.zeros(batch_size, 1, h, w, dtype=torch.float32, device=device),
            'iou_targets': torch.ones(batch_size, 1, device=device)
        }
        
        return inputs, targets


def calculate_mask_iou_targets(masks: torch.Tensor) -> torch.Tensor:
    """è®¡ç®—æ©ç çš„IoUç›®æ ‡å€¼"""
    if masks.numel() == 0:
        return torch.tensor([])
    
    # masks shape: [B, N, H, W]
    batch_size = masks.shape[0]
    num_objects = masks.shape[1] if len(masks.shape) > 3 else 1
    
    if len(masks.shape) == 3:
        # å¦‚æœæ˜¯ [B, H, W]ï¼Œæ·»åŠ å¯¹è±¡ç»´åº¦
        masks = masks.unsqueeze(1)  # [B, 1, H, W]
        num_objects = 1
    
    # è®¡ç®—æ¯ä¸ªæ©ç çš„é¢ç§¯æ¯”ä¾‹ä½œä¸ºIoUç›®æ ‡
    mask_areas = masks.sum(dim=(-2, -1))  # [B, N]
    total_area = masks.shape[-2] * masks.shape[-1]
    
    # ä½¿ç”¨é¢ç§¯æ¯”ä¾‹ä½œä¸ºIoUçš„ç²—ç•¥ä¼°è®¡ï¼Œå¹¶é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    iou_targets = torch.clamp(mask_areas.float() / total_area, 0.1, 1.0)
    
    return iou_targets


class MaskPostProcessor:
    """æ©ç åå¤„ç†å™¨"""
    
    def __init__(self, threshold: float = 0.0, remove_small_objects: bool = True, 
                 min_object_size: int = 100):
        self.threshold = threshold
        self.remove_small_objects = remove_small_objects
        self.min_object_size = min_object_size
    
    def process(self, masks: torch.Tensor) -> torch.Tensor:
        """åå¤„ç†é¢„æµ‹çš„æ©ç """
        # åº”ç”¨é˜ˆå€¼
        if self.threshold > 0:
            masks = (masks > self.threshold).float()
        else:
            masks = torch.sigmoid(masks)
        
        # ç§»é™¤å°å¯¹è±¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.remove_small_objects:
            masks = self._remove_small_objects(masks)
        
        return masks
    
    def _remove_small_objects(self, masks: torch.Tensor) -> torch.Tensor:
        """ç§»é™¤å°å¯¹è±¡"""
        processed_masks = masks.clone()
        
        for b in range(masks.shape[0]):
            for m in range(masks.shape[1]):
                mask = masks[b, m]
                mask_area = mask.sum()
                
                if mask_area < self.min_object_size:
                    processed_masks[b, m] = 0
        
        return processed_masks


class SAMDataAugmentation:
    """SAMè®­ç»ƒçš„æ•°æ®å¢å¼º"""
    
    def __init__(self, 
                 flip_prob: float = 0.5,
                 rotation_prob: float = 0.3,
                 scale_prob: float = 0.3,
                 noise_prob: float = 0.2):
        self.flip_prob = flip_prob
        self.rotation_prob = rotation_prob
        self.scale_prob = scale_prob
        self.noise_prob = noise_prob
    
    def augment_batch(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå¢å¼º"""
        augmented_batch = batch.copy()
        
        # éšæœºæ°´å¹³ç¿»è½¬
        if torch.rand(1) < self.flip_prob:
            augmented_batch = self._horizontal_flip(augmented_batch)
        
        # éšæœºå‚ç›´ç¿»è½¬
        if torch.rand(1) < self.flip_prob:
            augmented_batch = self._vertical_flip(augmented_batch)
        
        # æ·»åŠ å™ªå£°
        if torch.rand(1) < self.noise_prob:
            augmented_batch = self._add_noise(augmented_batch)
        
        return augmented_batch
    
    def _horizontal_flip(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """æ°´å¹³ç¿»è½¬"""
        batch['images'] = torch.flip(batch['images'], dims=[-1])
        if 'ground_truth_masks' in batch:
            batch['ground_truth_masks'] = torch.flip(batch['ground_truth_masks'], dims=[-1])
        
        # è°ƒæ•´ç‚¹åæ ‡
        if 'point_coords' in batch:
            for i, coords in enumerate(batch['point_coords']):
                if len(coords) > 0:
                    coords[:, 0] = batch['images'].shape[-1] - 1 - coords[:, 0]
        
        # è°ƒæ•´è¾¹ç•Œæ¡†
        if 'boxes' in batch:
            for i, boxes in enumerate(batch['boxes']):
                if len(boxes) > 0:
                    width = batch['images'].shape[-1]
                    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
                    boxes[:, 0] = width - 1 - x2
                    boxes[:, 2] = width - 1 - x1
        
        return batch
    
    def _vertical_flip(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """å‚ç›´ç¿»è½¬"""
        batch['images'] = torch.flip(batch['images'], dims=[-2])
        if 'ground_truth_masks' in batch:
            batch['ground_truth_masks'] = torch.flip(batch['ground_truth_masks'], dims=[-2])
        
        # è°ƒæ•´ç‚¹åæ ‡
        if 'point_coords' in batch:
            for i, coords in enumerate(batch['point_coords']):
                if len(coords) > 0:
                    coords[:, 1] = batch['images'].shape[-2] - 1 - coords[:, 1]
        
        # è°ƒæ•´è¾¹ç•Œæ¡†
        if 'boxes' in batch:
            for i, boxes in enumerate(batch['boxes']):
                if len(boxes) > 0:
                    height = batch['images'].shape[-2]
                    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
                    boxes[:, 1] = height - 1 - y2
                    boxes[:, 3] = height - 1 - y1
        
        return batch
    
    def _add_noise(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ å™ªå£°"""
        noise_level = 0.02 * torch.rand(1)
        noise = torch.randn_like(batch['images']) * noise_level
        batch['images'] = torch.clamp(batch['images'] + noise, 0, 1)
        
        return batch


class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.total_loss = 0.0
        self.focal_loss = 0.0
        self.dice_loss = 0.0
        self.iou_loss = 0.0
        self.count = 0
    
    def update(self, loss_dict: Dict[str, torch.Tensor]):
        """æ›´æ–°æŒ‡æ ‡"""
        def safe_item(value):
            """å®‰å…¨åœ°è·å–æ•°å€¼ï¼Œå¤„ç†tensorå’Œfloat"""
            if isinstance(value, torch.Tensor):
                return value.item()
            return float(value)
        
        self.total_loss += safe_item(loss_dict.get('total_loss', 0.0))
        self.focal_loss += safe_item(loss_dict.get('focal_loss', 0.0))
        self.dice_loss += safe_item(loss_dict.get('dice_loss', 0.0))
        self.iou_loss += safe_item(loss_dict.get('iou_loss', 0.0))
        self.count += 1
    
    def compute(self) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if self.count == 0:
            return {}
        
        return {
            'avg_total_loss': self.total_loss / self.count,
            'avg_focal_loss': self.focal_loss / self.count,
            'avg_dice_loss': self.dice_loss / self.count,
            'avg_iou_loss': self.iou_loss / self.count
        }


def validate_sam_batch(batch: Dict[str, Any]) -> bool:
    """éªŒè¯SAMæ‰¹æ¬¡æ•°æ®çš„æœ‰æ•ˆæ€§ - æ”¯æŒå¼ é‡æ©ç """
    required_keys = ['images', 'ground_truth_masks']
    
    for key in required_keys:
        if key not in batch:
            print(f"æ‰¹æ¬¡æ•°æ®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
            return False
    
    # æ£€æŸ¥å¼ é‡å½¢çŠ¶
    images = batch['images']
    masks = batch['ground_truth_masks']
    
    # æ£€æŸ¥å›¾åƒå¼ é‡
    if not isinstance(images, torch.Tensor):
        print(f"imagesä¸æ˜¯å¼ é‡: {type(images)}")
        return False
        
    if len(images.shape) != 4:
        print(f"å›¾åƒå¼ é‡å½¢çŠ¶é”™è¯¯: {images.shape}ï¼ŒæœŸæœ› [B, C, H, W]")
        return False
    
    # æ£€æŸ¥æ©ç å¼ é‡
    if not isinstance(masks, torch.Tensor):
        print(f"masksä¸æ˜¯å¼ é‡: {type(masks)}ï¼ŒæœŸæœ› torch.Tensor")
        return False
    
    if len(masks.shape) != 4:
        print(f"æ©ç å¼ é‡å½¢çŠ¶é”™è¯¯: {masks.shape}ï¼ŒæœŸæœ› [B, N, H, W]")
        return False
    
    # æ£€æŸ¥æ‰¹æ¬¡å¤§å°åŒ¹é…
    if images.shape[0] != masks.shape[0]:
        print(f"æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: å›¾åƒ {images.shape[0]} vs æ©ç  {masks.shape[0]}")
        return False
    
    return True


def create_sam_training_step(model, optimizer, loss_fn, device):
    """åˆ›å»ºSAMè®­ç»ƒæ­¥éª¤å‡½æ•°"""
    
    def training_step(batch: Dict[str, Any]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        # éªŒè¯æ‰¹æ¬¡æ•°æ®
        if not validate_sam_batch(batch):
            return {'error': 1.0}
        
        # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
        inputs, targets = prepare_sam_inputs(batch)
        
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                inputs[key] = value.to(device)
            elif isinstance(value, list):
                inputs[key] = [v.to(device) if isinstance(v, torch.Tensor) else v for v in value]
        
        for key, value in targets.items():
            if isinstance(value, torch.Tensor):
                targets[key] = value.to(device)
        
        # å‰å‘ä¼ æ’­
        model.train()
        predictions = model(inputs)
        
        # è®¡ç®—æŸå¤±
        loss_dict = loss_fn(predictions, targets)
        total_loss = loss_dict['total_loss']
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # è¿”å›æŸå¤±ä¿¡æ¯
        def safe_item(value):
            """å®‰å…¨åœ°è·å–æ•°å€¼"""
            if isinstance(value, torch.Tensor):
                return value.item()
            return float(value)
        
        return {key: safe_item(value) for key, value in loss_dict.items()}
    
    return training_step


def calculate_sam_loss(predictions: Dict[str, torch.Tensor], 
                      ground_truth: Dict[str, torch.Tensor],
                      loss_config: Optional[Dict] = None) -> Dict[str, torch.Tensor]:
    """è®¡ç®—SAMæŸå¤±çš„ä¾¿æ·å‡½æ•°"""
    if loss_config is None:
        loss_config = {
            'focal_loss_weight': 20.0,
            'dice_loss_weight': 1.0,
            'iou_loss_weight': 1.0
        }
    
    loss_fn = SAMLoss(**loss_config)
    return loss_fn(predictions, ground_truth)