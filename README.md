# MicroSAM - ç»†èƒåˆ†å‰²æ¨¡å‹è®­ç»ƒä¸è¯„æµ‹ç³»ç»Ÿ

ä¸€ä¸ªåŸºäºSAM (Segment Anything Model) çš„ç»†èƒåˆ†å‰²æ¨¡å‹è®­ç»ƒä¸æ‰¹é‡è¯„æµ‹ç³»ç»Ÿï¼Œä¸“é—¨é’ˆå¯¹æ˜¾å¾®é•œç»†èƒå›¾åƒè®¾è®¡ï¼Œæ”¯æŒLoRAå¾®è°ƒå’Œå¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‚

## ğŸŒŸ ä¸»è¦ç‰¹æ€§

### ğŸ”¬ ä¸“ä¸šçš„ç»†èƒåˆ†å‰²
- åŸºäºMeta SAMæ¨¡å‹çš„ç»†èƒåˆ†å‰²
- æ”¯æŒå¤šç§ç»†èƒç±»å‹ï¼ˆMSCã€Veroç­‰ï¼‰
- é€‚é…æ˜¾å¾®é•œå›¾åƒçš„ç‰¹æ®Šéœ€æ±‚

### ğŸ¯ LoRAå¾®è°ƒè®­ç»ƒ
- è½»é‡çº§LoRAé€‚é…å™¨å¾®è°ƒ
- æ˜¾è‘—å‡å°‘è®­ç»ƒå‚æ•°å’Œæ—¶é—´
- æ”¯æŒå¤šç§SAMæ¨¡å‹å˜ä½“ï¼ˆvit_t_lmã€vit_b_lmã€vit_l_lmï¼‰
- è‡ªåŠ¨å¤„ç†å¤šå®ä¾‹æ©ç åˆå¹¶

### ğŸ“Š å®Œæ•´çš„è¯„æµ‹æŒ‡æ ‡
- **AP50/AP75** - ä¸åŒIoUé˜ˆå€¼ä¸‹çš„å¹³å‡ç²¾åº¦
- **IoU** - äº¤å¹¶æ¯”
- **Dice** - Diceç³»æ•°
- **HD95** - 95%è±ªæ–¯å¤šå¤«è·ç¦»
- **å¤„ç†æ—¶é—´** - æ¨ç†æ•ˆç‡è¯„ä¼°

### ğŸš€ æ‰¹é‡è¯„æµ‹ç³»ç»Ÿ
- è‡ªåŠ¨å‘ç°å’Œç»„ç»‡æ•°æ®é›†
- å¤šæ¨¡å‹å¹¶è¡Œè¯„æµ‹
- ä¸°å¯Œçš„å¯è§†åŒ–æŠ¥å‘Š
- ç»Ÿä¸€çš„CSVç»“æœè¾“å‡º

## ğŸ“ é¡¹ç›®ç»“æ„

```
microsam/
â”œâ”€â”€ config/                 # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ lora_config.py     # LoRAè®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ settings.py        # è¯„æµ‹ç³»ç»Ÿé…ç½®
â”‚   â””â”€â”€ paths.py           # è·¯å¾„ç®¡ç†
â”œâ”€â”€ core/                   # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ dataset_manager.py # æ•°æ®é›†ç®¡ç†
â”‚   â”œâ”€â”€ evaluator.py       # æ‰¹é‡è¯„æµ‹å™¨
â”‚   â”œâ”€â”€ lora_trainer.py    # LoRAè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ metrics.py         # è¯„æµ‹æŒ‡æ ‡
â”‚   â”œâ”€â”€ model_handler.py   # æ¨¡å‹ç®¡ç†
â”‚   â””â”€â”€ sam_model_loader.py # SAMæ¨¡å‹åŠ è½½
â”œâ”€â”€ lora/                   # LoRAå¾®è°ƒ
â”‚   â”œâ”€â”€ adapters.py        # LoRAé€‚é…å™¨
â”‚   â”œâ”€â”€ data_loaders.py    # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ sam_lora_wrapper.py # SAM LoRAåŒ…è£…
â”‚   â””â”€â”€ training_utils.py  # è®­ç»ƒå·¥å…·
â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ file_utils.py      # æ–‡ä»¶å¤„ç†
â”‚   â”œâ”€â”€ model_utils.py     # æ¨¡å‹å·¥å…·
â”‚   â”œâ”€â”€ report_generator.py # æŠ¥å‘Šç”Ÿæˆ
â”‚   â””â”€â”€ visualization.py   # å¯è§†åŒ–
â”œâ”€â”€ main.py                 # æ‰¹é‡è¯„æµ‹å…¥å£
â””â”€â”€ lora_main.py           # LoRAè®­ç»ƒå…¥å£
```

## ğŸ”§ ç¯å¢ƒå®‰è£…

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- CUDA 11.0+ (æ¨è)
- 8GB+ GPUå†…å­˜

### å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd microsam

# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio
pip install numpy pandas matplotlib seaborn
pip install scikit-image pillow tifffile
pip install albumentations opencv-python
pip install tqdm wandb tensorboard

# å®‰è£…micro_sam
pip install micro-sam

# å¯é€‰ï¼šæ€§èƒ½ç›‘æ§
pip install gputil psutil
```

## ğŸ“‚ æ•°æ®æ ¼å¼

æ”¯æŒçš„æ•°æ®é›†ç›®å½•ç»“æ„ï¼š

```
data/
â”œâ”€â”€ ç»†èƒç±»å‹1/
â”‚   â”œâ”€â”€ æ—¥æœŸ1/
â”‚   â”‚   â”œâ”€â”€ æ”¾å¤§å€æ•°1/
â”‚   â”‚   â”‚   â”œâ”€â”€ images/        # åŸå§‹å›¾åƒ
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ img1.jpg
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ img2.png
â”‚   â”‚   â”‚   â””â”€â”€ masks/         # åˆ†å‰²æ©ç 
â”‚   â”‚   â”‚       â”œâ”€â”€ img1_seg.png
â”‚   â”‚   â”‚       â””â”€â”€ img2_seg.png
â”‚   â”‚   â””â”€â”€ æ”¾å¤§å€æ•°2/
â”‚   â””â”€â”€ æ—¥æœŸ2/
â””â”€â”€ ç»†èƒç±»å‹2/
```

**æ”¯æŒçš„å›¾åƒæ ¼å¼ï¼š** JPG, PNG, TIF, TIFF  
**æ©ç æ ¼å¼ï¼š** PNG, TIF (æ ‡ç­¾å›¾æˆ–äºŒå€¼å›¾)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ‰¹é‡è¯„æµ‹ (é›¶ä»£ç è®­ç»ƒ)

```bash
# åŸºæœ¬è¯„æµ‹
python main.py --data-dir /path/to/data --output-dir ./results

# æŒ‡å®šæ¨¡å‹å’Œç»†èƒç±»å‹
python main.py \
    --data-dir /path/to/data \
    --models vit_b_lm vit_l_lm \
    --cell-types MSC Vero \
    --output-dir ./results

# ä½¿ç”¨é¢„è®¾é…ç½®
python main.py --preset fast --data-dir /path/to/data
```

### 2. LoRAå¾®è°ƒè®­ç»ƒ

```bash
# å¿«é€Ÿè®­ç»ƒ (è°ƒè¯•ç”¨)
python lora_main.py train \
    --preset quick \
    --data-dir /path/to/data

# æ ‡å‡†è®­ç»ƒ
python lora_main.py train \
    --data-dir /path/to/data \
    --model vit_b_lm \
    --epochs 10 \
    --batch-size 8 \
    --learning-rate 1e-4

# è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹
python lora_main.py train-and-eval \
    --data-dir /path/to/train_data \
    --eval-data /path/to/test_data
```

### 3. æ¢å¤è®­ç»ƒ

```bash
python lora_main.py resume \
    --checkpoint ./lora_experiments/checkpoints/checkpoint_epoch_5.pth
```

## âš™ï¸ é«˜çº§é…ç½®

### LoRAè®­ç»ƒé…ç½®

```python
# config/lora_config.py
config = LoRATrainingSettings(
    lora=LoRAConfig(
        rank=8,                    # LoRA rank
        alpha=16.0,               # LoRA alpha
        dropout=0.1,              # Dropoutç‡
        target_modules=["qkv", "proj"]  # ç›®æ ‡æ¨¡å—
    ),
    training=TrainingConfig(
        learning_rate=1e-4,
        batch_size=8,
        num_epochs=10
    ),
    model=ModelConfig(
        base_model_name="vit_b_lm",
        apply_lora_to=["image_encoder"]
    )
)
```

### è¯„æµ‹ç³»ç»Ÿé…ç½®

```python
# config/settings.py
config = BatchEvaluationSettings(
    models=[
        ModelConfig("vit_t_lm"),
        ModelConfig("vit_b_lm"),
        ModelConfig("vit_l_lm")
    ],
    evaluation=EvaluationConfig(
        batch_size=None,  # å¤„ç†æ‰€æœ‰å›¾åƒ
        skip_existing=True,
        save_visualizations=True
    )
)
```

## ğŸ“Š ç»“æœè¾“å‡º

### è®­ç»ƒç»“æœ
```
lora_experiments/
â”œâ”€â”€ experiment_name/
â”‚   â”œâ”€â”€ config.json          # è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ checkpoints/         # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ final_model/         # æœ€ç»ˆæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ lora_weights.pth
â”‚   â”‚   â””â”€â”€ merged_sam_model.pth
â”‚   â””â”€â”€ logs/               # TensorBoardæ—¥å¿—
```

### è¯„æµ‹ç»“æœ
```
batch_evaluation_results/
â”œâ”€â”€ model_name/
â”‚   â””â”€â”€ dataset_id/
â”‚       â”œâ”€â”€ results.csv      # è¯¦ç»†ç»“æœ
â”‚       â””â”€â”€ summary.json     # æ‘˜è¦ç»Ÿè®¡
â””â”€â”€ summary_report_timestamp/
    â”œâ”€â”€ final_evaluation_summary.csv
    â”œâ”€â”€ model_comparison_statistics.csv
    â”œâ”€â”€ executive_summary.json
    â””â”€â”€ visualizations/      # å¯è§†åŒ–å›¾è¡¨
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### ä½¿ç”¨Weights & Biases

```bash
# å¯ç”¨W&Bç›‘æ§
python lora_main.py train \
    --data-dir /path/to/data \
    --use-wandb \
    --wandb-project "sam_cell_segmentation"
```

### TensorBoard

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir ./lora_experiments/logs
```

## ğŸ› ï¸ å‘½ä»¤è¡Œå·¥å…·

### è®­ç»ƒç›¸å…³å‘½ä»¤

```bash
# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
python lora_main.py info --model vit_b_lm

# å‡†å¤‡æ•°æ®é›†
python lora_main.py prepare-data \
    --data-dir /path/to/data \
    --train-ratio 0.8 \
    --val-ratio 0.1

# è¯„æµ‹LoRAæ¨¡å‹
python lora_main.py evaluate \
    --lora-model ./lora_experiments/final_model \
    --eval-data /path/to/test_data
```

### è¯„æµ‹ç›¸å…³å‘½ä»¤

```bash
# é¢„è§ˆæ¨¡å¼ (ä¸å®é™…è¿è¡Œ)
python main.py --dry-run --data-dir /path/to/data

# æŒ‡å®šç‰¹å®šæ•°æ®
python main.py \
    --data-dir /path/to/data \
    --cell-types MSC \
    --dates 2024-01-01 \
    --magnifications 10x
```

## ğŸ”¬ æŠ€æœ¯ç‰¹æ€§

### LoRAå¾®è°ƒä¼˜åŠ¿
- **å‚æ•°æ•ˆç‡**: åªè®­ç»ƒ1-5%çš„å‚æ•°
- **å†…å­˜å‹å¥½**: æ˜¾è‘—é™ä½GPUå†…å­˜éœ€æ±‚
- **è®­ç»ƒå¿«é€Ÿ**: å¤§å¹…å‡å°‘è®­ç»ƒæ—¶é—´
- **æ˜“äºéƒ¨ç½²**: è½»é‡çº§é€‚é…å™¨ä¾¿äºåˆ†å‘

### å¤šå®ä¾‹æ©ç å¤„ç†
- è‡ªåŠ¨æ£€æµ‹å¤šå®ä¾‹æ©ç 
- æ™ºèƒ½åˆå¹¶ä¸ºäºŒè¿›åˆ¶æ©ç 
- å…¼å®¹SAMå•æ©ç è¾“å‡ºé™åˆ¶
- ä¿æŒè®­ç»ƒç¨³å®šæ€§

### è¯„æµ‹ç³»ç»Ÿç‰¹ç‚¹
- **è‡ªåŠ¨å‘ç°**: æ™ºèƒ½æ‰«ææ•°æ®é›†ç›®å½•
- **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¤šGPUå¹¶è¡Œè¯„æµ‹
- **å®¹é”™æœºåˆ¶**: è‡ªåŠ¨è·³è¿‡æŸåæ•°æ®
- **è¿›åº¦è¿½è¸ª**: å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒä¼˜åŒ–
```bash
# å¤§æ‰¹é‡è®­ç»ƒ (éœ€è¦æ›´å¤šGPUå†…å­˜)
python lora_main.py train \
    --batch-size 16 \
    --learning-rate 2e-4

# å°å†…å­˜è®­ç»ƒ
python lora_main.py train \
    --batch-size 4 \
    --rank 4 \
    --alpha 8
```

### è¯„æµ‹ä¼˜åŒ–
```bash
# é™åˆ¶å¤„ç†æ•°é‡ (å¿«é€Ÿè¯„æµ‹)
python main.py \
    --data-dir /path/to/data \
    --batch-size 10

# è·³è¿‡å¯è§†åŒ– (èŠ‚çœæ—¶é—´)
python main.py \
    --data-dir /path/to/data \
    --no-visualizations
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: è®­ç»ƒæ—¶å‡ºç°å†…å­˜ä¸è¶³é”™è¯¯
**A**: å‡å°‘æ‰¹å¤§å°æˆ–ä½¿ç”¨æ›´å°çš„LoRA rank
```bash
python lora_main.py train --batch-size 4 --rank 4
```

### Q: æ•°æ®é›†æœªè¢«è¯†åˆ«
**A**: æ£€æŸ¥ç›®å½•ç»“æ„ï¼Œç¡®ä¿imageså’Œmasksç›®å½•å­˜åœ¨ä¸”é…å¯¹
```bash
python lora_main.py prepare-data --data-dir /path/to/data
```

### Q: è¯„æµ‹ç»“æœå¼‚å¸¸
**A**: æ£€æŸ¥æ©ç æ ¼å¼ï¼Œç¡®ä¿æ˜¯æ ‡ç­¾å›¾æˆ–äºŒå€¼å›¾
```bash
# æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡
python main.py --dry-run --data-dir /path/to/data
```

## ğŸ“ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„è¯„æµ‹æŒ‡æ ‡

```python
# core/metrics.py
class CustomMetrics:
    def calculate_custom_metric(self, gt_mask, pred_mask):
        # å®ç°è‡ªå®šä¹‰æŒ‡æ ‡
        return metric_value
```

### è‡ªå®šä¹‰LoRAé…ç½®

```python
# config/lora_config.py
custom_config = LoRATrainingSettings(
    lora=LoRAConfig(
        rank=16,
        target_modules=["custom_module"]
    )
)
```

## ğŸ” ç³»ç»Ÿè¦æ±‚æ£€æŸ¥

```bash
# æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ
python lora_main.py info

# éªŒè¯æ•°æ®é›†
python main.py --dry-run --data-dir /path/to/data
```

## ğŸ“ æ”¯æŒä¸è´¡çŒ®

- **é—®é¢˜æŠ¥å‘Š**: è¯·åœ¨GitHub Issuesä¸­æŠ¥å‘Šbug
- **åŠŸèƒ½å»ºè®®**: æ¬¢è¿æäº¤feature requests
- **è´¡çŒ®ä»£ç **: è¯·éµå¾ªä»£ç è§„èŒƒï¼Œæäº¤PRå‰è¯·æµ‹è¯•

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- Meta AIçš„Segment Anything Model (SAM)
- MicroSAMé¡¹ç›®çš„ä¼˜ç§€å·¥ä½œ
- å¼€æºç¤¾åŒºçš„å®è´µè´¡çŒ®

---

**å¿«é€Ÿå¼€å§‹**: `python main.py --preset fast --data-dir /your/data/path`

**è·å–å¸®åŠ©**: `python main.py --help` æˆ– `python lora_main.py --help`