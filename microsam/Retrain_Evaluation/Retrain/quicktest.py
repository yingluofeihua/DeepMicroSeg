#!/usr/bin/env python3
"""
å¿«é€ŸMicroSAMè®­ç»ƒæµ‹è¯•è„šæœ¬ - 5åˆ†é’Ÿå†…å®ŒæˆéªŒè¯
ç”¨äºéªŒè¯æ•°æ®åŠ è½½å™¨å’Œè®­ç»ƒæµç¨‹çš„å…¼å®¹æ€§ï¼Œé¿å…é•¿æ—¶é—´è®­ç»ƒåå¤±è´¥
"""

import os
import sys
import time
import torch
import numpy as np
from pathlib import Path
from datetime import datetime

# æ·»åŠ ä½ çš„é¡¹ç›®è·¯å¾„
sys.path.append("/LD-FS/home/yunshuchen/DeepMicroSeg/microsam/Retrain_Evaluation/Retrain")

# è®¾ç½®ç¯å¢ƒ
os.environ["MICROSAM_CACHEDIR"] = "/LD-FS/data/Model/micro_sam"

def quick_test_microsam():
    """å¿«é€Ÿæµ‹è¯•MicroSAMè®­ç»ƒå…¼å®¹æ€§"""
    print("="*60)
    print("MicroSAM å¿«é€Ÿå…¼å®¹æ€§æµ‹è¯•")
    print("="*60)
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    try:
        from retrain import (
            OptimizedDatasetHandler, 
            DetailedLogger,
            CompatibleMicroSAMDataset,
            MicroSAMDataLoader
        )
        print("âœ“ æˆåŠŸå¯¼å…¥æ‰€éœ€æ¨¡å—")
    except ImportError as e:
        print(f"âœ— æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = Path("/tmp/microsam_quick_test")
    test_dir.mkdir(exist_ok=True)
    
    # åˆå§‹åŒ–logger
    logger = DetailedLogger(test_dir)
    logger.log_info("å¼€å§‹å¿«é€Ÿå…¼å®¹æ€§æµ‹è¯•...")
    
    # æµ‹è¯•JSONæ–‡ä»¶ï¼ˆåªä½¿ç”¨ä¸€ä¸ªå°æ•°æ®é›†ï¼‰
    json_files = [
        "/LD-FS/data/public_dataset/Retrain/mappings/YIM_mapping.json"  # é€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†
    ]
    
    # éªŒè¯JSONæ–‡ä»¶å­˜åœ¨
    for json_file in json_files:
        if not Path(json_file).exists():
            logger.log_error(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
            return False
    
    logger.log_info("âœ“ JSONæ–‡ä»¶éªŒè¯é€šè¿‡")
    
    try:
        # Phase 1: å¿«é€Ÿæ•°æ®é›†å¤„ç†ï¼ˆé™åˆ¶è¡¥ä¸æ•°é‡ï¼‰
        logger.log_info("Phase 1: å¿«é€Ÿæ•°æ®é›†å¤„ç†...")
        
        dataset_handler = OptimizedDatasetHandler(
            json_files=json_files,
            train_ratio=0.8,
            patch_size=512,
            overlap=10,
            logger=logger,
            force_regenerate=False,  # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
            model_name="quick_test"
        )
        
        # é™åˆ¶æ•°æ®é‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        max_patches_for_test = 150
        if len(dataset_handler.all_patches) > max_patches_for_test:
            logger.log_info(f"é™åˆ¶æµ‹è¯•æ•°æ®é‡: {len(dataset_handler.all_patches)} -> {max_patches_for_test}")
            dataset_handler.all_patches = dataset_handler.all_patches[:max_patches_for_test]
            dataset_handler.split_train_val()
        
        logger.log_info(f"âœ“ æ•°æ®é›†å¤„ç†å®Œæˆ: {len(dataset_handler.train_patches)} è®­ç»ƒ, {len(dataset_handler.val_patches)} éªŒè¯")
        
        # Phase 2: æ•°æ®åŠ è½½å™¨æµ‹è¯•
        logger.log_info("Phase 2: æ•°æ®åŠ è½½å™¨å…¼å®¹æ€§æµ‹è¯•...")
        
        # å¢å¼ºæ•°æ®éªŒè¯
        train_count, val_count = dataset_handler.enhance_data_validation()
        logger.log_info(f"âœ“ æ•°æ®éªŒè¯å®Œæˆ: {train_count} è®­ç»ƒ, {val_count} éªŒè¯")
        
        if train_count < 5 or val_count < 2:
            logger.log_warning("æ•°æ®é‡å¤ªå°‘ï¼Œåˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†...")
            # åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†
            train_count, val_count = create_minimal_test_dataset(dataset_handler, logger)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = dataset_handler.create_dataloaders(batch_size=1, num_workers=0)
        logger.log_info("âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # Phase 3: æ•°æ®åŠ è½½æµ‹è¯•
        logger.log_info("Phase 3: æ•°æ®åŠ è½½å’Œåºåˆ—åŒ–æµ‹è¯•...")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        test_iterations = 3
        for i in range(test_iterations):
            try:
                batch = next(iter(train_loader))
                img_batch, mask_batch = batch
                
                logger.log_info(f"  æµ‹è¯•æ‰¹æ¬¡ {i+1}: å›¾åƒ {img_batch.shape}, æ©ç  {mask_batch.shape}")
                logger.log_info(f"    å›¾åƒèŒƒå›´: [{img_batch.min():.1f}, {img_batch.max():.1f}]")
                logger.log_info(f"    æ©ç èŒƒå›´: [{mask_batch.min():.3f}, {mask_batch.max():.3f}]")
                
                # éªŒè¯æ•°æ®èŒƒå›´
                if img_batch.min() < 0 or img_batch.max() > 255 or img_batch.max() <= 1.0:
                    raise ValueError(f"å›¾åƒæ•°æ®èŒƒå›´é”™è¯¯: [{img_batch.min()}, {img_batch.max()}]")
                
                # éªŒè¯å‰æ™¯å­˜åœ¨
                if len(mask_batch.shape) == 4 and mask_batch.shape[1] == 4:
                    foreground_pixels = (mask_batch[:, 2, :, :] > 0).sum().item()
                    if foreground_pixels < 10:
                        raise ValueError(f"å‰æ™¯åƒç´ ä¸è¶³: {foreground_pixels}")
                    logger.log_info(f"    âœ“ å‰æ™¯åƒç´ : {foreground_pixels}")
                
            except Exception as e:
                logger.log_error(f"æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥ (æ‰¹æ¬¡ {i+1}): {e}")
                return False
        
        logger.log_info("âœ“ æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡")
        
        # Phase 4: åºåˆ—åŒ–æµ‹è¯•ï¼ˆå…³é”®æµ‹è¯•ï¼‰
        logger.log_info("Phase 4: PyTorchåºåˆ—åŒ–å…¼å®¹æ€§æµ‹è¯•...")
        
        # æµ‹è¯•æ•°æ®é›†åºåˆ—åŒ–
        try:
            import pickle
            
            # æµ‹è¯•è®­ç»ƒæ•°æ®é›†åºåˆ—åŒ–
            train_dataset = train_loader.dataset
            serialized_train = pickle.dumps(train_dataset)
            deserialized_train = pickle.loads(serialized_train)
            logger.log_info("âœ“ è®­ç»ƒæ•°æ®é›†åºåˆ—åŒ–æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•éªŒè¯æ•°æ®é›†åºåˆ—åŒ–
            val_dataset = val_loader.dataset
            serialized_val = pickle.dumps(val_dataset)
            deserialized_val = pickle.loads(serialized_val)
            logger.log_info("âœ“ éªŒè¯æ•°æ®é›†åºåˆ—åŒ–æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•æ•°æ®åŠ è½½å™¨åºåˆ—åŒ–
            serialized_loader = pickle.dumps(train_loader)
            deserialized_loader = pickle.loads(serialized_loader)
            logger.log_info("âœ“ æ•°æ®åŠ è½½å™¨åºåˆ—åŒ–æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            logger.log_error(f"åºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}")
            logger.log_error("è¿™ä¼šå¯¼è‡´ä¿å­˜checkpointæ—¶å¤±è´¥ï¼")
            return False
        
        # Phase 5: æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤æµ‹è¯•
        logger.log_info("Phase 5: æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤æµ‹è¯•...")
        
        try:
            # å¯¼å…¥micro_samè®­ç»ƒç›¸å…³æ¨¡å—
            import micro_sam.training as sam_training
            
            # åˆ›å»ºæ¨¡å‹ï¼ˆä¸åŠ è½½æƒé‡ï¼Œä»…æµ‹è¯•å…¼å®¹æ€§ï¼‰
            logger.log_info("  åˆ›å»ºSAMæ¨¡å‹...")
            model = sam_training.get_trainable_sam_model(
                model_type="vit_t_lm",  # ä½¿ç”¨æœ€å°çš„æ¨¡å‹åŠ é€Ÿæµ‹è¯•
                device=torch.device("cpu"),  # ä½¿ç”¨CPUé¿å…GPUå†…å­˜é—®é¢˜
                checkpoint_path=None
            )
            logger.log_info("âœ“ SAMæ¨¡å‹åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
            logger.log_info("  æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
            batch = next(iter(train_loader))
            img_batch, mask_batch = batch
            
            # åˆ›å»ºæ¨¡æ‹Ÿçš„SAMè¾“å…¥
            convert_inputs = sam_training.ConvertToSamInputs()
            batched_inputs, sampled_ids = convert_inputs(
                img_batch, mask_batch, 
                n_pos=1, n_neg=0, get_boxes=False, n_objects_per_batch=5
            )
            
            logger.log_info("âœ“ SAMè¾“å…¥è½¬æ¢æˆåŠŸ")
            logger.log_info(f"  æ‰¹æ¬¡è¾“å…¥æ•°é‡: {len(batched_inputs)}")
            logger.log_info(f"  é‡‡æ ·IDæ•°é‡: {len(sampled_ids)}")
            
        except Exception as e:
            logger.log_error(f"æ¨¡æ‹Ÿè®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
            return False
        
        # Phase 6: å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ1ä¸ªepochï¼‰
        logger.log_info("Phase 6: å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ1ä¸ªepochï¼‰...")
        
        try:
            from micro_sam.training import train_sam
            
            # ä½¿ç”¨æœ€å°é…ç½®è¿›è¡Œ1ä¸ªepochçš„è®­ç»ƒæµ‹è¯•
            logger.log_info("  å¼€å§‹1ä¸ªepochè®­ç»ƒæµ‹è¯•...")
            start_time = time.time()
            
            train_sam(
                name="quick_test_model",
                model_type="vit_t_lm",  # æœ€å°æ¨¡å‹
                train_loader=train_loader,
                val_loader=val_loader,
                n_epochs=1,  # åªè®­ç»ƒ1ä¸ªepoch
                n_objects_per_batch=5,  # å‡å°‘å¯¹è±¡æ•°é‡
                with_segmentation_decoder=True,
                device=torch.device("cpu"),  # ä½¿ç”¨CPU
                lr=1e-4,
                save_root=str(test_dir),
                early_stopping=None,  # ç¦ç”¨æ—©åœ
                n_iterations=10,  # é™åˆ¶è¿­ä»£æ¬¡æ•°
                save_every_kth_epoch=None,  # ä¸ä¿å­˜ä¸­é—´checkpoint
                overwrite_training=True
            )
            
            training_time = time.time() - start_time
            logger.log_info(f"âœ“ å¿«é€Ÿè®­ç»ƒæµ‹è¯•å®Œæˆï¼ç”¨æ—¶: {training_time:.1f}ç§’")
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†checkpoint
            checkpoint_path = test_dir / "checkpoints" / "quick_test_model" / "best.pt"
            if checkpoint_path.exists():
                logger.log_info("âœ“ Checkpointä¿å­˜æˆåŠŸ")
                
                # æµ‹è¯•checkpointåŠ è½½
                checkpoint = torch.load(checkpoint_path, map_location="cpu")
                logger.log_info("âœ“ CheckpointåŠ è½½æˆåŠŸ")
            else:
                logger.log_warning("âš  Checkpointæœªæ‰¾åˆ°ï¼Œä½†è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            logger.log_error(f"å¿«é€Ÿè®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            logger.log_error(traceback.format_exc())
            return False
        
        # å…¨éƒ¨æµ‹è¯•é€šè¿‡
        logger.log_info("="*60)
        logger.log_info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MicroSAMè®­ç»ƒç¯å¢ƒå…¼å®¹æ€§éªŒè¯æˆåŠŸ")
        logger.log_info("="*60)
        logger.log_info("æµ‹è¯•æ€»ç»“:")
        logger.log_info("  âœ“ æ•°æ®é›†å¤„ç†å’Œè¡¥ä¸æå–")
        logger.log_info("  âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºå’ŒéªŒè¯")
        logger.log_info("  âœ“ æ•°æ®èŒƒå›´å’Œå‰æ™¯å¯¹è±¡éªŒè¯")
        logger.log_info("  âœ“ PyTorchåºåˆ—åŒ–å…¼å®¹æ€§")
        logger.log_info("  âœ“ SAMæ¨¡å‹å’Œè¾“å…¥è½¬æ¢")
        logger.log_info("  âœ“ å¿«é€Ÿè®­ç»ƒæµç¨‹ï¼ˆ1ä¸ªepochï¼‰")
        logger.log_info("  âœ“ Checkpointä¿å­˜å’ŒåŠ è½½")
        logger.log_info("")
        logger.log_info("ğŸš€ å¯ä»¥å®‰å…¨è¿›è¡Œå®Œæ•´è®­ç»ƒï¼")
        return True
        
    except Exception as e:
        logger.log_error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.log_error(traceback.format_exc())
        return False
    
    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        cleanup_test_files(test_dir, logger)


def create_minimal_test_dataset(dataset_handler, logger):
    """åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†"""
    logger.log_info("åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†...")
    
    # åˆ›å»ºå‡ ä¸ªæœ‰æ•ˆçš„æµ‹è¯•è¡¥ä¸
    test_patch_dir = Path("/tmp/microsam_test_patches")
    test_patch_dir.mkdir(exist_ok=True)
    
    minimal_patches = []
    
    for i in range(10):  # åˆ›å»º10ä¸ªæµ‹è¯•è¡¥ä¸
        # åˆ›å»ºæµ‹è¯•å›¾åƒï¼ˆ512x512ï¼Œ[0,255]èŒƒå›´ï¼‰
        test_img = np.random.randint(50, 200, (512, 512), dtype=np.uint8)
        
        # åˆ›å»ºæµ‹è¯•æ©ç ï¼ˆæœ‰å‰æ™¯å¯¹è±¡ï¼‰
        test_mask = np.zeros((512, 512), dtype=np.uint8)
        
        # åœ¨ä¸­å¿ƒæ·»åŠ ä¸€äº›éšæœºå¯¹è±¡
        for obj_id in range(1, 4):  # 3ä¸ªå¯¹è±¡
            center_y = 256 + np.random.randint(-100, 100)
            center_x = 256 + np.random.randint(-100, 100)
            radius = np.random.randint(20, 40)
            
            y, x = np.ogrid[:512, :512]
            mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
            test_mask[mask] = obj_id
        
        # ä¿å­˜æµ‹è¯•æ–‡ä»¶
        img_path = test_patch_dir / f"test_patch_{i}_img.png"
        mask_path = test_patch_dir / f"test_patch_{i}_mask.png"
        
        from PIL import Image
        Image.fromarray(test_img, mode='L').save(img_path)
        Image.fromarray(test_mask, mode='L').save(mask_path)
        
        # æ·»åŠ åˆ°è¡¥ä¸åˆ—è¡¨
        minimal_patches.append({
            'img_path': str(img_path),
            'mask_path': str(mask_path),
            'dataset': 'test',
            'model_name': 'quick_test',
            'original_image': f'test_image_{i}',
            'patch_info': {
                'patch_id': i,
                'position': (0, 0),
                'size': (512, 512),
                'foreground_pixels': np.sum(test_mask > 0)
            }
        })
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯
    dataset_handler.train_patches = minimal_patches[:8]
    dataset_handler.val_patches = minimal_patches[8:]
    
    logger.log_info(f"âœ“ åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†: {len(dataset_handler.train_patches)} è®­ç»ƒ, {len(dataset_handler.val_patches)} éªŒè¯")
    
    return len(dataset_handler.train_patches), len(dataset_handler.val_patches)


def cleanup_test_files(test_dir, logger):
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    try:
        import shutil
        
        # æ¸…ç†æµ‹è¯•è¡¥ä¸
        test_patch_dir = Path("/tmp/microsam_test_patches")
        if test_patch_dir.exists():
            shutil.rmtree(test_patch_dir)
        
        # æ¸…ç†æµ‹è¯•ç›®å½•
        if test_dir.exists():
            shutil.rmtree(test_dir)
        
        logger.log_info("âœ“ æµ‹è¯•æ–‡ä»¶æ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.log_warning(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")


if __name__ == "__main__":
    success = quick_test_microsam()
    
    if success:
        print("\n" + "="*60)
        print("ğŸ‰ å¿«é€Ÿæµ‹è¯•é€šè¿‡ï¼å¯ä»¥è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚")
        print("å»ºè®®çš„å®Œæ•´è®­ç»ƒå‘½ä»¤:")
        print("python retrain.py --model-type vit_l_lm --epochs 30")
        print("="*60)
        sys.exit(0)
    else:
        print("\n" + "="*60)
        print("âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥ï¼è¯·ä¿®å¤é—®é¢˜åå†è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚")
        print("="*60)
        sys.exit(1)