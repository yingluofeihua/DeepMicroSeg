"""
LoRAè®­ç»ƒä¸»å…¥å£æ–‡ä»¶ (å¢å¼ºç‰ˆ)
é›†æˆå®Œæ•´çš„LoRAå¾®è°ƒã€è¯„æµ‹å’Œæ‰¹é‡æ¨ç†æµ‹è¯•åŠŸèƒ½
æ”¯æŒæ•°æ®é›†åˆ’åˆ†ã€è¯¦ç»†è¯„æµ‹æŠ¥å‘Šå’Œæ€§èƒ½åˆ†æ
"""

import sys
import argparse
from pathlib import Path
import json
import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from config.lora_config import LoRATrainingSettings, LORA_PRESET_CONFIGS, get_config_for_model
from core.lora_trainer import LoRATrainer, create_trainer_from_config, resume_training
from core.evaluator import BatchEvaluator
from core.dataset_manager import DatasetManager
from config.settings import BatchEvaluationSettings
from lora.data_loaders import split_dataset, list_cached_splits, clean_old_splits, preview_data_split
from utils.file_utils import setup_logging
from utils.model_utils import get_device_info, optimize_memory
from utils.data_splitter import DatasetSplitter, print_split_summary, DataSplit

# ğŸ”§ æ–°å¢å¯¼å…¥ - è¯„æµ‹ç›¸å…³
from lora.stable_sam_lora_wrapper import load_stable_sam_lora_model
from core.metrics import ComprehensiveMetrics
from lora.data_loaders import SAMDataset, collate_fn
from config.lora_config import DataConfig


class EnhancedLoRAEvaluator:
    """å¢å¼ºç‰ˆLoRAè¯„æµ‹å™¨ - é›†æˆåˆ°lora_main.py"""
    
    def __init__(self, lora_model_path: str, device: str = "auto"):
        self.lora_model_path = Path(lora_model_path)
        self.device = self._setup_device(device)
        self.model = None
        self.metrics_calculator = ComprehensiveMetrics(enable_hd95=True)
        
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        return torch.device(device)
    
    def load_model(self) -> bool:
        """åŠ è½½LoRAæ¨¡å‹"""
        try:
            print(f"æ­£åœ¨åŠ è½½LoRAæ¨¡å‹: {self.lora_model_path}")
            
            # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
            config_file = self.lora_model_path / "sam_lora_config.json"
            if config_file.exists():
                with open(config_file, 'r') as f:
                    config = json.load(f)
                model_type = config.get('model_type', 'vit_b_lm')
            else:
                model_type = 'vit_b_lm'
                print("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ç±»å‹: vit_b_lm")
            
            self.model = load_stable_sam_lora_model(model_type, str(self.lora_model_path), str(self.device))
            
            if self.model is None:
                print("âŒ LoRAæ¨¡å‹åŠ è½½å¤±è´¥")
                return False
            
            self.model.eval()
            print(f"âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {self.device}")
            return True
            
        except Exception as e:
            print(f"åŠ è½½LoRAæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def evaluate_with_split_file(self, split_file: str, args) -> dict:
        """ä½¿ç”¨æ•°æ®åˆ’åˆ†æ–‡ä»¶è¿›è¡Œè¯„æµ‹"""
        print(f"ä½¿ç”¨æ•°æ®åˆ’åˆ†æ–‡ä»¶è¿›è¡Œè¯„æµ‹: {split_file}")
        
        # åŠ è½½æ•°æ®åˆ’åˆ†
        try:
            with open(split_file, 'r', encoding='utf-8') as f:
                split_data = json.load(f)
            
            split_result = DataSplit.from_dict(split_data)
            test_samples = split_result.test_samples
            
            print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_samples)}")
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®åˆ’åˆ†æ–‡ä»¶å¤±è´¥: {e}")
            return {}
        
        # éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        valid_samples = []
        for sample in test_samples:
            img_path = Path(sample['image_path'])
            mask_path = Path(sample['mask_path'])
            if img_path.exists() and mask_path.exists():
                valid_samples.append(sample)
        
        print(f"æœ‰æ•ˆæµ‹è¯•æ ·æœ¬æ•°: {len(valid_samples)}")
        
        if not valid_samples:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ ·æœ¬")
            return {}
        
        # é™åˆ¶è¯„æµ‹æ•°é‡
        max_samples = getattr(args, 'max_samples', None)
        if max_samples and max_samples < len(valid_samples):
            valid_samples = valid_samples[:max_samples]
            print(f"é™åˆ¶è¯„æµ‹æ ·æœ¬æ•°ä¸º: {max_samples}")
        
        return self._run_evaluation(valid_samples, args)
    
    def _run_evaluation(self, test_samples: list, args) -> dict:
        """æ‰§è¡Œè¯„æµ‹"""
        if not self.model:
            if not self.load_model():
                return {}
        
        # åˆ›å»ºæ•°æ®é›†
        config = DataConfig()
        config.batch_size = getattr(args, 'eval_batch_size', 1)
        
        test_dataset = SAMDataset(
            data_dir=None,
            config=config,
            split='test',
            samples=test_samples
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=2,
            collate_fn=collate_fn
        )
        
        print("å¼€å§‹æ¨¡å‹è¯„æµ‹...")
        
        results = []
        total_processing_time = 0.0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(test_loader, desc="è¯„æµ‹è¿›åº¦")):
                try:
                    # å‡†å¤‡è¾“å…¥
                    from lora.training_utils import prepare_sam_inputs
                    
                    inputs, targets = prepare_sam_inputs(batch)
                    
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            inputs[key] = value.to(self.device)
                        elif isinstance(value, list):
                            inputs[key] = [v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value]
                    
                    for key, value in targets.items():
                        if isinstance(value, torch.Tensor):
                            targets[key] = value.to(self.device)
                    
                    # è®¡ç®—å¤„ç†æ—¶é—´
                    start_time = time.time()
                    
                    # æ¨¡å‹æ¨ç†
                    predictions = self.model(inputs)
                    
                    processing_time = time.time() - start_time
                    total_processing_time += processing_time
                    
                    # åå¤„ç†é¢„æµ‹ç»“æœ
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    target_masks = targets['masks'].cpu().numpy()
                    
                    # è·å–æ ·æœ¬ä¿¡æ¯
                    sample_ids = batch.get('sample_ids', [f"sample_{batch_idx}"])
                    
                    # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ ·æœ¬
                    for i in range(pred_masks.shape[0]):
                        result = self._process_single_prediction(
                            pred_masks[i], target_masks[i], 
                            sample_ids[i] if i < len(sample_ids) else f"sample_{batch_idx}_{i}",
                            processing_time / pred_masks.shape[0],
                            test_samples[batch_idx * pred_masks.shape[0] + i] if test_samples else {}
                        )
                        
                        if result:
                            results.append(result)
                    
                except Exception as e:
                    print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        # ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š
        return self._generate_evaluation_report(results, total_processing_time, args)
    
    def _process_single_prediction(self, pred_mask: np.ndarray, target_mask: np.ndarray,
                                 sample_id: str, processing_time: float,
                                 sample_info: dict) -> dict:
        """å¤„ç†å•ä¸ªé¢„æµ‹ç»“æœ"""
        try:
            # å¤„ç†é¢„æµ‹æ©ç 
            if len(pred_mask.shape) > 2:
                pred_mask = pred_mask[0] if pred_mask.shape[0] > 0 else np.zeros(pred_mask.shape[1:])
            
            # å¤„ç†ç›®æ ‡æ©ç 
            if len(target_mask.shape) > 2:
                target_mask = (target_mask.sum(axis=0) > 0).astype(float)
            
            # è°ƒæ•´å°ºå¯¸åŒ¹é…ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if pred_mask.shape != target_mask.shape:
                import torch.nn.functional as F
                pred_tensor = torch.from_numpy(pred_mask).unsqueeze(0).unsqueeze(0).float()
                target_size = target_mask.shape
                pred_resized = F.interpolate(pred_tensor, size=target_size, mode='bilinear', align_corners=False)
                pred_mask = pred_resized.squeeze().numpy()
            
            # äºŒå€¼åŒ–
            pred_binary = (pred_mask > 0.5).astype(int)
            target_binary = (target_mask > 0.5).astype(int)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics_result = self.metrics_calculator.compute_all_metrics(target_binary, pred_binary)
            
            # æ„å»ºç»“æœå­—å…¸
            result = metrics_result.to_dict()
            result.update({
                'sample_id': sample_id,
                'processing_time': processing_time,
                'cell_type': sample_info.get('cell_type', 'unknown'),
                'date': sample_info.get('date', 'unknown'),
                'magnification': sample_info.get('magnification', 'unknown'),
                'dataset_id': sample_info.get('dataset_id', 'unknown'),
                'image_path': sample_info.get('image_path', ''),
                'mask_path': sample_info.get('mask_path', '')
            })
            
            return result
            
        except Exception as e:
            print(f"å¤„ç†å•ä¸ªé¢„æµ‹å¤±è´¥ {sample_id}: {e}")
            return None
    
    def _generate_evaluation_report(self, results: list, total_processing_time: float, args) -> dict:
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        if not results:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ç»“æœ")
            return {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = getattr(args, 'eval_output', None)
        if output_dir:
            output_dir = Path(output_dir)
        else:
            output_dir = self.lora_model_path.parent / "evaluation_results"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if getattr(args, 'save_detailed', True):
            df = pd.DataFrame(results)
            detailed_file = output_dir / "detailed_evaluation_results.csv"
            df.to_csv(detailed_file, index=False)
            print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_file}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._calculate_average_metrics(results)
        
        # æ·»åŠ å…ƒæ•°æ®
        avg_metrics.update({
            'model_path': str(self.lora_model_path),
            'total_samples': len(results),
            'total_processing_time': total_processing_time,
            'average_processing_time_per_sample': total_processing_time / len(results),
            'device': str(self.device),
            'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
        
        # ä¿å­˜æ‘˜è¦ç»“æœ
        summary_file = output_dir / "evaluation_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            def convert_numpy(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                return obj
            
            json.dump(avg_metrics, f, indent=2, ensure_ascii=False, default=convert_numpy)
        
        print(f"è¯„æµ‹æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_evaluation_summary(avg_metrics)
        
        return avg_metrics
    
    def _calculate_average_metrics(self, results: list) -> dict:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        metrics_cols = ['ap50', 'ap75', 'iou_score', 'dice_score', 'hd95', 
                       'gt_instances', 'pred_instances', 'processing_time']
        
        avg_metrics = {}
        df = pd.DataFrame(results)
        
        for col in metrics_cols:
            if col in df.columns:
                if col == 'hd95':
                    finite_values = df[col][np.isfinite(df[col])]
                    avg_metrics[f'avg_{col}'] = finite_values.mean() if len(finite_values) > 0 else float('inf')
                    avg_metrics[f'median_{col}'] = finite_values.median() if len(finite_values) > 0 else float('inf')
                    avg_metrics[f'finite_count_{col}'] = len(finite_values)
                    avg_metrics[f'infinite_count_{col}'] = len(df) - len(finite_values)
                else:
                    avg_metrics[f'avg_{col}'] = df[col].mean()
                    avg_metrics[f'std_{col}'] = df[col].std()
                    avg_metrics[f'median_{col}'] = df[col].median()
        
        # æŒ‰ç»†èƒç±»å‹ç»Ÿè®¡
        if 'cell_type' in df.columns:
            cell_type_stats = {}
            for cell_type in df['cell_type'].unique():
                if cell_type != 'unknown':
                    cell_data = df[df['cell_type'] == cell_type]
                    cell_stats = {}
                    for metric in ['ap50', 'ap75', 'iou_score', 'dice_score']:
                        if metric in cell_data.columns:
                            cell_stats[metric] = {
                                'mean': cell_data[metric].mean(),
                                'std': cell_data[metric].std(),
                                'count': len(cell_data)
                            }
                    cell_type_stats[cell_type] = cell_stats
            
            avg_metrics['by_cell_type'] = cell_type_stats
        
        return avg_metrics
    
    def _print_evaluation_summary(self, metrics: dict):
        """æ‰“å°è¯„æµ‹æ‘˜è¦"""
        print(f"\n{'='*60}")
        print("LoRAæ¨¡å‹è¯„æµ‹ç»“æœæ‘˜è¦")
        print(f"{'='*60}")
        
        print(f"æ¨¡å‹è·¯å¾„: {metrics.get('model_path', 'N/A')}")
        print(f"è¯„æµ‹æ ·æœ¬æ•°: {metrics.get('total_samples', 0)}")
        print(f"æ€»å¤„ç†æ—¶é—´: {metrics.get('total_processing_time', 0):.2f}s")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {metrics.get('average_processing_time_per_sample', 0):.4f}s/æ ·æœ¬")
        
        print(f"\nä¸»è¦æ€§èƒ½æŒ‡æ ‡:")
        key_metrics = ['avg_ap50', 'avg_ap75', 'avg_iou_score', 'avg_dice_score']
        for metric in key_metrics:
            if metric in metrics:
                value = metrics[metric]
                std_key = metric.replace('avg_', 'std_')
                std_value = metrics.get(std_key, 0)
                print(f"  {metric.replace('avg_', '').upper()}: {value:.4f} Â± {std_value:.4f}")
        
        # HD95ç‰¹æ®Šå¤„ç†
        if 'avg_hd95' in metrics:
            hd95_val = metrics['avg_hd95']
            finite_count = metrics.get('finite_count_hd95', 0)
            total_count = metrics.get('total_samples', 0)
            if hd95_val == float('inf'):
                print(f"  HD95: âˆ (æ‰€æœ‰å€¼éƒ½æ˜¯æ— ç©·)")
            else:
                print(f"  HD95: {hd95_val:.4f} (åŸºäº {finite_count}/{total_count} ä¸ªæœ‰æ•ˆå€¼)")
        
        # æŒ‰ç»†èƒç±»å‹æ˜¾ç¤º
        if 'by_cell_type' in metrics:
            print(f"\næŒ‰ç»†èƒç±»å‹ç»Ÿè®¡:")
            for cell_type, stats in metrics['by_cell_type'].items():
                print(f"  {cell_type}:")
                for metric, values in stats.items():
                    print(f"    {metric.upper()}: {values['mean']:.4f} Â± {values['std']:.4f} (n={values['count']})")
        
        print(f"{'='*60}")
    
    def batch_inference_test(self, test_samples: list, batch_sizes: list = [1, 2, 4, 8]) -> dict:
        """æ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•")
        
        if not self.model:
            if not self.load_model():
                return {}
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ç”¨äºæ€§èƒ½æµ‹è¯•
        test_samples = test_samples[:min(50, len(test_samples))]
        
        batch_results = {}
        
        for batch_size in batch_sizes:
            print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            if batch_size > len(test_samples):
                print(f"æ‰¹æ¬¡å¤§å° {batch_size} è¶…è¿‡å¯ç”¨æ ·æœ¬æ•° {len(test_samples)}ï¼Œè·³è¿‡")
                continue
            
            try:
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                config = DataConfig()
                config.batch_size = batch_size
                
                test_dataset = SAMDataset(
                    data_dir=None,
                    config=config,
                    split='test',
                    samples=test_samples[:batch_size * 3]  # é™åˆ¶æ ·æœ¬æ•°
                )
                
                test_loader = torch.utils.data.DataLoader(
                    test_dataset,
                    batch_size=batch_size,
                    shuffle=False,
                    num_workers=1,
                    collate_fn=collate_fn
                )
                
                # æ€§èƒ½æµ‹è¯•
                times = []
                
                with torch.no_grad():
                    for batch_idx, batch in enumerate(test_loader):
                        if batch_idx >= 3:  # åªæµ‹è¯•å‰å‡ ä¸ªæ‰¹æ¬¡
                            break
                        
                        from lora.training_utils import prepare_sam_inputs
                        inputs, targets = prepare_sam_inputs(batch)
                        
                        # ç§»åŠ¨åˆ°è®¾å¤‡
                        for key, value in inputs.items():
                            if isinstance(value, torch.Tensor):
                                inputs[key] = value.to(self.device)
                            elif isinstance(value, list):
                                inputs[key] = [v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value]
                        
                        # è®¡æ—¶
                        if torch.cuda.is_available():
                            torch.cuda.synchronize()
                        
                        start_time = time.time()
                        predictions = self.model(inputs)
                        
                        if torch.cuda.is_available():
                            torch.cuda.synchronize()
                        
                        end_time = time.time()
                        times.append(end_time - start_time)
                
                # ç»Ÿè®¡ç»“æœ
                if times:
                    avg_time = np.mean(times)
                    time_per_image = avg_time / batch_size
                    throughput = batch_size / avg_time
                    
                    batch_results[batch_size] = {
                        'avg_batch_time': avg_time,
                        'time_per_image': time_per_image,
                        'throughput': throughput,
                        'num_batches_tested': len(times)
                    }
                    
                    print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_time:.4f}s")
                    print(f"  æ¯å›¾åƒæ—¶é—´: {time_per_image:.4f}s")
                    print(f"  ååé‡: {throughput:.2f} å›¾åƒ/ç§’")
                
            except Exception as e:
                print(f"æ‰¹æ¬¡å¤§å° {batch_size} æµ‹è¯•å¤±è´¥: {e}")
                batch_results[batch_size] = {'error': str(e)}
        
        return batch_results


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - å¢å¼ºç‰ˆ"""
    parser = argparse.ArgumentParser(
        description="SAM LoRAå¾®è°ƒè®­ç»ƒç³»ç»Ÿ (å¢å¼ºç‰ˆ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¿«é€Ÿè®­ç»ƒ
  python lora_main.py train --preset quick --data-dir /path/to/data
  
  # ä½¿ç”¨æ•°æ®åˆ’åˆ†æ–‡ä»¶è¯„æµ‹
  python lora_main.py evaluate --lora-model /path/to/lora --split-file /path/to/split.json --batch-test
  
  # è¯¦ç»†è¯„æµ‹ä¸æ‰¹é‡æ¨ç†æµ‹è¯•
  python lora_main.py evaluate --lora-model /path/to/lora --split-file /path/to/split.json --max-samples 1000 --batch-test --save-detailed
  
  # è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹
  python lora_main.py train-and-eval --data-dir /path/to/data --eval-split-file /path/to/split.json
        """
    )
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # è®­ç»ƒå‘½ä»¤
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒLoRAæ¨¡å‹')
    add_train_arguments(train_parser)
    
    # æ¢å¤è®­ç»ƒå‘½ä»¤
    resume_parser = subparsers.add_parser('resume', help='æ¢å¤è®­ç»ƒ')
    resume_parser.add_argument('--checkpoint', required=True, help='æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    resume_parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # ğŸ”§ å¢å¼ºç‰ˆè¯„æµ‹å‘½ä»¤
    eval_parser = subparsers.add_parser('evaluate', help='è¯„æµ‹LoRAæ¨¡å‹ (å¢å¼ºç‰ˆ)')
    add_enhanced_eval_arguments(eval_parser)
    
    # è®­ç»ƒ+è¯„æµ‹å‘½ä»¤
    train_eval_parser = subparsers.add_parser('train-and-eval', help='è®­ç»ƒåè‡ªåŠ¨è¯„æµ‹')
    add_train_arguments(train_eval_parser)
    add_enhanced_eval_arguments(train_eval_parser)
    
    # æ•°æ®å‡†å¤‡å‘½ä»¤
    data_parser = subparsers.add_parser('prepare-data', help='å‡†å¤‡è®­ç»ƒæ•°æ®/é¢„è§ˆæ•°æ®åˆ’åˆ†')
    data_parser.add_argument('--data-dir', required=True, help='æ•°æ®ç›®å½•')
    data_parser.add_argument('--train-ratio', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹')
    data_parser.add_argument('--val-ratio', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹')
    data_parser.add_argument('--test-split', type=float, default=0.1, help='æµ‹è¯•é›†æ¯”ä¾‹')
    data_parser.add_argument('--cell-types', nargs='+', help='è¦å¤„ç†çš„ç»†èƒç±»å‹')
    data_parser.add_argument('--split-method', choices=['random', 'by_dataset'], default='random', help='åˆ’åˆ†æ–¹æ³•')
    data_parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    data_parser.add_argument('--preview-only', action='store_true', help='åªé¢„è§ˆï¼Œä¸åˆ›å»ºå®é™…åˆ’åˆ†')
    
    # æ•°æ®åˆ’åˆ†ç®¡ç†å‘½ä»¤
    splits_parser = subparsers.add_parser('manage-splits', help='ç®¡ç†æ•°æ®åˆ’åˆ†ç¼“å­˜')
    splits_parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ’åˆ†')
    splits_parser.add_argument('--clean', action='store_true', help='æ¸…ç†æ—§çš„åˆ’åˆ†æ–‡ä»¶')
    splits_parser.add_argument('--keep', type=int, default=10, help='ä¿ç•™çš„æœ€æ–°åˆ’åˆ†æ•°é‡')
    splits_parser.add_argument('--split-dir', default='./data/lora_split', help='åˆ’åˆ†æ–‡ä»¶å­˜å‚¨ç›®å½•')
    
    # æ¨¡å‹ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser('info', help='æ˜¾ç¤ºæ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯')
    info_parser.add_argument('--model', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                            default='vit_b_lm', help='æ¨¡å‹ç±»å‹')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    return parser.parse_args()


def add_enhanced_eval_arguments(parser):
    """æ·»åŠ å¢å¼ºç‰ˆè¯„æµ‹ç›¸å…³å‚æ•°"""
    # ğŸ”§ åŸºæœ¬è¯„æµ‹å‚æ•°
    parser.add_argument('--lora-model', required=True, help='LoRAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--split-file', help='æ•°æ®åˆ’åˆ†æ–‡ä»¶è·¯å¾„ (æ¨è)')
    parser.add_argument('--eval-data', help='è¯„æµ‹æ•°æ®ç›®å½• (å¤‡é€‰)')
    parser.add_argument('--eval-output', help='è¯„æµ‹ç»“æœè¾“å‡ºç›®å½•')
    
    # ğŸ”§ æ–°å¢ï¼šè¯¦ç»†è¯„æµ‹å‚æ•°
    parser.add_argument('--max-samples', type=int, help='æœ€å¤§è¯„æµ‹æ ·æœ¬æ•°')
    parser.add_argument('--eval-batch-size', type=int, default=1, help='è¯„æµ‹æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--save-detailed', action='store_true', default=True, help='ä¿å­˜è¯¦ç»†ç»“æœ')
    parser.add_argument('--cell-types', nargs='+', help='ç»†èƒç±»å‹è¿‡æ»¤')
    
    # ğŸ”§ æ–°å¢ï¼šæ‰¹é‡æ¨ç†æµ‹è¯•å‚æ•°
    parser.add_argument('--batch-test', action='store_true', help='æ‰§è¡Œæ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•')
    parser.add_argument('--batch-sizes', nargs='+', type=int, default=[1, 2, 4, 8], 
                       help='æ‰¹é‡æ¨ç†æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°')
    
    # ğŸ”§ æ–°å¢ï¼šæ€§èƒ½åˆ†æå‚æ•°
    parser.add_argument('--benchmark', action='store_true', help='æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--compare-baseline', action='store_true', help='ä¸åŸºç¡€æ¨¡å‹å¯¹æ¯”')


def add_train_arguments(parser):
    """æ·»åŠ è®­ç»ƒç›¸å…³å‚æ•°"""
    # æ•°æ®é…ç½®
    parser.add_argument('--data-dir', required=True, help='è®­ç»ƒæ•°æ®ç›®å½•')
    parser.add_argument('--val-data-dir', help='éªŒè¯æ•°æ®ç›®å½•')
    parser.add_argument('--output-dir', default='./data/lora_experiments', help='è¾“å‡ºç›®å½•')
    
    # æ•°æ®åˆ’åˆ†å‚æ•°
    parser.add_argument('--test-split', type=float, default=0.1, help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰')
    parser.add_argument('--val-split', type=float, help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œä»train_ratioè®¡ç®—ï¼‰')
    parser.add_argument('--split-method', choices=['random', 'by_dataset'], default='random', help='æ•°æ®åˆ’åˆ†æ–¹æ³•')
    parser.add_argument('--split-seed', type=int, default=42, help='æ•°æ®åˆ’åˆ†éšæœºç§å­')
    parser.add_argument('--no-cached-split', action='store_true', help='ä¸ä½¿ç”¨ç¼“å­˜çš„æ•°æ®åˆ’åˆ†')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model', choices=['vit_t_lm', 'vit_b_lm', 'vit_l_lm'], 
                       default='vit_b_lm', help='åŸºç¡€æ¨¡å‹')
    parser.add_argument('--preset', choices=list(LORA_PRESET_CONFIGS.keys()), 
                       help='é¢„è®¾é…ç½®')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # LoRAé…ç½®
    parser.add_argument('--rank', type=int, default=8, help='LoRA rank')
    parser.add_argument('--alpha', type=float, default=16.0, help='LoRA alpha')
    parser.add_argument('--dropout', type=float, default=0.1, help='LoRA dropout')
    parser.add_argument('--lora-target', choices=['image_encoder', 'mask_decoder', 'both'],
                       default='image_encoder', help='LoRAåº”ç”¨ç›®æ ‡')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--save-steps', type=int, default=500, help='ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--eval-steps', type=int, default=100, help='éªŒè¯çš„æ­¥æ•°é—´éš”')
    parser.add_argument('--logging-steps', type=int, default=50, help='æ—¥å¿—è®°å½•çš„æ­¥æ•°é—´éš”')
    
    # å®éªŒé…ç½®
    parser.add_argument('--experiment-name', default='sam_lora_finetune', help='å®éªŒåç§°')
    parser.add_argument('--use-wandb', action='store_true', help='ä½¿ç”¨Weights & Biases')
    parser.add_argument('--wandb-project', default='sam_lora_training', help='W&Bé¡¹ç›®å')
    parser.add_argument('--cell-types', nargs='+', help='è¦è®­ç»ƒçš„ç»†èƒç±»å‹ï¼Œå¦‚: --cell-types 293T MSC')


# ğŸ”§ å¢å¼ºç‰ˆè¯„æµ‹å‡½æ•°
def evaluate_lora_model_enhanced(args, lora_model_path: str = None) -> bool:
    """å¢å¼ºç‰ˆLoRAæ¨¡å‹è¯„æµ‹"""
    print("="*60)
    print("å¼€å§‹å¢å¼ºç‰ˆSAM LoRAæ¨¡å‹è¯„æµ‹")
    print("="*60)
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if lora_model_path is None:
        lora_model_path = args.lora_model
    
    if not lora_model_path:
        print("é”™è¯¯: æœªæŒ‡å®šLoRAæ¨¡å‹è·¯å¾„")
        return False
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆè¯„æµ‹å™¨
        evaluator = EnhancedLoRAEvaluator(lora_model_path, device="auto")
        
        print(f"LoRAæ¨¡å‹è·¯å¾„: {lora_model_path}")
        
        # ğŸ”§ ä¼˜å…ˆä½¿ç”¨æ•°æ®åˆ’åˆ†æ–‡ä»¶
        if hasattr(args, 'split_file') and args.split_file:
            print(f"ä½¿ç”¨æ•°æ®åˆ’åˆ†æ–‡ä»¶: {args.split_file}")
            
            # æ‰§è¡Œè¯„æµ‹
            eval_results = evaluator.evaluate_with_split_file(args.split_file, args)
            
            if not eval_results:
                print("âŒ è¯„æµ‹å¤±è´¥")
                return False
            
            # ğŸ”§ æ‰¹é‡æ¨ç†æµ‹è¯•
            if getattr(args, 'batch_test', False):
                print("\n" + "="*60)
                print("æ‰§è¡Œæ‰¹é‡æ¨ç†æ€§èƒ½æµ‹è¯•")
                print("="*60)
                
                # é‡æ–°åŠ è½½æµ‹è¯•æ ·æœ¬ç”¨äºæ‰¹é‡æµ‹è¯•
                try:
                    with open(args.split_file, 'r', encoding='utf-8') as f:
                        split_data = json.load(f)
                    split_result = DataSplit.from_dict(split_data)
                    test_samples = split_result.test_samples
                    
                    # éªŒè¯æ ·æœ¬æœ‰æ•ˆæ€§
                    valid_samples = []
                    for sample in test_samples:
                        if Path(sample['image_path']).exists() and Path(sample['mask_path']).exists():
                            valid_samples.append(sample)
                    
                    if valid_samples:
                        batch_sizes = getattr(args, 'batch_sizes', [1, 2, 4, 8])
                        batch_results = evaluator.batch_inference_test(valid_samples, batch_sizes)
                        
                        # ä¿å­˜æ‰¹é‡æµ‹è¯•ç»“æœ
                        if batch_results:
                            output_dir = Path(getattr(args, 'eval_output', evaluator.lora_model_path.parent / "evaluation_results"))
                            batch_file = output_dir / "batch_inference_results.json"
                            with open(batch_file, 'w') as f:
                                json.dump(batch_results, f, indent=2)
                            print(f"æ‰¹é‡æ¨ç†æµ‹è¯•ç»“æœå·²ä¿å­˜: {batch_file}")
                            
                            # æ‰“å°æ‰¹é‡æµ‹è¯•æ‘˜è¦
                            print(f"\nğŸ“Š æ‰¹é‡æ¨ç†æµ‹è¯•æ‘˜è¦:")
                            print(f"{'æ‰¹æ¬¡å¤§å°':<8} {'å¹³å‡æ—¶é—´(s)':<12} {'ååé‡(å›¾åƒ/s)':<15} {'çŠ¶æ€':<10}")
                            print("-" * 50)
                            for batch_size, result in batch_results.items():
                                if 'error' not in result:
                                    throughput = result['throughput']
                                    avg_time = result['avg_batch_time']
                                    status = "âœ… æˆåŠŸ"
                                    print(f"{batch_size:<8} {avg_time:<12.4f} {throughput:<15.2f} {status:<10}")
                                else:
                                    print(f"{batch_size:<8} {'N/A':<12} {'N/A':<15} {'âŒ å¤±è´¥':<10}")
                
                except Exception as e:
                    print(f"æ‰¹é‡æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        
        # ğŸ”§ å¤‡é€‰ï¼šä½¿ç”¨æ•°æ®ç›®å½•è¯„æµ‹
        elif hasattr(args, 'eval_data') and args.eval_data:
            print(f"ä½¿ç”¨æ•°æ®ç›®å½•: {args.eval_data}")
            
            from lora.data_loaders import create_data_loaders
            from config.lora_config import DataConfig
            
            # åˆ›å»ºæ•°æ®é…ç½®
            config = DataConfig() 
            config.test_data_dir = args.eval_data
            config.batch_size = getattr(args, 'eval_batch_size', 1)
            config._cell_types_filter = getattr(args, 'cell_types', None)
            
            try:
                data_loaders = create_data_loaders(config, dataset_type="sam")
                
                if 'test' not in data_loaders:
                    print("âŒ æ— æ³•åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨")
                    return False
                
                test_loader = data_loaders['test']
                print(f"æµ‹è¯•æ•°æ®: {len(test_loader)} æ‰¹æ¬¡")
                
                # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å®ç°åŸºäºæ•°æ®ç›®å½•çš„è¯„æµ‹...
                print("åŸºäºæ•°æ®ç›®å½•çš„è¯„æµ‹åŠŸèƒ½å¼€å‘ä¸­...")
                
            except Exception as e:
                print(f"ä½¿ç”¨æ•°æ®ç›®å½•è¯„æµ‹å¤±è´¥: {e}")
                return False
        
        else:
            print("é”™è¯¯: è¯·æŒ‡å®š --split-file æˆ– --eval-data")
            return False
        
        print("\nâœ… å¢—å¼ºç‰ˆLoRAæ¨¡å‹è¯„æµ‹å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if hasattr(args, 'verbose') and args.verbose:
            import traceback
            traceback.print_exc()
        return False


# ä¿æŒåŸæœ‰å‡½æ•°ï¼Œä½†æ”¹ååŒºåˆ†
def create_config_from_args(args) -> LoRATrainingSettings:
    """ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®"""
    
    # ä½¿ç”¨é¢„è®¾é…ç½®
    if hasattr(args, 'preset') and args.preset:
        config = LORA_PRESET_CONFIGS[args.preset]
        print(f"ä½¿ç”¨é¢„è®¾é…ç½®: {args.preset}")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    elif hasattr(args, 'config') and args.config:
        config = LoRATrainingSettings.from_json(args.config)
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
    
    # ä¸ºç‰¹å®šæ¨¡å‹åˆ›å»ºé…ç½®
    elif hasattr(args, 'model'):
        config = get_config_for_model(args.model)
        print(f"ä¸ºæ¨¡å‹ {args.model} åˆ›å»ºé…ç½®")
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    else:
        config = LoRATrainingSettings()
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
    
    # æ›´æ–°é…ç½® - ä¿æŒåŸæœ‰é€»è¾‘
    if hasattr(args, 'data_dir'):
        config.data.train_data_dir = args.data_dir
    
    if hasattr(args, 'val_data_dir') and args.val_data_dir:
        config.data.val_data_dir = args.val_data_dir
    
    if hasattr(args, 'output_dir'):
        config.experiment.output_dir = args.output_dir
    
    if hasattr(args, 'model'):
        config.model.base_model_name = args.model
    
    # æ•°æ®åˆ’åˆ†é…ç½®
    if hasattr(args, 'test_split'):
        config.data.test_split_ratio = args.test_split
        
        if hasattr(args, 'val_split') and args.val_split is not None:
            config.data.val_split_ratio = args.val_split
            config.data.train_split_ratio = 1.0 - args.test_split - args.val_split
        else:
            if args.test_split >= 0.9:
                config.data.val_split_ratio = 0.0
                config.data.train_split_ratio = 1.0 - args.test_split
            else:
                remaining_ratio = 1.0 - args.test_split
                config.data.train_split_ratio = remaining_ratio * 0.9
                config.data.val_split_ratio = remaining_ratio * 0.1
        
        print(f"æ•°æ®åˆ’åˆ†æ¯”ä¾‹: train={config.data.train_split_ratio:.3f}, "
              f"val={config.data.val_split_ratio:.3f}, test={config.data.test_split_ratio:.3f}")
    
    if hasattr(args, 'split_method'):
        config.data.split_method = args.split_method
        
    if hasattr(args, 'split_seed'):
        config.data.split_seed = args.split_seed
        
    if hasattr(args, 'no_cached_split'):
        config.data.use_cached_split = not args.no_cached_split
    
    # LoRAå‚æ•°
    if hasattr(args, 'rank'):
        config.lora.rank = args.rank
    if hasattr(args, 'alpha'):
        config.lora.alpha = args.alpha
    if hasattr(args, 'dropout'):
        config.lora.dropout = args.dropout
    
    # LoRAç›®æ ‡è®¾ç½®
    if hasattr(args, 'lora_target'):
        if args.lora_target == 'image_encoder':
            config.model.apply_lora_to = ['image_encoder']
        elif args.lora_target == 'mask_decoder':
            config.model.apply_lora_to = ['mask_decoder']
        elif args.lora_target == 'both':
            config.model.apply_lora_to = ['image_encoder', 'mask_decoder']
    
    # è®­ç»ƒå‚æ•°
    if hasattr(args, 'epochs'):
        config.training.num_epochs = args.epochs
    if hasattr(args, 'batch_size'):
        config.training.batch_size = args.batch_size
    if hasattr(args, 'learning_rate'):
        config.training.learning_rate = args.learning_rate
    if hasattr(args, 'weight_decay'):
        config.training.weight_decay = args.weight_decay
    if hasattr(args, 'save_steps'):
        config.training.save_steps = args.save_steps
    if hasattr(args, 'eval_steps'):
        config.training.eval_steps = args.eval_steps  
    if hasattr(args, 'logging_steps'):
        config.training.logging_steps = args.logging_steps
    
    # å®éªŒé…ç½®
    if hasattr(args, 'experiment_name'):
        config.experiment.experiment_name = args.experiment_name
    if hasattr(args, 'use_wandb'):
        config.experiment.use_wandb = args.use_wandb
    if hasattr(args, 'wandb_project'):
        config.experiment.wandb_project = args.wandb_project

    # æ·»åŠ ç»†èƒç±»å‹è¿‡æ»¤
    if hasattr(args, 'cell_types') and args.cell_types:
        config.data._cell_types_filter = args.cell_types
    else:
        config.data._cell_types_filter = None
    
    # è°ƒè¯•æ¨¡å¼
    if hasattr(args, 'debug') and args.debug:
        config.experiment.debug_mode = True
        config.training.num_epochs = 2
        config.training.batch_size = 2
        config.training.save_steps = 10
        config.training.eval_steps = 5
        config.training.logging_steps = 1
        print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    return config


def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
    device_info = get_device_info()
    print(f"CUDAå¯ç”¨: {device_info['cuda_available']}")
    
    if device_info['cuda_available']:
        print(f"GPUæ•°é‡: {device_info['cuda_device_count']}")
        for gpu_name, gpu_info in device_info['gpu_memory'].items():
            print(f"  {gpu_name}: {gpu_info['name']}, "
                  f"å†…å­˜: {gpu_info['total_memory'] / 1e9:.1f} GB")
    
    # æ£€æŸ¥å†…å­˜
    cpu_memory = device_info['cpu_memory']
    print(f"CPUå†…å­˜: {cpu_memory['total'] / 1e9:.1f} GB "
          f"(å¯ç”¨: {cpu_memory['available'] / 1e9:.1f} GB)")
    
    # æ£€æŸ¥micro_sam
    try:
        import micro_sam
        print(f"micro_samå·²å®‰è£…")
    except ImportError:
        print("è­¦å‘Š: micro_samæœªå®‰è£…ï¼Œå¯èƒ½å½±å“æ¨¡å‹åŠ è½½")
    
    print("ç³»ç»Ÿæ£€æŸ¥å®Œæˆ\n")


def train_lora_model(args) -> str:
    """è®­ç»ƒLoRAæ¨¡å‹ - ä¿æŒåŸæœ‰é€»è¾‘"""
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†åˆ«è®­ç»ƒå¤šä¸ªç»†èƒç±»å‹
    if hasattr(args, 'cell_types') and args.cell_types and len(args.cell_types) > 1:
        return train_multiple_cell_types(args)
    
    print("="*60)
    print("å¼€å§‹SAM LoRAå¾®è°ƒè®­ç»ƒ")
    print("="*60)
    
    check_system_requirements()
    config = create_config_from_args(args)
    
    if not config.validate():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return None
    
    # å¦‚æœæ˜¯å•ä¸ªç»†èƒç±»å‹ï¼Œåˆ›å»ºæ›´è¯¦ç»†çš„å®éªŒåç§°
    if hasattr(args, 'cell_types') and args.cell_types and len(args.cell_types) == 1:
        cell_type = args.cell_types[0]
        test_ratio = int(args.test_split * 100) if hasattr(args, 'test_split') else 10
        val_ratio = int(args.val_split * 100) if hasattr(args, 'val_split') and args.val_split else 10
        train_ratio = 100 - test_ratio - val_ratio
        
        split_suffix = f"train{train_ratio}_val{val_ratio}_test{test_ratio}"
        config.experiment.experiment_name = f"sam_lora_{cell_type.lower()}_{split_suffix}"
        config.experiment.output_dir = f"{config.experiment.output_dir}_{cell_type.lower()}_{split_suffix}"
    
    # é¢„è§ˆæ•°æ®åˆ’åˆ†
    if hasattr(args, 'test_split') and args.test_split > 0:
        print(f"\né¢„è§ˆæ•°æ®åˆ’åˆ†...")
        try:
            preview_stats = preview_data_split(
                data_dir=config.data.train_data_dir,
                train_ratio=config.data.train_split_ratio,
                val_ratio=config.data.val_split_ratio,
                test_ratio=config.data.test_split_ratio,
                cell_types=config.data._cell_types_filter,
                split_method=config.data.split_method,
                seed=config.data.split_seed,
                split_storage_dir=config.data.split_storage_dir
            )
            
            if preview_stats:
                print(f"  æ€»æ ·æœ¬æ•°: {preview_stats['total_samples']}")
                print(f"  è®­ç»ƒé›†: {preview_stats['train_count']} æ ·æœ¬")
                print(f"  éªŒè¯é›†: {preview_stats['val_count']} æ ·æœ¬")
                print(f"  æµ‹è¯•é›†: {preview_stats['test_count']} æ ·æœ¬")
                
                if 'cell_type_distribution' in preview_stats:
                    print(f"  ç»†èƒç±»å‹åˆ†å¸ƒ: {preview_stats['cell_type_distribution']}")
                    
        except Exception as e:
            print(f"é¢„è§ˆæ•°æ®åˆ’åˆ†å¤±è´¥: {e}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  åŸºç¡€æ¨¡å‹: {config.model.base_model_name}")
    print(f"  LoRAé…ç½®:")
    print(f"    rank: {config.lora.rank}")
    print(f"    alpha: {config.lora.alpha}")
    print(f"    dropout: {config.lora.dropout}")
    print(f"    åº”ç”¨åˆ°: {config.model.apply_lora_to}")
    print(f"  è®­ç»ƒé…ç½®:")
    print(f"    å­¦ä¹ ç‡: {config.training.learning_rate}")
    print(f"    æ‰¹å¤§å°: {config.training.batch_size}")
    print(f"    è®­ç»ƒè½®æ•°: {config.training.num_epochs}")
    print(f"  è¾“å‡ºç›®å½•: {config.experiment.output_dir}")
    print(f"  æ•°æ®ç›®å½•: {config.data.train_data_dir}")
    
    trainer = LoRATrainer(config)
    success = trainer.train()
    
    if success:
        model_path = trainer.output_dir / "final_model"
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        return str(model_path)
    else:
        print("\nè®­ç»ƒå¤±è´¥!")
        return None


def train_multiple_cell_types(args) -> str:
    """ä¸ºå¤šä¸ªç»†èƒç±»å‹åˆ†åˆ«è®­ç»ƒæ¨¡å‹ - ä¿æŒåŸæœ‰é€»è¾‘"""
    print("="*60)
    print("å¼€å§‹å¤šç»†èƒç±»å‹åˆ†åˆ«è®­ç»ƒ")
    print("="*60)
    print(f"å°†è®­ç»ƒçš„ç»†èƒç±»å‹: {', '.join(args.cell_types)}")
    
    results = {}
    
    for cell_type in args.cell_types:
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒ {cell_type} æ¨¡å‹...")
        
        # åˆ›å»ºå•ä¸ªç»†èƒç±»å‹çš„å‚æ•°å‰¯æœ¬
        single_args = type(args)()
        for attr in dir(args):
            if not attr.startswith('_'):
                setattr(single_args, attr, getattr(args, attr))
        
        # è®¾ç½®ä¸ºå•ä¸ªç»†èƒç±»å‹
        single_args.cell_types = [cell_type]
        
        # è®­ç»ƒ
        model_path = train_lora_model(single_args)
        results[cell_type] = model_path
        
        if model_path:
            print(f"âœ… {cell_type} è®­ç»ƒå®Œæˆ: {model_path}")
        else:
            print(f"âŒ {cell_type} è®­ç»ƒå¤±è´¥")
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # ç”Ÿæˆæ‘˜è¦
    print(f"\n{'='*60}")
    print("å¤šç»†èƒç±»å‹è®­ç»ƒå®Œæˆ")
    print(f"{'='*60}")
    
    successful = sum(1 for path in results.values() if path)
    print(f"æˆåŠŸè®­ç»ƒ: {successful}/{len(args.cell_types)}")
    
    for cell_type, path in results.items():
        if path:
            print(f"  âœ… {cell_type}: {path}")
        else:
            print(f"  âŒ {cell_type}: å¤±è´¥")
    
    return f"å¤šç»†èƒç±»å‹è®­ç»ƒå®Œæˆï¼ŒæˆåŠŸ: {successful}/{len(args.cell_types)}"


def resume_lora_training(args) -> str:
    """æ¢å¤LoRAè®­ç»ƒ - ä¿æŒåŸæœ‰é€»è¾‘"""
    print("="*60)
    print("æ¢å¤SAM LoRAè®­ç»ƒ")
    print("="*60)
    
    trainer = resume_training(args.checkpoint, args.config)
    success = trainer.train()
    
    if success:
        model_path = trainer.output_dir / "final_model"
        print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
        return str(model_path)
    else:
        print("\nè®­ç»ƒå¤±è´¥!")
        return None


def prepare_training_data(args):
    """å‡†å¤‡è®­ç»ƒæ•°æ®/é¢„è§ˆæ•°æ®åˆ’åˆ† - ä¿æŒåŸæœ‰é€»è¾‘"""
    print("="*60)
    print("å‡†å¤‡è®­ç»ƒæ•°æ® / æ•°æ®åˆ’åˆ†é¢„è§ˆ")
    print("="*60)
    
    data_dir = args.data_dir
    train_ratio = args.train_ratio
    val_ratio = args.val_ratio
    test_ratio = getattr(args, 'test_split', 0.1)
    
    # éªŒè¯æ¯”ä¾‹æ€»å’Œ
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        print(f"è­¦å‘Š: æ¯”ä¾‹æ€»å’Œä¸ä¸º1.0 ({total_ratio})ï¼Œæ­£åœ¨è‡ªåŠ¨å½’ä¸€åŒ–...")
        train_ratio /= total_ratio
        val_ratio /= total_ratio
        test_ratio /= total_ratio
    
    cell_types = getattr(args, 'cell_types', None)
    split_method = getattr(args, 'split_method', 'random')
    seed = getattr(args, 'seed', 42)
    preview_only = getattr(args, 'preview_only', False)
    
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"åˆ†å‰²æ¯”ä¾‹ - è®­ç»ƒ: {train_ratio:.3f}, éªŒè¯: {val_ratio:.3f}, æµ‹è¯•: {test_ratio:.3f}")
    print(f"ç»†èƒç±»å‹è¿‡æ»¤: {cell_types}")
    print(f"åˆ†å‰²æ–¹æ³•: {split_method}")
    print(f"éšæœºç§å­: {seed}")
    print(f"é¢„è§ˆæ¨¡å¼: {preview_only}")
    
    try:
        if preview_only:
            stats = preview_data_split(
                data_dir=data_dir,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio,
                cell_types=cell_types,
                split_method=split_method,
                seed=seed
            )
            
            if stats:
                print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†é¢„è§ˆ:")
                print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
                print(f"  è®­ç»ƒé›†: {stats['train_count']} æ ·æœ¬ ({stats['train_count']/stats['total_samples']*100:.1f}%)")
                print(f"  éªŒè¯é›†: {stats['val_count']} æ ·æœ¬ ({stats['val_count']/stats['total_samples']*100:.1f}%)")
                print(f"  æµ‹è¯•é›†: {stats['test_count']} æ ·æœ¬ ({stats['test_count']/stats['total_samples']*100:.1f}%)")
                
                if 'cell_type_distribution' in stats:
                    print(f"  ç»†èƒç±»å‹åˆ†å¸ƒ:")
                    for cell_type, count in stats['cell_type_distribution'].items():
                        print(f"    {cell_type}: {count} æ ·æœ¬")
            else:
                print("é¢„è§ˆå¤±è´¥")
        else:
            from utils.data_splitter import create_data_split, print_split_summary
            
            split_result = create_data_split(
                data_dir=data_dir,
                train_ratio=train_ratio,
                val_ratio=val_ratio,
                test_ratio=test_ratio,
                cell_types=cell_types,
                split_method=split_method,
                seed=seed,
                use_cached=True
            )
            
            print_split_summary(split_result)
            
        print("æ•°æ®å‡†å¤‡å®Œæˆ!")
        
    except Exception as e:
        print(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def manage_data_splits(args):
    """ç®¡ç†æ•°æ®åˆ’åˆ†ç¼“å­˜ - ä¿æŒåŸæœ‰é€»è¾‘"""
    print("="*60)
    print("æ•°æ®åˆ’åˆ†ç¼“å­˜ç®¡ç†")
    print("="*60)
    
    split_dir = args.split_dir
    
    if args.list:
        print(f"ğŸ“‹ åˆ—å‡ºç¼“å­˜çš„æ•°æ®åˆ’åˆ† (ç›®å½•: {split_dir})")
        try:
            cached_splits = list_cached_splits(split_dir)
            
            if not cached_splits:
                print("  æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜çš„æ•°æ®åˆ’åˆ†")
            else:
                print(f"  æ‰¾åˆ° {len(cached_splits)} ä¸ªç¼“å­˜æ–‡ä»¶:")
                
                for i, split_info in enumerate(cached_splits, 1):
                    print(f"\n  {i}. æ–‡ä»¶: {Path(split_info['file_path']).name}")
                    print(f"     å¤§å°: {split_info['file_size_mb']:.2f} MB")
                    print(f"     æ•°æ®ç›®å½•: {split_info.get('data_dir', 'N/A')}")
                    print(f"     æ¯”ä¾‹: train={split_info.get('train_ratio', 0):.2f}, "
                          f"val={split_info.get('val_ratio', 0):.2f}, "
                          f"test={split_info.get('test_ratio', 0):.2f}")
                    print(f"     æ ·æœ¬æ•°: {split_info.get('total_samples', 0)}")
                    if split_info.get('cell_types'):
                        print(f"     ç»†èƒç±»å‹: {split_info['cell_types']}")
                    print(f"     åˆ›å»ºæ—¶é—´: {split_info.get('created_at', 'N/A')}")
                        
        except Exception as e:
            print(f"åˆ—å‡ºç¼“å­˜å¤±è´¥: {e}")
    
    if args.clean:
        print(f"\nğŸ§¹ æ¸…ç†æ—§çš„æ•°æ®åˆ’åˆ†æ–‡ä»¶ (ä¿ç•™æœ€æ–° {args.keep} ä¸ª)")
        try:
            clean_old_splits(split_dir, args.keep)
            print("æ¸…ç†å®Œæˆ!")
        except Exception as e:
            print(f"æ¸…ç†å¤±è´¥: {e}")


def show_model_info(args):
    """æ˜¾ç¤ºæ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯ - ä¿æŒåŸæœ‰é€»è¾‘"""
    print("="*60)
    print("æ¨¡å‹å’Œç³»ç»Ÿä¿¡æ¯")
    print("="*60)
    
    # ç³»ç»Ÿä¿¡æ¯
    check_system_requirements()
    
    # å°è¯•åŠ è½½æ¨¡å‹ä¿¡æ¯
    try:
        from core.sam_model_loader import create_sam_model_loader
        from utils.model_utils import print_model_summary
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ä¿¡æ¯: {args.model}")
        
        loader = create_sam_model_loader(args.model, "cpu")
        if loader.load_model():
            print("\næ¨¡å‹åŠ è½½æˆåŠŸ!")
            
            components = loader.get_trainable_components()
            print(f"\næ¨¡å‹ç»„ä»¶:")
            for name, component in components.items():
                param_count = sum(p.numel() for p in component.parameters())
                print(f"  {name}: {param_count:,} å‚æ•°")
            
            if loader.model is not None:
                print_model_summary(loader.model)
            
        else:
            print("æ¨¡å‹åŠ è½½å¤±è´¥")
    
    except Exception as e:
        print(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆ"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # è§£æå‚æ•°
    args = parse_arguments()
    
    if args.command is None:
        print("è¯·æŒ‡å®šå‘½ä»¤ã€‚ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        return
    
    try:
        if args.command == 'train':
            train_lora_model(args)
        
        elif args.command == 'resume':
            resume_lora_training(args)
        
        elif args.command == 'evaluate':
            # ğŸ”§ ä½¿ç”¨å¢å¼ºç‰ˆè¯„æµ‹
            evaluate_lora_model_enhanced(args)
        
        elif args.command == 'train-and-eval':
            # å…ˆè®­ç»ƒ
            lora_model_path = train_lora_model(args)
            
            # å†è¯„æµ‹
            if lora_model_path:
                print("\n" + "="*60)
                print("å¼€å§‹è‡ªåŠ¨è¯„æµ‹")
                print("="*60)
                
                # ğŸ”§ å¦‚æœæœ‰split_fileå‚æ•°ï¼Œç›´æ¥ä½¿ç”¨å¢å¼ºç‰ˆè¯„æµ‹
                if hasattr(args, 'split_file') and args.split_file:
                    # è®¾ç½®è¯„æµ‹å‚æ•°
                    args.lora_model = lora_model_path
                    evaluate_lora_model_enhanced(args)
                else:
                    # å›é€€åˆ°ä¼ ç»Ÿè¯„æµ‹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
                    print("è®­ç»ƒå®Œæˆï¼Œä½†æœªæŒ‡å®šæ•°æ®åˆ’åˆ†æ–‡ä»¶ï¼Œè·³è¿‡è‡ªåŠ¨è¯„æµ‹")
                    print("å»ºè®®ä½¿ç”¨ --split-file å‚æ•°æŒ‡å®šæµ‹è¯•æ•°æ®")
        
        elif args.command == 'prepare-data':
            prepare_training_data(args)
        
        elif args.command == 'manage-splits':
            manage_data_splits(args)
        
        elif args.command == 'info':
            show_model_info(args)
        
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {args.command}")
    
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        optimize_memory()


if __name__ == "__main__":
    main()