"""
LoRAè®­ç»ƒå·¥å…·å‡½æ•° - ä¿®å¤ç‰ˆ
ä¿®å¤SAMè®­ç»ƒä¸­çš„æ©ç å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Any, Optional


class SAMLoss(nn.Module):
    """SAMè®­ç»ƒçš„ç»¼åˆæŸå¤±å‡½æ•°"""
    
    def __init__(
        self,
        focal_loss_weight: float = 20.0,
        dice_loss_weight: float = 1.0,
        iou_loss_weight: float = 1.0,
        use_focal_loss: bool = True,
        use_dice_loss: bool = True,
        use_iou_loss: bool = True
    ):
        super().__init__()
        
        self.focal_loss_weight = focal_loss_weight
        self.dice_loss_weight = dice_loss_weight
        self.iou_loss_weight = iou_loss_weight
        self.use_focal_loss = use_focal_loss
        self.use_dice_loss = use_dice_loss
        self.use_iou_loss = use_iou_loss
        
        # Focal losså‚æ•°
        self.focal_alpha = 0.8
        self.focal_gamma = 2.0
    
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è®¡ç®—æ€»æŸå¤± - ä¿®å¤å¤šå®ä¾‹æ©ç å¤„ç†"""
        pred_masks = predictions['masks']  # [B, 1, H, W] - SAMè¾“å‡ºå•ä¸ªæ©ç 
        target_masks = targets['masks']    # [B, N, H, W] - å¤šä¸ªå®ä¾‹æ©ç 
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†å¤šå®ä¾‹ç›®æ ‡æ©ç åˆå¹¶ä¸ºå•ä¸ªäºŒè¿›åˆ¶æ©ç 
        if target_masks.shape[1] > 1:
            # å°†æ‰€æœ‰å®ä¾‹æ©ç åˆå¹¶ä¸ºä¸€ä¸ªäºŒè¿›åˆ¶æ©ç  [B, N, H, W] -> [B, 1, H, W]
            combined_target = (target_masks.sum(dim=1, keepdim=True) > 0).float()
        else:
            combined_target = target_masks.float()
        
        # ç¡®ä¿é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶åŒ¹é…
        if pred_masks.shape != combined_target.shape:
            # å¦‚æœç©ºé—´å°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´ç›®æ ‡æ©ç å°ºå¯¸
            combined_target = F.interpolate(
                combined_target, 
                size=pred_masks.shape[-2:], 
                mode='nearest'
            )
        
        loss_dict = {}
        total_loss = 0.0
        
        # Focal Loss
        if self.use_focal_loss:
            focal_loss = self._focal_loss(pred_masks, combined_target)
            loss_dict['focal_loss'] = focal_loss
            total_loss += self.focal_loss_weight * focal_loss
        
        # Dice Loss
        if self.use_dice_loss:
            dice_loss = self._dice_loss(pred_masks, combined_target)
            loss_dict['dice_loss'] = dice_loss
            total_loss += self.dice_loss_weight * dice_loss
        
        # IoU Loss
        if self.use_iou_loss:
            iou_loss = self._iou_loss(pred_masks, combined_target)
            loss_dict['iou_loss'] = iou_loss
            total_loss += self.iou_loss_weight * iou_loss
        
        # IoUé¢„æµ‹æŸå¤±ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'iou_predictions' in predictions and 'iou_targets' in targets:
            iou_pred_loss = F.mse_loss(
                predictions['iou_predictions'],
                targets['iou_targets']
            )
            loss_dict['iou_prediction_loss'] = iou_pred_loss
            total_loss += iou_pred_loss
        
        loss_dict['total_loss'] = total_loss
        return loss_dict
    
    def _focal_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """Focal Losså®ç°"""
        # ä½¿ç”¨sigmoidæ¿€æ´»
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        # è®¡ç®—focal loss
        ce_loss = F.binary_cross_entropy_with_logits(
            pred_masks, target_masks, reduction='none'
        )
        
        p_t = pred_sigmoid * target_masks + (1 - pred_sigmoid) * (1 - target_masks)
        alpha_t = self.focal_alpha * target_masks + (1 - self.focal_alpha) * (1 - target_masks)
        
        focal_weight = alpha_t * (1 - p_t) ** self.focal_gamma
        focal_loss = focal_weight * ce_loss
        
        return focal_loss.mean()
    
    def _dice_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """Dice Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        # å¹³æ»‘å› å­
        smooth = 1.0
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„dice loss
        intersection = (pred_sigmoid * target_masks).sum(dim=(-2, -1))
        total = pred_sigmoid.sum(dim=(-2, -1)) + target_masks.sum(dim=(-2, -1))
        
        dice_score = (2.0 * intersection + smooth) / (total + smooth)
        dice_loss = 1.0 - dice_score
        
        return dice_loss.mean()
    
    def _iou_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """IoU Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        # è®¡ç®—IoU
        intersection = (pred_sigmoid * target_masks).sum(dim=(-2, -1))
        union = pred_sigmoid.sum(dim=(-2, -1)) + target_masks.sum(dim=(-2, -1)) - intersection
        
        # é¿å…é™¤é›¶
        iou_score = intersection / (union + 1e-6)
        iou_loss = 1.0 - iou_score
        
        return iou_loss.mean()


def prepare_sam_inputs(batch: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor]]:
    """å‡†å¤‡SAMè®­ç»ƒçš„è¾“å…¥å’Œç›®æ ‡ - ä¿®å¤å¤šå®ä¾‹æ©ç å¤„ç†"""
    
    try:
        # print(f"batch: {batch.keys()}")
        # è¾“å…¥æ•°æ®
        inputs = {
            'images': batch['images'],  # [B, C, H, W]
            'point_coords': batch.get('point_coords', []),
            'point_labels': batch.get('point_labels', []),
            'boxes': batch.get('boxes', []),
            'mask_inputs': batch.get('mask_inputs', None),
            'multimask_output': batch.get('multimask_output', False)
        }
        
        # ç›®æ ‡æ•°æ®
        ground_truth_masks = batch['ground_truth_masks']  # [B, N, H, W]
        # print(f"ground_truth_masks: {ground_truth_masks}")
        # print(ground_truth_masks.shape)
        # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        device = inputs['images'].device

        targets_masks = ground_truth_masks.to(device)
        
        # if isinstance(ground_truth_masks, torch.Tensor):
        #     targets_masks = ground_truth_masks.to(device)
            
        #     # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†å¤šå®ä¾‹æ©ç 
        #     if targets_masks.shape[1] > 1:
        #         # æ–¹æ¡ˆ1ï¼šåˆå¹¶æ‰€æœ‰å®ä¾‹ä¸ºå•ä¸ªäºŒè¿›åˆ¶æ©ç 
        #         binary_masks = (targets_masks.sum(dim=1, keepdim=True) > 0).float()
        #         targets_masks = binary_masks
            
        # else:
        #     # å‘åå…¼å®¹å¤„ç†
        #     print(f"WARNING: ground_truth_masksè¿˜æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè½¬æ¢ä¸ºå¼ é‡")
            
        #     if isinstance(ground_truth_masks, list):
        #         processed_masks = []
                
        #         for i, masks in enumerate(ground_truth_masks):
        #             if isinstance(masks, torch.Tensor):
        #                 masks = masks.to(device)
        #                 if len(masks.shape) == 2:
        #                     masks = masks.unsqueeze(0)
                        
        #                 # å¦‚æœæœ‰å¤šä¸ªå®ä¾‹ï¼Œåˆå¹¶ä¸ºäºŒè¿›åˆ¶æ©ç 
        #                 if masks.shape[0] > 1:
        #                     binary_mask = (masks.sum(dim=0, keepdim=True) > 0).float()
        #                     processed_masks.append(binary_mask)
        #                 else:
        #                     processed_masks.append(masks)
        #             else:
        #                 h, w = inputs['images'].shape[-2:]
        #                 default_mask = torch.zeros(1, h, w, dtype=torch.float32, device=device)
        #                 processed_masks.append(default_mask)
                
        #         # ç»Ÿä¸€å½¢çŠ¶å¹¶å †å 
        #         target_size = processed_masks[0].shape[-2:]
        #         unified_masks = []
                
        #         for masks in processed_masks:
        #             if masks.shape[-2:] != target_size:
        #                 masks = torch.nn.functional.interpolate(
        #                     masks.unsqueeze(1).float(),
        #                     size=target_size,
        #                     mode='nearest'
        #                 ).squeeze(1)
        #             unified_masks.append(masks)
                
        #         targets_masks = torch.stack(unified_masks)
        #     else:
        #         # åˆ›å»ºé»˜è®¤å¼ é‡
        #         batch_size = inputs['images'].shape[0]
        #         h, w = inputs['images'].shape[-2:]
        #         targets_masks = torch.zeros(batch_size, 1, h, w, dtype=torch.float32, device=device)
        
        targets = {
            'masks': targets_masks  # [B, 1, H, W] - ç°åœ¨æ˜¯å•ä¸ªäºŒè¿›åˆ¶æ©ç 
        }
        
        # è®¡ç®—IoUç›®æ ‡
        try:
            if targets_masks.numel() > 0:
                iou_targets = calculate_mask_iou_targets(targets_masks)
                targets['iou_targets'] = iou_targets
        except Exception as e:
            print(f"WARNING: IoU targets calculation failed: {e}")
            targets['iou_targets'] = torch.ones(targets_masks.shape[0], targets_masks.shape[1], device=device)
        return inputs, targets
        
    except Exception as e:
        print(f"ERROR in prepare_sam_inputs: {e}")
        import traceback
        traceback.print_exc()
        
        # è¿”å›é»˜è®¤å€¼
        batch_size = batch['images'].shape[0]
        device = batch['images'].device
        h, w = batch['images'].shape[-2:]
        
        inputs = {
            'images': batch['images'],
            'point_coords': [],
            'point_labels': [],
            'boxes': [],
            'mask_inputs': None,
            'multimask_output': False
        }
        
        targets = {
            'masks': torch.zeros(batch_size, 1, h, w, dtype=torch.float32, device=device),
            'iou_targets': torch.ones(batch_size, 1, device=device)
        }
        
        return inputs, targets


def calculate_mask_iou_targets(masks: torch.Tensor) -> torch.Tensor:
    """è®¡ç®—æ©ç çš„IoUç›®æ ‡å€¼"""
    if masks.numel() == 0:
        return torch.tensor([])
    
    # masks shape: [B, N, H, W]
    batch_size = masks.shape[0]
    num_objects = masks.shape[1] if len(masks.shape) > 3 else 1
    
    if len(masks.shape) == 3:
        # å¦‚æœæ˜¯ [B, H, W]ï¼Œæ·»åŠ å¯¹è±¡ç»´åº¦
        masks = masks.unsqueeze(1)  # [B, 1, H, W]
        num_objects = 1
    
    # è®¡ç®—æ¯ä¸ªæ©ç çš„é¢ç§¯æ¯”ä¾‹ä½œä¸ºIoUç›®æ ‡
    mask_areas = masks.sum(dim=(-2, -1))  # [B, N]
    total_area = masks.shape[-2] * masks.shape[-1]
    
    # ä½¿ç”¨é¢ç§¯æ¯”ä¾‹ä½œä¸ºIoUçš„ç²—ç•¥ä¼°è®¡ï¼Œå¹¶é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    iou_targets = torch.clamp(mask_areas.float() / total_area, 0.1, 1.0)
    
    return iou_targets


class MaskPostProcessor:
    """æ©ç åå¤„ç†å™¨"""
    
    def __init__(self, threshold: float = 0.0, remove_small_objects: bool = True, 
                 min_object_size: int = 100):
        self.threshold = threshold
        self.remove_small_objects = remove_small_objects
        self.min_object_size = min_object_size
    
    def process(self, masks: torch.Tensor) -> torch.Tensor:
        """åå¤„ç†é¢„æµ‹çš„æ©ç """
        # åº”ç”¨é˜ˆå€¼
        if self.threshold > 0:
            masks = (masks > self.threshold).float()
        else:
            masks = torch.sigmoid(masks)
        
        # ç§»é™¤å°å¯¹è±¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.remove_small_objects:
            masks = self._remove_small_objects(masks)
        
        return masks
    
    def _remove_small_objects(self, masks: torch.Tensor) -> torch.Tensor:
        """ç§»é™¤å°å¯¹è±¡"""
        processed_masks = masks.clone()
        
        for b in range(masks.shape[0]):
            for m in range(masks.shape[1]):
                mask = masks[b, m]
                mask_area = mask.sum()
                
                if mask_area < self.min_object_size:
                    processed_masks[b, m] = 0
        
        return processed_masks


class SAMDataAugmentation:
    """SAMè®­ç»ƒçš„æ•°æ®å¢å¼º"""
    
    def __init__(self, 
                 flip_prob: float = 0.5,
                 rotation_prob: float = 0.3,
                 scale_prob: float = 0.3,
                 noise_prob: float = 0.2):
        self.flip_prob = flip_prob
        self.rotation_prob = rotation_prob
        self.scale_prob = scale_prob
        self.noise_prob = noise_prob
    
    def augment_batch(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå¢å¼º"""
        augmented_batch = batch.copy()
        
        # éšæœºæ°´å¹³ç¿»è½¬
        if torch.rand(1) < self.flip_prob:
            augmented_batch = self._horizontal_flip(augmented_batch)
        
        # éšæœºå‚ç›´ç¿»è½¬
        if torch.rand(1) < self.flip_prob:
            augmented_batch = self._vertical_flip(augmented_batch)
        
        # æ·»åŠ å™ªå£°
        if torch.rand(1) < self.noise_prob:
            augmented_batch = self._add_noise(augmented_batch)
        
        return augmented_batch
    
    def _horizontal_flip(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """æ°´å¹³ç¿»è½¬"""
        batch['images'] = torch.flip(batch['images'], dims=[-1])
        if 'ground_truth_masks' in batch:
            batch['ground_truth_masks'] = torch.flip(batch['ground_truth_masks'], dims=[-1])
        
        # è°ƒæ•´ç‚¹åæ ‡
        if 'point_coords' in batch:
            for i, coords in enumerate(batch['point_coords']):
                if len(coords) > 0:
                    coords[:, 0] = batch['images'].shape[-1] - 1 - coords[:, 0]
        
        # è°ƒæ•´è¾¹ç•Œæ¡†
        if 'boxes' in batch:
            for i, boxes in enumerate(batch['boxes']):
                if len(boxes) > 0:
                    width = batch['images'].shape[-1]
                    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
                    boxes[:, 0] = width - 1 - x2
                    boxes[:, 2] = width - 1 - x1
        
        return batch
    
    def _vertical_flip(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """å‚ç›´ç¿»è½¬"""
        batch['images'] = torch.flip(batch['images'], dims=[-2])
        if 'ground_truth_masks' in batch:
            batch['ground_truth_masks'] = torch.flip(batch['ground_truth_masks'], dims=[-2])
        
        # è°ƒæ•´ç‚¹åæ ‡
        if 'point_coords' in batch:
            for i, coords in enumerate(batch['point_coords']):
                if len(coords) > 0:
                    coords[:, 1] = batch['images'].shape[-2] - 1 - coords[:, 1]
        
        # è°ƒæ•´è¾¹ç•Œæ¡†
        if 'boxes' in batch:
            for i, boxes in enumerate(batch['boxes']):
                if len(boxes) > 0:
                    height = batch['images'].shape[-2]
                    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
                    boxes[:, 1] = height - 1 - y2
                    boxes[:, 3] = height - 1 - y1
        
        return batch
    
    def _add_noise(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ å™ªå£°"""
        noise_level = 0.02 * torch.rand(1)
        noise = torch.randn_like(batch['images']) * noise_level
        batch['images'] = torch.clamp(batch['images'] + noise, 0, 1)
        
        return batch


class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.total_loss = 0.0
        self.focal_loss = 0.0
        self.dice_loss = 0.0
        self.iou_loss = 0.0
        self.count = 0
    
    def update(self, loss_dict: Dict[str, torch.Tensor]):
        """æ›´æ–°æŒ‡æ ‡"""
        def safe_item(value):
            """å®‰å…¨åœ°è·å–æ•°å€¼ï¼Œå¤„ç†tensorå’Œfloat"""
            if isinstance(value, torch.Tensor):
                return value.item()
            return float(value)
        
        self.total_loss += safe_item(loss_dict.get('total_loss', 0.0))
        self.focal_loss += safe_item(loss_dict.get('focal_loss', 0.0))
        self.dice_loss += safe_item(loss_dict.get('dice_loss', 0.0))
        self.iou_loss += safe_item(loss_dict.get('iou_loss', 0.0))
        self.count += 1
    
    def compute(self) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if self.count == 0:
            return {}
        
        return {
            'avg_total_loss': self.total_loss / self.count,
            'avg_focal_loss': self.focal_loss / self.count,
            'avg_dice_loss': self.dice_loss / self.count,
            'avg_iou_loss': self.iou_loss / self.count
        }


def validate_sam_batch(batch: Dict[str, Any]) -> bool:
    """éªŒè¯SAMæ‰¹æ¬¡æ•°æ®çš„æœ‰æ•ˆæ€§ - æ”¯æŒå¼ é‡æ©ç """
    required_keys = ['images', 'ground_truth_masks']
    
    for key in required_keys:
        if key not in batch:
            print(f"æ‰¹æ¬¡æ•°æ®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
            return False
    
    # æ£€æŸ¥å¼ é‡å½¢çŠ¶
    images = batch['images']
    masks = batch['ground_truth_masks']
    
    # æ£€æŸ¥å›¾åƒå¼ é‡
    if not isinstance(images, torch.Tensor):
        print(f"imagesä¸æ˜¯å¼ é‡: {type(images)}")
        return False
        
    if len(images.shape) != 4:
        print(f"å›¾åƒå¼ é‡å½¢çŠ¶é”™è¯¯: {images.shape}ï¼ŒæœŸæœ› [B, C, H, W]")
        return False
    
    # æ£€æŸ¥æ©ç å¼ é‡
    if not isinstance(masks, torch.Tensor):
        print(f"masksä¸æ˜¯å¼ é‡: {type(masks)}ï¼ŒæœŸæœ› torch.Tensor")
        return False
    
    if len(masks.shape) != 4:
        print(f"æ©ç å¼ é‡å½¢çŠ¶é”™è¯¯: {masks.shape}ï¼ŒæœŸæœ› [B, N, H, W]")
        return False
    
    # æ£€æŸ¥æ‰¹æ¬¡å¤§å°åŒ¹é…
    if images.shape[0] != masks.shape[0]:
        print(f"æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: å›¾åƒ {images.shape[0]} vs æ©ç  {masks.shape[0]}")
        return False
    
    return True


def create_sam_training_step(model, optimizer, loss_fn, device):
    """åˆ›å»ºSAMè®­ç»ƒæ­¥éª¤å‡½æ•°"""
    
    def training_step(batch: Dict[str, Any]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        # éªŒè¯æ‰¹æ¬¡æ•°æ®
        if not validate_sam_batch(batch):
            return {'error': 1.0}
        
        # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
        inputs, targets = prepare_sam_inputs(batch)
        
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                inputs[key] = value.to(device)
            elif isinstance(value, list):
                inputs[key] = [v.to(device) if isinstance(v, torch.Tensor) else v for v in value]
        
        for key, value in targets.items():
            if isinstance(value, torch.Tensor):
                targets[key] = value.to(device)
        
        # å‰å‘ä¼ æ’­
        model.train()
        predictions = model(inputs)
        
        # è®¡ç®—æŸå¤±
        loss_dict = loss_fn(predictions, targets)
        total_loss = loss_dict['total_loss']
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # è¿”å›æŸå¤±ä¿¡æ¯
        def safe_item(value):
            """å®‰å…¨åœ°è·å–æ•°å€¼"""
            if isinstance(value, torch.Tensor):
                return value.item()
            return float(value)
        
        return {key: safe_item(value) for key, value in loss_dict.items()}
    
    return training_step


def calculate_sam_loss(predictions: Dict[str, torch.Tensor], 
                      ground_truth: Dict[str, torch.Tensor],
                      loss_config: Optional[Dict] = None) -> Dict[str, torch.Tensor]:
    """è®¡ç®—SAMæŸå¤±çš„ä¾¿æ·å‡½æ•°"""
    if loss_config is None:
        loss_config = {
            'focal_loss_weight': 20.0,
            'dice_loss_weight': 1.0,
            'iou_loss_weight': 1.0
        }
    
    loss_fn = SAMLoss(**loss_config)
    return loss_fn(predictions, ground_truth)

def prepare_sam_inputs_multi_instance(batch: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor]]:
    """å‡†å¤‡SAMè®­ç»ƒçš„è¾“å…¥å’Œç›®æ ‡ - æ”¯æŒå¤šå®ä¾‹"""
    
    try:
        # è¾“å…¥æ•°æ®
        inputs = {
            'images': batch['images'],  # [B, 3, H, W]
            'point_coords': batch.get('point_coords', []),
            'point_labels': batch.get('point_labels', []),
            'boxes': batch.get('boxes', []),
            'mask_inputs': batch.get('mask_inputs', None),
            'multimask_output': batch.get('multimask_output', False)
        }
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿æŒå¤šå®ä¾‹æ©ç æ ¼å¼
        ground_truth_masks = batch['ground_truth_masks']  # [B, N, H, W]
        device = inputs['images'].device
        
        if isinstance(ground_truth_masks, torch.Tensor):
            targets_masks = ground_truth_masks.to(device)
        else:
            # å¤„ç†åˆ—è¡¨æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            batch_size = inputs['images'].shape[0]
            h, w = inputs['images'].shape[-2:]
            targets_masks = torch.zeros(batch_size, 1, h, w, dtype=torch.float32, device=device)
        
        targets = {
            'masks': targets_masks,  # [B, N, H, W] - ä¿æŒå¤šå®ä¾‹æ ¼å¼
            'num_instances': torch.tensor([masks.shape[1] for masks in [targets_masks]], device=device)
        }
        
        return inputs, targets
        
    except Exception as e:
        print(f"ERROR in prepare_sam_inputs_multi_instance: {e}")
        import traceback
        traceback.print_exc()
        
        # è¿”å›é»˜è®¤å€¼
        batch_size = batch['images'].shape[0]
        device = batch['images'].device
        h, w = batch['images'].shape[-2:]
        
        inputs = {
            'images': batch['images'],
            'point_coords': [],
            'point_labels': [],
            'boxes': [],
            'mask_inputs': None,
            'multimask_output': False
        }
        
        targets = {
            'masks': torch.zeros(batch_size, 1, h, w, dtype=torch.float32, device=device),
            'num_instances': torch.ones(batch_size, device=device)
        }
        
        return inputs, targets


class SAMLossMultiInstance(nn.Module):
    """SAMè®­ç»ƒçš„å¤šå®ä¾‹æŸå¤±å‡½æ•°"""
    
    def __init__(
        self,
        focal_loss_weight: float = 20.0,
        dice_loss_weight: float = 1.0,
        iou_loss_weight: float = 1.0,
        instance_loss_weight: float = 5.0,
        use_focal_loss: bool = True,
        use_dice_loss: bool = True,
        use_iou_loss: bool = True,
        use_instance_loss: bool = True
    ):
        super().__init__()
        
        self.focal_loss_weight = focal_loss_weight
        self.dice_loss_weight = dice_loss_weight
        self.iou_loss_weight = iou_loss_weight
        self.instance_loss_weight = instance_loss_weight
        
        self.use_focal_loss = use_focal_loss
        self.use_dice_loss = use_dice_loss
        self.use_iou_loss = use_iou_loss
        self.use_instance_loss = use_instance_loss
        
        # Focal losså‚æ•°
        self.focal_alpha = 0.8
        self.focal_gamma = 2.0
    
    def forward(self, predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """è®¡ç®—å¤šå®ä¾‹æŸå¤±"""
        
        pred_masks = predictions['masks']  # [B, N_pred, H, W] æˆ– [B, 1, H, W]
        target_masks = targets['masks']    # [B, N_target, H, W]
        
        device = pred_masks.device
        batch_size = pred_masks.shape[0]
        
        loss_dict = {}
        total_loss = 0.0
        
        # ğŸ”§ ç­–ç•¥1ï¼šå¦‚æœSAMè¾“å‡ºå•ä¸ªæ©ç ï¼Œä¸æœ€ä½³åŒ¹é…çš„ç›®æ ‡è®¡ç®—æŸå¤±
        if pred_masks.shape[1] == 1:
            # SAMè¾“å‡ºå•ä¸ªæ©ç ï¼Œæ‰¾åˆ°æœ€ä½³åŒ¹é…çš„ç›®æ ‡å®ä¾‹
            best_matches = []
            
            for b in range(batch_size):
                pred_mask = pred_masks[b, 0]  # [H, W]
                target_instances = target_masks[b]  # [N, H, W]
                
                best_iou = 0.0
                best_target = None
                
                # æ‰¾åˆ°IoUæœ€é«˜çš„ç›®æ ‡å®ä¾‹
                for i in range(target_instances.shape[0]):
                    target_mask = target_instances[i]
                    if target_mask.sum() == 0:  # è·³è¿‡ç©ºæ©ç 
                        continue
                    
                    iou = self._calculate_iou(pred_mask, target_mask)
                    if iou > best_iou:
                        best_iou = iou
                        best_target = target_mask
                
                if best_target is not None:
                    best_matches.append(best_target)
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œä½¿ç”¨åˆå¹¶çš„ç›®æ ‡
                    combined_target = (target_instances.sum(dim=0) > 0).float()
                    best_matches.append(combined_target)
            
            # é‡æ–°ç»„ç»‡ä¸ºæ‰¹æ¬¡æ ¼å¼
            matched_targets = torch.stack(best_matches).unsqueeze(1)  # [B, 1, H, W]
            
            # è®¡ç®—æŸå¤±
            if self.use_focal_loss:
                focal_loss = self._focal_loss(pred_masks, matched_targets)
                loss_dict['focal_loss'] = focal_loss
                total_loss += self.focal_loss_weight * focal_loss
            
            if self.use_dice_loss:
                dice_loss = self._dice_loss(pred_masks, matched_targets)
                loss_dict['dice_loss'] = dice_loss
                total_loss += self.dice_loss_weight * dice_loss
            
            if self.use_iou_loss:
                iou_loss = self._iou_loss(pred_masks, matched_targets)
                loss_dict['iou_loss'] = iou_loss
                total_loss += self.iou_loss_weight * iou_loss
        
        # ğŸ”§ ç­–ç•¥2ï¼šå¦‚æœSAMè¾“å‡ºå¤šä¸ªæ©ç ï¼Œè¿›è¡Œå®ä¾‹çº§åŒ¹é…
        else:
            # å®ä¾‹çº§æŸå¤±è®¡ç®—ï¼ˆæ›´å¤æ‚ï¼Œä½†æ›´å‡†ç¡®ï¼‰
            instance_losses = []
            
            for b in range(batch_size):
                pred_instances = pred_masks[b]  # [N_pred, H, W]
                target_instances = target_masks[b]  # [N_target, H, W]
                
                # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•è¿›è¡Œæœ€ä¼˜åŒ¹é…ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                instance_loss = self._compute_instance_matching_loss(
                    pred_instances, target_instances
                )
                instance_losses.append(instance_loss)
            
            avg_instance_loss = torch.stack(instance_losses).mean()
            loss_dict['instance_loss'] = avg_instance_loss
            total_loss += self.instance_loss_weight * avg_instance_loss
        
        loss_dict['total_loss'] = total_loss
        return loss_dict
    
    def _calculate_iou(self, pred: torch.Tensor, target: torch.Tensor) -> float:
        """è®¡ç®—IoU"""
        pred_binary = (pred > 0.5).float()
        target_binary = (target > 0.5).float()
        
        intersection = (pred_binary * target_binary).sum()
        union = pred_binary.sum() + target_binary.sum() - intersection
        
        if union > 0:
            return (intersection / union).item()
        return 0.0
    
    def _compute_instance_matching_loss(self, pred_instances: torch.Tensor, 
                                      target_instances: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å®ä¾‹åŒ¹é…æŸå¤±ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå®ç°ç®€åŒ–çš„åŒ¹é…ç­–ç•¥
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•è¿›è¡Œæœ€ä¼˜åŒ¹é…
        
        n_pred = pred_instances.shape[0]
        n_target = target_instances.shape[0]
        
        if n_pred == 0 or n_target == 0:
            return torch.tensor(1.0, device=pred_instances.device)
        
        # è®¡ç®—æ‰€æœ‰é¢„æµ‹-ç›®æ ‡å¯¹çš„æŸå¤±çŸ©é˜µ
        loss_matrix = torch.zeros(n_pred, n_target, device=pred_instances.device)
        
        for i in range(n_pred):
            for j in range(n_target):
                if target_instances[j].sum() == 0:  # è·³è¿‡ç©ºç›®æ ‡
                    continue
                
                pred_mask = pred_instances[i:i+1]  # [1, H, W]
                target_mask = target_instances[j:j+1]  # [1, H, W]
                
                # è®¡ç®—äºŒå€¼äº¤å‰ç†µæŸå¤±
                bce_loss = F.binary_cross_entropy_with_logits(pred_mask, target_mask)
                loss_matrix[i, j] = bce_loss
        
        # ç®€åŒ–åŒ¹é…ï¼šé€‰æ‹©æ¯ä¸ªé¢„æµ‹çš„æœ€å°æŸå¤±
        if loss_matrix.numel() > 0:
            min_losses = loss_matrix.min(dim=1)[0]
            return min_losses.mean()
        else:
            return torch.tensor(1.0, device=pred_instances.device)
    
    def _focal_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """Focal Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        ce_loss = F.binary_cross_entropy_with_logits(
            pred_masks, target_masks, reduction='none'
        )
        
        p_t = pred_sigmoid * target_masks + (1 - pred_sigmoid) * (1 - target_masks)
        alpha_t = self.focal_alpha * target_masks + (1 - self.focal_alpha) * (1 - target_masks)
        
        focal_weight = alpha_t * (1 - p_t) ** self.focal_gamma
        focal_loss = focal_weight * ce_loss
        
        return focal_loss.mean()
    
    def _dice_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """Dice Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        smooth = 1.0
        
        intersection = (pred_sigmoid * target_masks).sum(dim=(-2, -1))
        total = pred_sigmoid.sum(dim=(-2, -1)) + target_masks.sum(dim=(-2, -1))
        
        dice_score = (2.0 * intersection + smooth) / (total + smooth)
        dice_loss = 1.0 - dice_score
        
        return dice_loss.mean()
    
    def _iou_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -> torch.Tensor:
        """IoU Losså®ç°"""
        pred_sigmoid = torch.sigmoid(pred_masks)
        
        intersection = (pred_sigmoid * target_masks).sum(dim=(-2, -1))
        union = pred_sigmoid.sum(dim=(-2, -1)) + target_masks.sum(dim=(-2, -1)) - intersection
        
        iou_score = intersection / (union + 1e-6)
        iou_loss = 1.0 - iou_score
        
        return iou_loss.mean()

def validate_sam_batch_multi_instance(batch: Dict[str, Any]) -> bool:
    """éªŒè¯SAMå¤šå®ä¾‹æ‰¹æ¬¡æ•°æ®çš„æœ‰æ•ˆæ€§"""
    required_keys = ['images', 'ground_truth_masks']
    
    for key in required_keys:
        if key not in batch:
            print(f"æ‰¹æ¬¡æ•°æ®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
            return False
    
    # æ£€æŸ¥å¼ é‡å½¢çŠ¶
    images = batch['images']
    masks = batch['ground_truth_masks']
    
    if not isinstance(images, torch.Tensor):
        print(f"imagesä¸æ˜¯å¼ é‡: {type(images)}")
        return False
        
    if len(images.shape) != 4:
        print(f"å›¾åƒå¼ é‡å½¢çŠ¶é”™è¯¯: {images.shape}ï¼ŒæœŸæœ› [B, C, H, W]")
        return False
    
    if not isinstance(masks, torch.Tensor):
        print(f"masksä¸æ˜¯å¼ é‡: {type(masks)}ï¼ŒæœŸæœ› torch.Tensor")
        return False
    
    if len(masks.shape) != 4:
        print(f"æ©ç å¼ é‡å½¢çŠ¶é”™è¯¯: {masks.shape}ï¼ŒæœŸæœ› [B, N, H, W]")
        return False
    
    # æ£€æŸ¥æ‰¹æ¬¡å¤§å°åŒ¹é…
    if images.shape[0] != masks.shape[0]:
        print(f"æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: å›¾åƒ {images.shape[0]} vs æ©ç  {masks.shape[0]}")
        return False
    
    return True

def create_sam_training_step_multi_instance(model, optimizer, loss_fn, device):
    """åˆ›å»ºå¤šå®ä¾‹SAMè®­ç»ƒæ­¥éª¤å‡½æ•°"""
    
    def training_step(batch: Dict[str, Any]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€ä¸ªå¤šå®ä¾‹è®­ç»ƒæ­¥éª¤"""
        
        # éªŒè¯æ‰¹æ¬¡æ•°æ®
        if not validate_sam_batch_multi_instance(batch):
            return {'error': 1.0}
        
        # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
        inputs, targets = prepare_sam_inputs_multi_instance(batch)
        
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                inputs[key] = value.to(device)
            elif isinstance(value, list):
                inputs[key] = [v.to(device) if isinstance(v, torch.Tensor) else v for v in value]
        
        for key, value in targets.items():
            if isinstance(value, torch.Tensor):
                targets[key] = value.to(device)
        
        # å‰å‘ä¼ æ’­
        model.train()
        predictions = model(inputs)
        
        # è®¡ç®—æŸå¤±
        loss_dict = loss_fn(predictions, targets)
        total_loss = loss_dict['total_loss']
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # è¿”å›æŸå¤±ä¿¡æ¯
        def safe_item(value):
            if isinstance(value, torch.Tensor):
                return value.item()
            return float(value)
        
        return {key: safe_item(value) for key, value in loss_dict.items()}
    
    return training_step


