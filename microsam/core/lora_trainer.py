"""
LoRAè®­ç»ƒå™¨ (ä¿®æ”¹ç‰ˆ)
è´Ÿè´£LoRAæ¨¡å‹çš„è®­ç»ƒã€éªŒè¯å’Œä¿å­˜
ä½¿ç”¨æ–°çš„SAMæ¨¡å‹åŠ è½½æ¶æ„
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import time
import json
from tqdm import tqdm
import wandb

from config.lora_config import LoRATrainingSettings
from lora.sam_lora_wrapper import SAMLoRAWrapper, create_sam_lora_model
from lora.data_loaders import create_data_loaders
from lora.training_utils import SAMLoss, prepare_sam_inputs, TrainingMetrics, create_sam_training_step
from core.metrics import ComprehensiveMetrics, MetricsResult
from utils.file_utils import setup_logging
from utils.model_utils import optimize_memory, get_device_info, print_model_summary


class LoRATrainer:
    """LoRAè®­ç»ƒå™¨ - ä½¿ç”¨æ–°çš„SAMæ¶æ„"""
    
    def __init__(self, config: LoRATrainingSettings):
        self.config = config
        self.device = self._setup_device()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.data_loaders = {}
        self.loss_fn = None
        self.metrics_calculator = ComprehensiveMetrics()
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_metric = float('inf')
        self.early_stopping_counter = 0
        
        # æ—¥å¿—å’Œç›‘æ§
        self.writer = None
        self.logger = setup_logging()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = self.config.get_model_output_dir()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        self.config.save_to_json(self.output_dir / "config.json")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if self.config.experiment.device == "auto":
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            device = torch.device(self.config.experiment.device)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device.type == "cuda":
            device_info = get_device_info()
            print(f"GPUè®¾å¤‡æ•°é‡: {device_info['cuda_device_count']}")
            print(f"å½“å‰GPU: {torch.cuda.current_device()}")
            if device_info['gpu_memory']:
                current_gpu = f"gpu_{torch.cuda.current_device()}"
                if current_gpu in device_info['gpu_memory']:
                    gpu_info = device_info['gpu_memory'][current_gpu]
                    print(f"GPUå†…å­˜: {gpu_info['total_memory'] / 1e9:.1f} GB")
        
        return device
    
    # åœ¨ core/lora_trainer.py ä¸­æ·»åŠ è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥

    def setup_model(self) -> bool:
        """è®¾ç½®SAM LoRAæ¨¡å‹ - ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§"""
        try:
            print("æ­£åœ¨è®¾ç½®SAM LoRAæ¨¡å‹...")
            
            # åˆ›å»ºLoRAé…ç½®
            lora_config = {
                'rank': self.config.lora.rank,
                'alpha': self.config.lora.alpha,
                'dropout': self.config.lora.dropout,
                'target_modules': self.config.lora.target_modules,
                'apply_lora_to': self.config.model.apply_lora_to,
                'freeze_image_encoder': self.config.model.freeze_backbone,
                'freeze_prompt_encoder': self.config.model.freeze_prompt_encoder,
                'freeze_mask_decoder': self.config.model.freeze_mask_decoder
            }
            
            # åˆ›å»ºSAM LoRAæ¨¡å‹
            self.model = create_sam_lora_model(
                model_type=self.config.model.base_model_name,
                lora_config=lora_config,
                device=str(self.device)
            )
            
            if self.model is None:
                print("SAM LoRAæ¨¡å‹åˆ›å»ºå¤±è´¥")
                return False
            
            # ğŸ”§ ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            self.model = self.model.to(self.device)
            
            # ğŸ”§ ç¡®ä¿æ‰€æœ‰LoRAæ¨¡å—åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if hasattr(self.model, 'lora_modules'):
                for name, lora_module in self.model.lora_modules.items():
                    if hasattr(lora_module, 'lora'):
                        lora_module.lora = lora_module.lora.to(self.device)
                    lora_module = lora_module.to(self.device)
            
            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            self.model.print_model_info()
            print_model_summary(self.model)
            
            # ğŸ”§ éªŒè¯æ¨¡å‹è®¾å¤‡ä¸€è‡´æ€§
            self._verify_model_device_consistency()
            
            return True
            
        except Exception as e:
            print(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _verify_model_device_consistency(self):
        """éªŒè¯æ¨¡å‹è®¾å¤‡ä¸€è‡´æ€§"""
        print(f"\néªŒè¯æ¨¡å‹è®¾å¤‡ä¸€è‡´æ€§...")
        
        device_counts = {}
        
        # æ£€æŸ¥ä¸»è¦ç»„ä»¶
        for name, module in [
            ('image_encoder', self.model.image_encoder),
            ('prompt_encoder', self.model.prompt_encoder), 
            ('mask_decoder', self.model.mask_decoder)
        ]:
            if module is not None:
                for param_name, param in module.named_parameters():
                    device = str(param.device)
                    device_counts[device] = device_counts.get(device, 0) + 1
        
        # æ£€æŸ¥LoRAæ¨¡å—
        if hasattr(self.model, 'lora_modules'):
            for lora_name, lora_module in self.model.lora_modules.items():
                if hasattr(lora_module, 'lora'):
                    for param_name, param in lora_module.lora.named_parameters():
                        device = str(param.device)
                        device_counts[device] = device_counts.get(device, 0) + 1
        
        print(f"è®¾å¤‡åˆ†å¸ƒ: {device_counts}")
        
        if len(device_counts) > 1:
            print(f"âš ï¸  å‘ç°å¤šä¸ªè®¾å¤‡ï¼Œæ­£åœ¨ç»Ÿä¸€åˆ° {self.device}")
            # å¼ºåˆ¶ç§»åŠ¨æ‰€æœ‰ç»„ä»¶
            self.model = self.model.to(self.device)
            print(f"âœ… æ‰€æœ‰æ¨¡å‹ç»„ä»¶å·²ç§»åŠ¨åˆ° {self.device}")
        else:
            print(f"âœ… æ‰€æœ‰æ¨¡å‹ç»„ä»¶éƒ½åœ¨ {self.device} ä¸Š")
    
    def setup_data_loaders(self) -> bool:
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        try:
            print("æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
            
            self.data_loaders = create_data_loaders(
                config=self.config.data,
                dataset_type="sam"  # ä½¿ç”¨SAMæ•°æ®é›†æ ¼å¼
            )
            
            print("æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ:")
            for split, loader in self.data_loaders.items():
                print(f"  {split}: {len(loader)} æ‰¹æ¬¡, {len(loader.dataset)} æ ·æœ¬")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å™¨è®¾ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def setup_optimizer_and_loss(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°"""
        # åªä¼˜åŒ–LoRAå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(trainable_params)}")
        total_trainable = sum(p.numel() for p in trainable_params)
        print(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_trainable:,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if self.config.training.optimizer.lower() == "adamw":
            self.optimizer = optim.AdamW(
                trainable_params,
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay,
                betas=(self.config.training.adam_beta1, self.config.training.adam_beta2),
                eps=self.config.training.adam_epsilon
            )
        else:
            self.optimizer = optim.Adam(
                trainable_params,
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.training.lr_scheduler_type == "cosine":
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training.num_epochs,
                eta_min=self.config.training.learning_rate * 0.01
            )
        elif self.config.training.lr_scheduler_type == "step":
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.training.num_epochs // 3,
                gamma=0.1
            )
        else:
            self.scheduler = None
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        loss_config = {
            'focal_loss_weight': 20.0,
            'dice_loss_weight': 1.0,
            'iou_loss_weight': 1.0,
            'use_focal_loss': True,
            'use_dice_loss': True,
            'use_iou_loss': True
        }
        self.loss_fn = SAMLoss(**loss_config)
        
        print(f"ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ: {type(self.optimizer).__name__}")
        print(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {type(self.scheduler).__name__ if self.scheduler else 'None'}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        # TensorBoard
        log_dir = self.config.get_logs_dir()
        self.writer = SummaryWriter(log_dir)
        
        # Weights & Biases
        if self.config.experiment.use_wandb:
            wandb.init(
                project=self.config.experiment.wandb_project,
                entity=self.config.experiment.wandb_entity,
                name=self.config.experiment.run_name,
                config=self.config.to_dict(),
                dir=str(self.output_dir)
            )
    
    def train(self) -> bool:
        """å¼€å§‹è®­ç»ƒ"""
        print("="*60)
        print("å¼€å§‹SAM LoRAè®­ç»ƒ")
        print("="*60)
        
        # è®¾ç½®æ‰€æœ‰ç»„ä»¶
        if not self.setup_model():
            return False
        
        if not self.setup_data_loaders():
            return False
        
        self.setup_optimizer_and_loss()
        self.setup_logging()
        
        # åˆ›å»ºè®­ç»ƒæ­¥éª¤å‡½æ•°
        training_step_fn = create_sam_training_step(
            self.model, self.optimizer, self.loss_fn, self.device
        )
        
        # è®­ç»ƒå¾ªç¯
        try:
            for epoch in range(self.config.training.num_epochs):
                self.current_epoch = epoch
                
                print(f"\nå¼€å§‹ç¬¬ {epoch + 1}/{self.config.training.num_epochs} è½®è®­ç»ƒ")
                
                # è®­ç»ƒä¸€ä¸ªepoch
                train_metrics = self.train_epoch(training_step_fn)
                
                # éªŒè¯
                val_metrics = self.validate() if 'val' in self.data_loaders else {}
                
                # è®°å½•æ—¥å¿—
                self.log_metrics(train_metrics, val_metrics, epoch)
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % max(1, self.config.training.save_steps // len(self.data_loaders['train'])) == 0:
                    self.save_checkpoint(epoch, val_metrics)
                
                # æ—©åœæ£€æŸ¥
                if self.check_early_stopping(val_metrics):
                    print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒ")
                    break
                
                # æ›´æ–°å­¦ä¹ ç‡
                if self.scheduler:
                    self.scheduler.step()
                
                # å†…å­˜ä¼˜åŒ–
                optimize_memory()
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_final_model()
            
            print("\n" + "="*60)
            print("è®­ç»ƒå®Œæˆ!")
            print("="*60)
            return True
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            if self.writer:
                self.writer.close()
            if self.config.experiment.use_wandb:
                wandb.finish()
    
    def train_epoch(self, training_step_fn) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_metrics = TrainingMetrics()
        
        progress_bar = tqdm(
            self.data_loaders['train'],
            desc=f"Epoch {self.current_epoch + 1}/{self.config.training.num_epochs}"
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # try:
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            step_metrics = training_step_fn(batch)
            
            # æ›´æ–°ç»Ÿè®¡
            if 'error' not in step_metrics:
                epoch_metrics.update(step_metrics)
                self.global_step += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'loss': f"{step_metrics.get('total_loss', 0):.4f}",
                    'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}"
                })
                
                # è®°å½•æ­¥éª¤æ—¥å¿—
                if self.global_step % self.config.training.logging_steps == 0:
                    self.log_step_metrics(step_metrics, batch_idx)
            else:
                print(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥")
                
            # except Exception as e:
            #     print(f"è®­ç»ƒæ­¥éª¤å¤±è´¥ (æ‰¹æ¬¡ {batch_idx}): {e}")
            #     continue
        
        # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
        avg_metrics = epoch_metrics.compute()
        avg_metrics['learning_rate'] = self.optimizer.param_groups[0]['lr']
        
        return avg_metrics
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        if 'val' not in self.data_loaders:
            return {}
        
        print("æ­£åœ¨éªŒè¯...")
        self.model.eval()
        
        val_metrics = TrainingMetrics()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch in tqdm(self.data_loaders['val'], desc="Validating"):
                try:
                    # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                    inputs, targets = prepare_sam_inputs(batch)
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            inputs[key] = value.to(self.device)
                        elif isinstance(value, list):
                            inputs[key] = [v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value]
                    
                    for key, value in targets.items():
                        if isinstance(value, torch.Tensor):
                            targets[key] = value.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    predictions = self.model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_fn(predictions, targets)
                    val_metrics.update(loss_dict)
                    
                    # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡ç”¨äºæŒ‡æ ‡è®¡ç®—
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    target_masks = targets['masks'].cpu().numpy()
                    
                    all_predictions.extend(pred_masks)
                    all_targets.extend(target_masks)
                    
                except Exception as e:
                    print(f"éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_val_metrics = val_metrics.compute()
        
        # è®¡ç®—åˆ†å‰²æŒ‡æ ‡
        if all_predictions and all_targets:
            seg_metrics = self._compute_segmentation_metrics(all_predictions[:10], all_targets[:10])  # é™åˆ¶æ•°é‡ä»¥èŠ‚çœæ—¶é—´
            avg_val_metrics.update(seg_metrics)
        
        return avg_val_metrics
    
    def _compute_segmentation_metrics(self, predictions: List, targets: List) -> Dict[str, float]:
        """è®¡ç®—åˆ†å‰²æŒ‡æ ‡"""
        try:
            all_ious = []
            all_dices = []
            
            for pred, target in zip(predictions, targets):
                if pred.ndim > 2:
                    pred = pred[0]  # å–ç¬¬ä¸€ä¸ªé€šé“
                if target.ndim > 2:
                    target = target[0]
                
                # äºŒå€¼åŒ–
                pred_binary = (pred > 0.5).astype(np.int32)
                target_binary = (target > 0.5).astype(np.int32)
                
                # è®¡ç®—IoUå’ŒDice
                intersection = np.sum(pred_binary * target_binary)
                union = np.sum(pred_binary) + np.sum(target_binary) - intersection
                
                if union > 0:
                    iou = intersection / union
                    dice = 2 * intersection / (np.sum(pred_binary) + np.sum(target_binary))
                    
                    all_ious.append(iou)
                    all_dices.append(dice)
            
            return {
                'val_iou': np.mean(all_ious) if all_ious else 0.0,
                'val_dice': np.mean(all_dices) if all_dices else 0.0
            }
            
        except Exception as e:
            print(f"åˆ†å‰²æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {'val_iou': 0.0, 'val_dice': 0.0}
    
    def log_metrics(self, train_metrics: Dict[str, float], 
                   val_metrics: Dict[str, float], epoch: int):
        """è®°å½•æŒ‡æ ‡"""
        # TensorBoardæ—¥å¿—
        if self.writer:
            for key, value in train_metrics.items():
                self.writer.add_scalar(f'train/{key}', value, epoch)
            
            for key, value in val_metrics.items():
                self.writer.add_scalar(f'val/{key}', value, epoch)
        
        # Weights & Biasesæ—¥å¿—
        if self.config.experiment.use_wandb:
            log_dict = {f'train/{k}': v for k, v in train_metrics.items()}
            log_dict.update({f'val/{k}': v for k, v in val_metrics.items()})
            log_dict['epoch'] = epoch
            wandb.log(log_dict)
        
        # æ§åˆ¶å°è¾“å‡º
        print(f"\nEpoch {epoch + 1}/{self.config.training.num_epochs} - æŒ‡æ ‡æ‘˜è¦:")
        print("è®­ç»ƒæŒ‡æ ‡:")
        for key, value in train_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        if val_metrics:
            print("éªŒè¯æŒ‡æ ‡:")
            for key, value in val_metrics.items():
                print(f"  {key}: {value:.4f}")
    
    def log_step_metrics(self, step_metrics: Dict[str, float], step: int):
        """è®°å½•æ­¥éª¤æŒ‡æ ‡"""
        if self.writer:
            for key, value in step_metrics.items():
                self.writer.add_scalar(f'train/step_{key}', value, self.global_step)
        
        if self.config.experiment.use_wandb:
            log_dict = {f'train/step_{k}': v for k, v in step_metrics.items()}
            log_dict['global_step'] = self.global_step
            wandb.log(log_dict)
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = self.config.get_checkpoint_dir()
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_metric': self.best_metric,
            'config': self.config.to_dict(),
            'metrics': metrics
        }
        
        # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹
        checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch + 1}.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_metric = metrics.get('avg_total_loss', float('inf'))
        if current_metric < self.best_metric:
            self.best_metric = current_metric
            best_path = checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {current_metric:.4f})")
        
        # ä¿å­˜LoRAæƒé‡
        lora_save_path = checkpoint_dir / f"lora_epoch_{epoch + 1}"
        self.model.save_lora_weights(lora_save_path)
        
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def check_early_stopping(self, metrics: Dict[str, float]) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        if not metrics or 'avg_total_loss' not in metrics:
            return False
        
        current_metric = metrics['avg_total_loss']
        
        if current_metric < self.best_metric:
            self.early_stopping_counter = 0
        else:
            self.early_stopping_counter += 1
        
        return self.early_stopping_counter >= self.config.training.early_stopping_patience
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        # ä¿å­˜LoRAæƒé‡
        final_dir = self.output_dir / "final_model"
        self.model.save_lora_weights(final_dir)
        
        # ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
        merged_path = final_dir / "merged_sam_model.pth"
        self.model.merge_and_save_full_model(merged_path)
        
        # ä¿å­˜è®­ç»ƒæ‘˜è¦
        summary = {
            'training_completed': True,
            'total_epochs': self.current_epoch + 1,
            'total_steps': self.global_step,
            'best_metric': self.best_metric,
            'model_type': self.config.model.base_model_name,
            'lora_config': {
                'rank': self.config.lora.rank,
                'alpha': self.config.lora.alpha,
                'dropout': self.config.lora.dropout
            },
            'config': self.config.to_dict()
        }
        
        with open(final_dir / "training_summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_dir}")
    
    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if self.scheduler and checkpoint['scheduler_state_dict']:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint['epoch']
            self.global_step = checkpoint['global_step']
            self.best_metric = checkpoint['best_metric']
            
            print(f"æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
            print(f"ä»ç¬¬ {self.current_epoch + 1} è½®ç»§ç»­è®­ç»ƒ")
            
            return True
            
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def evaluate_model(self, test_data_loader: Optional[DataLoader] = None) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if test_data_loader is None:
            test_data_loader = self.data_loaders.get('test')
        
        if test_data_loader is None:
            print("æ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯„ä¼°")
            return {}
        
        self.model.eval()
        test_metrics = TrainingMetrics()
        all_predictions = []
        all_targets = []
        
        print("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        with torch.no_grad():
            for batch in tqdm(test_data_loader, desc="Evaluating"):
                try:
                    # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                    inputs, targets = prepare_sam_inputs(batch)
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            inputs[key] = value.to(self.device)
                        elif isinstance(value, list):
                            inputs[key] = [v.to(self.device) if isinstance(v, torch.Tensor) else v for v in value]
                    
                    for key, value in targets.items():
                        if isinstance(value, torch.Tensor):
                            targets[key] = value.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    predictions = self.model(inputs)
                    
                    # è®¡ç®—æŸå¤±
                    loss_dict = self.loss_fn(predictions, targets)
                    test_metrics.update(loss_dict)
                    
                    # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
                    pred_masks = torch.sigmoid(predictions['masks']).cpu().numpy()
                    target_masks = targets['masks'].cpu().numpy()
                    
                    all_predictions.extend(pred_masks)
                    all_targets.extend(target_masks)
                    
                except Exception as e:
                    print(f"è¯„ä¼°æ­¥éª¤å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_metrics = test_metrics.compute()
        
        # è®¡ç®—åˆ†å‰²æŒ‡æ ‡
        if all_predictions and all_targets:
            seg_metrics = self._compute_segmentation_metrics(all_predictions, all_targets)
            final_metrics.update(seg_metrics)
        
        print("è¯„ä¼°å®Œæˆ:")
        for key, value in final_metrics.items():
            print(f"  {key}: {value:.4f}")
        
        return final_metrics


def create_trainer_from_config(config_path: str) -> LoRATrainer:
    """ä»é…ç½®æ–‡ä»¶åˆ›å»ºè®­ç»ƒå™¨"""
    config = LoRATrainingSettings.from_json(config_path)
    return LoRATrainer(config)


def resume_training(checkpoint_path: str, config_path: Optional[str] = None) -> LoRATrainer:
    """æ¢å¤è®­ç»ƒ"""
    if config_path:
        trainer = create_trainer_from_config(config_path)
    else:
        # ä»æ£€æŸ¥ç‚¹ç›®å½•åŠ è½½é…ç½®
        checkpoint_dir = Path(checkpoint_path).parent
        config_file = checkpoint_dir.parent / "config.json"
        if config_file.exists():
            trainer = create_trainer_from_config(str(config_file))
        else:
            print("æ— æ³•æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            from config.lora_config import LoRATrainingSettings
            trainer = LoRATrainer(LoRATrainingSettings())
    
    trainer.load_checkpoint(checkpoint_path)
    return trainer