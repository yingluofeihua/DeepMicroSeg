"""
LoRAé€‚é…å™¨å®ç°
æ”¯æŒå¯¹SAMæ¨¡å‹çš„å„ä¸ªç»„ä»¶æ·»åŠ LoRAé€‚é…å™¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
import math
from pathlib import Path
import json


class LoRALayer(nn.Module):
    """LoRAé€‚é…å™¨å±‚"""
    
    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.1,
        bias: bool = False
    ):
        super().__init__()
        
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # LoRAæƒé‡
        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=bias)
        
        # Dropout
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        
        # åˆå§‹åŒ–
        self.reset_parameters()
    
    def reset_parameters(self):
        """é‡ç½®å‚æ•°"""
        # Aä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
        nn.init.normal_(self.lora_A.weight, std=1/self.rank)
        # Båˆå§‹åŒ–ä¸º0ï¼Œç¡®ä¿å¼€å§‹æ—¶LoRAè¾“å‡ºä¸º0
        nn.init.zeros_(self.lora_B.weight)
        if self.lora_B.bias is not None:
            nn.init.zeros_(self.lora_B.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.lora_B(self.dropout(self.lora_A(x))) * self.scaling


# åœ¨ lora/adapters.py ä¸­ä¿®å¤ LoRALinear ç±»

class LoRALinear(nn.Module):
    """å¸¦LoRAçš„çº¿æ€§å±‚ - ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§"""
    
    def __init__(
        self,
        original_layer: nn.Linear,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.1
    ):
        super().__init__()
        
        # ä¿å­˜åŸå§‹å±‚ï¼ˆå†»ç»“ï¼‰
        self.original_layer = original_layer
        for param in self.original_layer.parameters():
            param.requires_grad = False
        
        # è·å–åŸå§‹å±‚çš„è®¾å¤‡
        device = next(self.original_layer.parameters()).device
        
        # LoRAé€‚é…å™¨ - ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.lora = LoRALayer(
            in_features=original_layer.in_features,
            out_features=original_layer.out_features,
            rank=rank,
            alpha=alpha,
            dropout=dropout,
            bias=original_layer.bias is not None
        ).to(device)  # ğŸ”§ ç¡®ä¿LoRAåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        
        # æ˜¯å¦å¯ç”¨LoRA
        self.enable_lora = True
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        # ç¡®ä¿LoRAå±‚åœ¨ä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡ä¸Š
        if self.lora.lora_A.weight.device != x.device:
            self.lora = self.lora.to(x.device)
        
        # åŸå§‹å±‚è¾“å‡º
        output = self.original_layer(x)
        
        # æ·»åŠ LoRAè¾“å‡º
        if self.enable_lora:
            lora_output = self.lora(x)
            output = output + lora_output
        
        return output
    
    def merge_weights(self):
        """å°†LoRAæƒé‡åˆå¹¶åˆ°åŸå§‹æƒé‡ä¸­ - ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§"""
        if not self.enable_lora:
            return
        
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = self.original_layer.weight.device
        
        # å°†LoRAæƒé‡ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        self.lora = self.lora.to(device)
        
        # è®¡ç®—LoRAæƒé‡
        lora_weight = self.lora.lora_B.weight @ self.lora.lora_A.weight * self.lora.scaling
        
        # åˆå¹¶åˆ°åŸå§‹æƒé‡
        self.original_layer.weight.data += lora_weight
        
        if self.original_layer.bias is not None and self.lora.lora_B.bias is not None:
            self.original_layer.bias.data += self.lora.lora_B.bias.data
        
        # ç¦ç”¨LoRA
        self.enable_lora = False
    
    def unmerge_weights(self):
        """ä»åŸå§‹æƒé‡ä¸­åˆ†ç¦»LoRAæƒé‡ - ä¿®å¤è®¾å¤‡ä¸€è‡´æ€§"""
        if self.enable_lora:
            return
        
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = self.original_layer.weight.device
        
        # å°†LoRAæƒé‡ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        self.lora = self.lora.to(device)
        
        # è®¡ç®—LoRAæƒé‡
        lora_weight = self.lora.lora_B.weight @ self.lora.lora_A.weight * self.lora.scaling
        
        # ä»åŸå§‹æƒé‡ä¸­å‡å»
        self.original_layer.weight.data -= lora_weight
        
        if self.original_layer.bias is not None and self.lora.lora_B.bias is not None:
            self.original_layer.bias.data -= self.lora.lora_B.bias.data
        
        # å¯ç”¨LoRA
        self.enable_lora = True


class LoRAMultiheadAttention(nn.Module):
    """å¸¦LoRAçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(
        self,
        original_attention: nn.Module,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.1,
        target_modules: List[str] = None
    ):
        super().__init__()
        
        self.original_attention = original_attention
        
        # é»˜è®¤ç›®æ ‡æ¨¡å—
        if target_modules is None:
            target_modules = ["query", "key", "value", "dense"]
        
        # ä¸ºæŒ‡å®šçš„æ¨¡å—æ·»åŠ LoRA
        self.lora_modules = {}
        for name, module in original_attention.named_modules():
            if isinstance(module, nn.Linear):
                module_name = name.split('.')[-1]
                if any(target in module_name.lower() for target in target_modules):
                    self.lora_modules[name] = LoRALinear(
                        module, rank=rank, alpha=alpha, dropout=dropout
                    )
        
        # æ³¨å†ŒLoRAæ¨¡å—
        for name, lora_module in self.lora_modules.items():
            self.add_module(f"lora_{name.replace('.', '_')}", lora_module)
    
    def forward(self, *args, **kwargs):
        """å‰å‘ä¼ æ’­ - è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°"""
        # ä¸´æ—¶æ›¿æ¢åŸå§‹æ¨¡å—
        original_modules = {}
        for name, lora_module in self.lora_modules.items():
            original_modules[name] = self._get_submodule(self.original_attention, name)
            self._set_submodule(self.original_attention, name, lora_module)
        
        try:
            # æ‰§è¡ŒåŸå§‹æ³¨æ„åŠ›è®¡ç®—
            output = self.original_attention(*args, **kwargs)
        finally:
            # æ¢å¤åŸå§‹æ¨¡å—
            for name, original_module in original_modules.items():
                self._set_submodule(self.original_attention, name, original_module)
        
        return output
    
    def _get_submodule(self, module: nn.Module, path: str) -> nn.Module:
        """è·å–å­æ¨¡å—"""
        for name in path.split('.'):
            module = getattr(module, name)
        return module
    
    def _set_submodule(self, module: nn.Module, path: str, value: nn.Module):
        """è®¾ç½®å­æ¨¡å—"""
        *path_parts, name = path.split('.')
        for part in path_parts:
            module = getattr(module, part)
        setattr(module, name, value)


class LoRAModelWrapper(nn.Module):
    """LoRAæ¨¡å‹åŒ…è£…å™¨"""
    
    def __init__(
        self,
        base_model: nn.Module,
        config: Dict[str, Any]
    ):
        super().__init__()
        
        self.base_model = base_model
        self.config = config
        self.lora_modules = {}
        
        # å†»ç»“åŸºç¡€æ¨¡å‹
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # æ·»åŠ LoRAé€‚é…å™¨
        self._add_lora_adapters()
    
    def _add_lora_adapters(self):
        """æ·»åŠ LoRAé€‚é…å™¨åˆ°æ¨¡å‹"""
        rank = self.config.get('rank', 8)
        alpha = self.config.get('alpha', 16.0)
        dropout = self.config.get('dropout', 0.1)
        target_modules = self.config.get('target_modules', [])
        
        # éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—
        for name, module in self.base_model.named_modules():
            if self._should_add_lora(name, module, target_modules):
                if isinstance(module, nn.Linear):
                    lora_module = LoRALinear(
                        module, rank=rank, alpha=alpha, dropout=dropout
                    )
                    self.lora_modules[name] = lora_module
                    self._replace_module(name, lora_module)
                
                elif hasattr(module, 'query') or hasattr(module, 'attention'):
                    # æ³¨æ„åŠ›æ¨¡å—
                    lora_module = LoRAMultiheadAttention(
                        module, rank=rank, alpha=alpha, dropout=dropout
                    )
                    self.lora_modules[name] = lora_module
                    self._replace_module(name, lora_module)
        
        print(f"æ·»åŠ äº† {len(self.lora_modules)} ä¸ªLoRAé€‚é…å™¨")
    
    def _should_add_lora(self, name: str, module: nn.Module, target_modules: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ºè¯¥æ¨¡å—æ·»åŠ LoRA"""
        if not target_modules:
            # é»˜è®¤ç­–ç•¥ï¼šä¸ºæ‰€æœ‰çº¿æ€§å±‚å’Œæ³¨æ„åŠ›å±‚æ·»åŠ LoRA
            return isinstance(module, nn.Linear) or 'attention' in name.lower()
        
        # æ£€æŸ¥æ¨¡å—åæ˜¯å¦åŒ¹é…ç›®æ ‡æ¨¡å—
        return any(target in name.lower() for target in target_modules)
    
    def _replace_module(self, name: str, new_module: nn.Module):
        """æ›¿æ¢æ¨¡å—"""
        *path_parts, module_name = name.split('.')
        parent = self.base_model
        
        for part in path_parts:
            parent = getattr(parent, part)
        
        setattr(parent, module_name, new_module)
    
    def forward(self, *args, **kwargs):
        """å‰å‘ä¼ æ’­"""
        return self.base_model(*args, **kwargs)
    
    def save_lora_weights(self, save_path: str):
        """ä¿å­˜LoRAæƒé‡"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜LoRAæƒé‡
        lora_state_dict = {}
        for name, module in self.lora_modules.items():
            if hasattr(module, 'lora'):
                lora_state_dict[f"{name}.lora_A.weight"] = module.lora.lora_A.weight
                lora_state_dict[f"{name}.lora_B.weight"] = module.lora.lora_B.weight
                if module.lora.lora_B.bias is not None:
                    lora_state_dict[f"{name}.lora_B.bias"] = module.lora.lora_B.bias
        
        torch.save(lora_state_dict, save_path / "lora_weights.pth")
        
        # ä¿å­˜é…ç½®
        with open(save_path / "lora_config.json", 'w') as f:
            json.dump(self.config, f, indent=2)
        
        print(f"LoRAæƒé‡å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_lora_weights(self, load_path: str):
        """åŠ è½½LoRAæƒé‡"""
        load_path = Path(load_path)
        
        # åŠ è½½æƒé‡
        weights_file = load_path / "lora_weights.pth"
        if weights_file.exists():
            lora_state_dict = torch.load(weights_file, map_location='cpu')
            
            # åŠ è½½æƒé‡åˆ°å¯¹åº”æ¨¡å—
            for name, module in self.lora_modules.items():
                if hasattr(module, 'lora'):
                    if f"{name}.lora_A.weight" in lora_state_dict:
                        module.lora.lora_A.weight.data = lora_state_dict[f"{name}.lora_A.weight"]
                    if f"{name}.lora_B.weight" in lora_state_dict:
                        module.lora.lora_B.weight.data = lora_state_dict[f"{name}.lora_B.weight"]
                    if f"{name}.lora_B.bias" in lora_state_dict:
                        module.lora.lora_B.bias.data = lora_state_dict[f"{name}.lora_B.bias"]
            
            print(f"LoRAæƒé‡å·²ä» {load_path} åŠ è½½")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°LoRAæƒé‡æ–‡ä»¶ {weights_file}")
    
    def merge_and_save(self, save_path: str):
        """åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹"""
        # åˆå¹¶æ‰€æœ‰LoRAæƒé‡
        for module in self.lora_modules.values():
            if hasattr(module, 'merge_weights'):
                module.merge_weights()
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        torch.save(self.base_model.state_dict(), save_path)
        print(f"åˆå¹¶çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        
        # æ¢å¤LoRAçŠ¶æ€
        for module in self.lora_modules.values():
            if hasattr(module, 'unmerge_weights'):
                module.unmerge_weights()
    
    def get_trainable_parameters(self) -> int:
        """è·å–å¯è®­ç»ƒå‚æ•°æ•°é‡"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'trainable_percentage': 100 * trainable_params / total_params
        }
    
    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        info = self.get_trainable_parameters()
        print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°æ•°: {info['total_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ•°: {info['trainable_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {info['trainable_percentage']:.2f}%")
        print(f"  LoRAæ¨¡å—æ•°: {len(self.lora_modules)}")


def create_lora_model(base_model: nn.Module, config: Dict[str, Any]) -> LoRAModelWrapper:
    """åˆ›å»ºLoRAæ¨¡å‹çš„ä¾¿æ·å‡½æ•°"""
    return LoRAModelWrapper(base_model, config)


def load_lora_model(base_model: nn.Module, lora_path: str) -> LoRAModelWrapper:
    """åŠ è½½å·²è®­ç»ƒçš„LoRAæ¨¡å‹"""
    lora_path = Path(lora_path)
    
    # åŠ è½½LoRAé…ç½®
    config_file = lora_path / "lora_config.json"
    if config_file.exists():
        with open(config_file, 'r') as f:
            config = json.load(f)
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = {'rank': 8, 'alpha': 16.0, 'dropout': 0.1}
    
    # åˆ›å»ºLoRAæ¨¡å‹
    lora_model = LoRAModelWrapper(base_model, config)
    
    # åŠ è½½æƒé‡
    lora_model.load_lora_weights(lora_path)
    
    return lora_model