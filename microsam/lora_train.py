#!/usr/bin/env python3
"""
åŸºäºSAMçš„ç»†èƒåˆ†å‰²LoRAå¾®è°ƒç³»ç»Ÿ - å‘½ä»¤è¡Œè®­ç»ƒå·¥å…·
"""

import argparse
import warnings
import os
import sys
from glob import glob
from typing import Union, Tuple, Optional, List

import numpy as np
import imageio.v3 as imageio
from matplotlib import pyplot as plt
from skimage.measure import label as connected_components

import torch

from torch_em.util.debug import check_loader
from torch_em.data import MinInstanceSampler
from torch_em.util.util import get_random_colors

import micro_sam.training as sam_training
from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data
from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation

warnings.filterwarnings("ignore")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="åŸºäºSAMçš„ç»†èƒåˆ†å‰²LoRAå¾®è°ƒè®­ç»ƒå·¥å…·",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # æ•°æ®è·¯å¾„å‚æ•°
    parser.add_argument("--train_image_dir", type=str, required=True,
                       help="è®­ç»ƒå›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--train_mask_dir", type=str, required=True,
                       help="è®­ç»ƒæ©ç ç›®å½•è·¯å¾„")
    parser.add_argument("--val_image_dir", type=str, required=True,
                       help="éªŒè¯å›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--val_mask_dir", type=str, required=True,
                       help="éªŒè¯æ©ç ç›®å½•è·¯å¾„")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_type", type=str, default="vit_b_lm",
                       choices=["vit_t", "vit_b", "vit_l", "vit_h", "vit_b_lm", "vit_l_lm"],
                       help="SAMæ¨¡å‹ç±»å‹")
    parser.add_argument("--checkpoint_name", type=str, required=True,
                       help="ä¿å­˜çš„æ£€æŸ¥ç‚¹åç§°")
    parser.add_argument("--pretrained_checkpoint", type=str, default=None,
                       help="é¢„è®­ç»ƒæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    
    # LoRAå‚æ•°
    parser.add_argument("--rank", type=int, default=8,
                       help="LoRA rankå‚æ•°")
    parser.add_argument("--attention_layers_to_update", type=int, nargs="+", 
                       default=[9, 10, 11],
                       help="éœ€è¦æ›´æ–°çš„æ³¨æ„åŠ›å±‚ç´¢å¼•åˆ—è¡¨")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--save_root", type=str, required=True,
                       help="æ¨¡å‹ä¿å­˜æ ¹ç›®å½•")
    parser.add_argument("--train_number", type=int, default=0,
                       help="è®­ç»ƒçš„å›¾ç‰‡æ•°é‡")
    parser.add_argument("--gpu_id", type=int, default=0,
                       help="æŒ‡å®šä½¿ç”¨çš„GPU ID")
    parser.add_argument("--n_epochs", type=int, default=100,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--n_objects_per_batch", type=int, default=5,
                       help="æ¯æ‰¹æ¬¡é‡‡æ ·çš„å¯¹è±¡æ•°é‡")
    parser.add_argument("--patch_height", type=int, default=512,
                       help="è®­ç»ƒè¡¥ä¸é«˜åº¦")
    parser.add_argument("--patch_width", type=int, default=512,
                       help="è®­ç»ƒè¡¥ä¸å®½åº¦")
    parser.add_argument("--min_instance_size", type=int, default=25,
                       help="æœ€å°å®ä¾‹å¤§å°")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--train_instance_segmentation", action="store_true",
                       help="æ˜¯å¦è®­ç»ƒå®ä¾‹åˆ†å‰²è§£ç å™¨")
    parser.add_argument("--freeze_prompt_encoder", action="store_true",
                       help="æ˜¯å¦å†»ç»“prompt encoder")
    parser.add_argument("--visualize_first_sample", action="store_true",
                       help="æ˜¯å¦å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    return parser.parse_args()


def validate_paths(args):
    """éªŒè¯è¾“å…¥è·¯å¾„æ˜¯å¦å­˜åœ¨"""
    paths_to_check = [
        ("train_image_dir", args.train_image_dir),
        ("train_mask_dir", args.train_mask_dir),
        ("val_image_dir", args.val_image_dir),
        ("val_mask_dir", args.val_mask_dir),
    ]
    
    for name, path in paths_to_check:
        if not os.path.exists(path):
            raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {name} = {path}")
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
    if args.pretrained_checkpoint and not os.path.exists(args.pretrained_checkpoint):
        print(f"Warning: é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.pretrained_checkpoint}")
        print("å°†ä½¿ç”¨é»˜è®¤çš„SAMé¢„è®­ç»ƒæƒé‡")
        args.pretrained_checkpoint = None
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_root, exist_ok=True)


def setup_device(gpu_id):
    """è®¾ç½®GPUè®¾å¤‡"""
    if torch.cuda.is_available():
        if gpu_id >= torch.cuda.device_count():
            raise ValueError(f"GPU ID {gpu_id} ä¸å¯ç”¨ï¼Œæ€»å…±æœ‰ {torch.cuda.device_count()} ä¸ªGPU")
        
        # è®¾ç½®å½“å‰CUDAè®¾å¤‡
        torch.cuda.set_device(gpu_id)
        
        # micro_sam åªæ¥å— 'cuda' ä½œä¸ºè®¾å¤‡åï¼Œè€Œä¸æ˜¯ 'cuda:X'
        device = "cuda"
        print(f"ä½¿ç”¨GPU: cuda:{gpu_id} ({torch.cuda.get_device_name(gpu_id)})")
        print(f"CUDAè®¾å¤‡å·²è®¾ç½®ä¸º: {torch.cuda.current_device()}")
    else:
        device = "cpu"
        print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    return device


def load_data_paths(image_dir, mask_dir):
    """åŠ è½½æ•°æ®è·¯å¾„"""
    image_paths = sorted(glob(os.path.join(image_dir, "*")))
    mask_paths = sorted(glob(os.path.join(mask_dir, "*")))
    
    if len(image_paths) == 0:
        raise ValueError(f"åœ¨ {image_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
    if len(mask_paths) == 0:
        raise ValueError(f"åœ¨ {mask_dir} ä¸­æœªæ‰¾åˆ°æ©ç æ–‡ä»¶")
    if len(image_paths) != len(mask_paths):
        raise ValueError(f"å›¾åƒæ•°é‡ ({len(image_paths)}) ä¸æ©ç æ•°é‡ ({len(mask_paths)}) ä¸åŒ¹é…")
    
    return image_paths, mask_paths


def visualize_sample(image_paths, mask_paths):
    """å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬"""
    image_path, mask_path = image_paths[0], mask_paths[0]
    image = imageio.imread(image_path)
    mask = imageio.imread(mask_path)

    fig, ax = plt.subplots(1, 2, figsize=(12, 6))

    ax[0].imshow(image, cmap="gray")
    ax[0].set_title("Input Image")
    ax[0].axis("off")

    mask = connected_components(mask)
    ax[1].imshow(mask, cmap=get_random_colors(mask), interpolation="nearest")
    ax[1].set_title("Ground Truth Instances")
    ax[1].axis("off")

    plt.tight_layout()
    plt.show()
    plt.close()


def create_data_loader(image_paths, mask_paths, args, is_train=True):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    sampler = MinInstanceSampler(min_size=args.min_instance_size)
    patch_shape = (1, args.patch_height, args.patch_width)

    if args.train_number>0:
        image_paths = image_paths[:args.train_number]
        mask_paths = mask_paths[:args.train_number]
    
    loader = sam_training.default_sam_loader(
        raw_paths=image_paths,
        raw_key=None,
        label_paths=mask_paths,
        label_key=None,
        with_segmentation_decoder=args.train_instance_segmentation,
        patch_shape=patch_shape,
        batch_size=args.batch_size,
        is_seg_dataset=True,
        shuffle=is_train,
        raw_transform=sam_training.identity,
        sampler=sampler,
    )
    
    return loader


def print_training_config(args, device):
    """æ‰“å°è®­ç»ƒé…ç½®"""
    print("=" * 80)
    print("ğŸ”¬ åŸºäºSAMçš„ç»†èƒåˆ†å‰²LoRAå¾®è°ƒç³»ç»Ÿ")
    print("=" * 80)
    print("ğŸ“Š æ•°æ®é…ç½®:")
    print(f"  è®­ç»ƒå›¾åƒç›®å½•: {args.train_image_dir}")
    print(f"  è®­ç»ƒæ©ç ç›®å½•: {args.train_mask_dir}")
    print(f"  éªŒè¯å›¾åƒç›®å½•: {args.val_image_dir}")
    print(f"  éªŒè¯æ©ç ç›®å½•: {args.val_mask_dir}")
    print()
    print("ğŸ¤– æ¨¡å‹é…ç½®:")
    print(f"  æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"  æ£€æŸ¥ç‚¹åç§°: {args.checkpoint_name}")
    print(f"  é¢„è®­ç»ƒæ£€æŸ¥ç‚¹: {args.pretrained_checkpoint or 'ä½¿ç”¨é»˜è®¤SAMæƒé‡'}")
    print()
    print("ğŸ”§ LoRAé…ç½®:")
    print(f"  Rank: {args.rank}")
    print(f"  æ›´æ–°å±‚: {args.attention_layers_to_update}")
    print(f"  å†»ç»“Prompt Encoder: {args.freeze_prompt_encoder}")
    print()
    print("ğŸ‹ï¸ è®­ç»ƒé…ç½®:")
    print(f"  è®­ç»ƒè½®æ•°: {args.n_epochs}")
    print(f"  æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"  æ¯æ‰¹æ¬¡å¯¹è±¡æ•°: {args.n_objects_per_batch}")
    print(f"  è¡¥ä¸å¤§å°: {args.patch_height}x{args.patch_width}")
    print(f"  æœ€å°å®ä¾‹å¤§å°: {args.min_instance_size}")
    print(f"  è®¾å¤‡: cuda:{args.gpu_id} (å½“å‰è®¾ç½®: {device})")
    print(f"  ä¿å­˜è·¯å¾„: {args.save_root}")
    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # éªŒè¯è·¯å¾„
    validate_paths(args)
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(args.gpu_id)
    
    # åŠ è½½æ•°æ®è·¯å¾„
    print("ğŸ“‚ åŠ è½½æ•°æ®è·¯å¾„...")
    train_image_paths, train_mask_paths = load_data_paths(args.train_image_dir, args.train_mask_dir)
    val_image_paths, val_mask_paths = load_data_paths(args.val_image_dir, args.val_mask_dir)
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_image_paths)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_image_paths)}")
    
    # å¯è§†åŒ–æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
    if args.visualize_first_sample:
        print("ğŸ–¼ï¸ å¯è§†åŒ–ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬...")
        visualize_sample(train_image_paths, train_mask_paths)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    # if args.train_number==0:
    train_loader = create_data_loader(train_image_paths, train_mask_paths, args, is_train=True)
    val_loader = create_data_loader(val_image_paths, val_mask_paths, args, is_train=False)
    # else:
    #     train_loader = create_data_loader(train_image_paths[:args.train_number], train_mask_paths, args, is_train=True)
    #     val_loader = create_data_loader(val_image_paths[:args.train_number], val_mask_paths, args, is_train=False)
    
    # æ‰“å°é…ç½®
    print_training_config(args, device)
    
    # è®¾ç½®LoRAå‚æ•°
    peft_kwargs = {
        "rank": args.rank,
        "attention_layers_to_update": args.attention_layers_to_update,
    }
    
    if args.freeze_prompt_encoder:
        freeze=['prompt_encoder']
    else:
        freeze=[]
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        sam_training.train_sam(
            name=args.checkpoint_name,
            save_root=args.save_root,
            model_type=args.model_type,
            checkpoint_path=args.pretrained_checkpoint,
            train_loader=train_loader,
            val_loader=val_loader,
            n_epochs=args.n_epochs,
            n_objects_per_batch=args.n_objects_per_batch,
            with_segmentation_decoder=args.train_instance_segmentation,
            device=device,
            peft_kwargs=peft_kwargs,
            freeze=freeze,
        )
        
        print("=" * 80)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åˆ°: {args.save_root}")
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()